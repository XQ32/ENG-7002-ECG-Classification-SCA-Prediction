% ========================================================================
% File: generate_sca_classifier_v3.m
% Overview: Use *_info.mat metadata generated by predict_shhs1_all.m (no EDF reading)
%           Build a new "PVC â†’ recovery (variable window)" dynamics feature set, and
%           train an imbalance-friendly binary classifier (Alive=1 / Dead=0).
%           Focus on post-PVC RR/T/QTc recovery time constants, half-lives,
%           overshoot, oscillations, and instability counts. Outputs an English-only confusion matrix.
% Key constraints:
%   - Read only *_info.mat (including: predPVCIndices, patientVital, fs, recordNumSamples,
%     rGlobalAll, isPVCBeat, qrs_dur_vec, r_amp_vec, sqi_vec, t_amp_vec, tGlobalIndices).
%   - If required info is insufficient, print suggested fields at the end so you can add
%     them when generating *_info.mat.
% Tunables:
%   processFirstN        : Process first N records (<=0 means all), default 750.
%   splitRatioTest       : Test split ratio (default 0.20).
%   randomSeed           : Random seed.
%   params.*             : Recovery window/baseline/decision parameters (see below).
%   modelOptions.*       : Model/cost/threshold moving/CV threshold search configuration.
% Outputs:
%   results/post_ectopic_features_v3.mat          : Record-level feature table.
%   results/SCA_trainingFeatureTable_v3.mat       : Training feature table.
%   results/SCA_testingFeatureTable_v3.mat        : Testing feature table.
%   results/sca_classifier_v3.mat                 : Trained model and configuration.
% Notes:
%   - If you need more morphology-focused indicators (e.g., precise QTend, ST reference point,
%     QRS on/off indices, P-wave features), please save more beat-wise indices/amplitudes
%     in predict_shhs1_all.m when creating the *_info.mat.
% Changelog:
%   2025-09-04: v3 initial release with the new feature set and training/evaluation flow (no EDF reading).
% ========================================================================

clc; clear; close all;

% ===================== Directories and configuration =====================
rootDir = pwd;
edfDir = fullfile(rootDir, 'shhs','polysomnography','edfs','shhs1');
if ~isfolder(edfDir)
    error('EDF directory not found: %s', edfDir);
end
addpath(genpath(rootDir));

resultsDir = fullfile(rootDir, 'results');
if ~isfolder(resultsDir), mkdir(resultsDir); end

% Read SHHS demographic/follow-up summary (optional)
covarCsv = fullfile(rootDir, 'shhs','datasets','shhs-cvd-summary-dataset-0.21.0.csv');
covarMap = containers.Map('KeyType','char','ValueType','any');
if exist(covarCsv,'file')
    try
        Tcov = readtable(covarCsv);
    % Normalize nsrrid to string
        if any(strcmpi(Tcov.Properties.VariableNames, 'nsrrid'))
            idStr = string(Tcov.nsrrid);
        else
            % Fallback: try the first column (patient/id)
            idStr = string(Tcov{:,1});
        end
    % Select required columns (if present)
        has_cens = any(strcmpi(Tcov.Properties.VariableNames,'censdate'));
        has_gender = any(strcmpi(Tcov.Properties.VariableNames,'gender'));
        has_race   = any(strcmpi(Tcov.Properties.VariableNames,'race'));
        has_age    = any(strcmpi(Tcov.Properties.VariableNames,'age_s1'));
        for ii = 1:height(Tcov)
            key = char(strtrim(idStr(ii)));
            val = struct();
            if has_cens,  val.censdate = double(Tcov.censdate(ii)); else, val.censdate = NaN; end
            if has_gender, val.gender   = double(Tcov.gender(ii));   else, val.gender   = NaN; end
            if has_race,   val.race     = double(Tcov.race(ii));     else, val.race     = NaN; end
            if has_age,    val.age_s1   = double(Tcov.age_s1(ii));   else, val.age_s1   = NaN; end
            covarMap(key) = val;
        end
        fprintf('Loaded covariates: %d rows from %s\n', height(Tcov), covarCsv);
    catch ME
        fprintf('Failed to read covariate CSV: %s\n', ME.message);
    end
else
    fprintf('Covariate CSV not found (skip): %s\n', covarCsv);
end

% Process only the first N *_info.mat records (<=0 means all)
processFirstN = 0;

% Stratified split ratio
splitRatioTest = 0.20;

% Random seed
randomSeed = 42;
rng(randomSeed);

% Recovery/baseline/threshold parameters (tune as needed)
params = struct();
params.baselineSec        = 50;   % Baseline look-back window before PVC (seconds)
params.baselineMinBeats   = 10;   % Minimum sinus beats for baseline
params.maxObsSec          = 50;   % Max observation duration after PVC (seconds)
params.consecStableBeats  = 10;    % Consecutive beats required to declare "stable recovery"
params.rrTolFrac          = 0.08; % RR recovery threshold: |RR - muRR| <= max(rrTolFrac*muRR, rrTolSigma*sigmaRR)
params.rrTolSigma         = 1.5;
params.hrt_ts_low_thr     = 0.0;  % HRT-TS low threshold (smaller = more abnormal)
params.minRR              = 0.30; % RR quality threshold (seconds)
params.maxRR              = 2.50; % RR quality threshold (seconds)
params.minPVCPerRecord    = 10;   % Quality gate: minimum PVC count per record
params.minSQIRatio        = 0.60; % Quality gate: fraction of non-PVC beats with SQI=true

% Model and class-imbalance settings
modelOptions = struct();
modelOptions.Method                = 'LogitBoost'; % Use the latest best by default
modelOptions.NumLearningCycles     = 200;         % Best: 200
modelOptions.LearnRate             = 0.03;        % Best: 0.03
modelOptions.MaxNumSplits          = 40;          % Best: 40
modelOptions.MinLeafSize           = 20;          % Best: 20
modelOptions.NumVariablesToSample  = 'sqrt';
modelOptions.enableCostMatrix      = true;   % Cost matrix (bias to reduce Dead-class FN)
modelOptions.costFP                = 2;
modelOptions.costFN                = 6;
modelOptions.enableThresholdMoving = true;   % Post-training threshold tuning (optimize Dead-class F1)
% Initialize default threshold to recent best (will be overwritten by CV search)
modelOptions.pvcThreshold          = 0.26;   % Best threshold (from recent search)
modelOptions.cvKFold               = 5;
modelOptions.thrCandidates         = 0.02:0.01:0.98;
% Feature selection configuration (MRMR + CV)
modelOptions.enableFeatureSelection = true;
modelOptions.featureKList          = [8, 12, 16, 24];
% Correlation filtering threshold (further de-redundancy after selection)
modelOptions.maxFeatureCorrelation = 0.85;
% Low-discriminative feature filtering (train-set univariate only)
modelOptions.filterLowDiscriminative = true;
modelOptions.filter_auc_margin   = 0.05;  % minimum |AUC - 0.5| deviation
modelOptions.filter_min_abs_d    = 0.10;  % min |Cohen's d|
modelOptions.filter_min_mi_bits  = 0.005; % minimum mutual information (bits)
modelOptions.filter_max_p        = 0.20;  % max ranksum p-value (significance)

% Auto candidate methods and hyperparameter search grid (effective when Method='auto')
modelOptions.candidateMethods = {'LogitBoost','GentleBoost','AdaBoostM1','Bag'}; % Exclude RUSBoost and Subspace
modelOptions.searchGrid = struct();
modelOptions.searchGrid.NumLearningCycles    = [200, 300];       % Boosting/Bagging cycles
modelOptions.searchGrid.LearnRate            = [0.03, 0.05, 0.10]; % Boosting only
modelOptions.searchGrid.MaxNumSplits         = [40, 60, 100];
modelOptions.searchGrid.MinLeafSize          = [8, 12, 20];
modelOptions.searchGrid.NumVariablesToSample = {'sqrt'};         % Subfeature sampling strategy
modelOptions.autoSelectVerbose               = false;            % Quiet by default: fewer logs
modelOptions.disableAutoInAblation           = true;             % Reuse main-model hyperparams in ablation

% Output files
featOutFile = fullfile(resultsDir, 'post_ectopic_features_v3.mat');
trainTblFile = fullfile(resultsDir, 'SCA_trainingFeatureTable_v3.mat');
testTblFile  = fullfile(resultsDir, 'SCA_testingFeatureTable_v3.mat');
modelOutFile = fullfile(resultsDir, 'sca_classifier_v3.mat');

fprintf('=== SCA Risk (v3): variable recovery window features & RUSBoost ===\n');
fprintf('Info MAT directory: %s\n', edfDir);

% ===================== Load blacklist =====================
blackListIds = string([]);
blackListNames = string([]);
blacklistFile = fullfile(resultsDir, 'badsignallist.csv');
if exist(blacklistFile,'file')
    try
        C = readcell(blacklistFile);
        vals = string(C(:));
        vals = strtrim(vals);
        vals = replace(vals, '"','');
        vals = replace(vals, '''','');
        vals = vals(vals ~= "");
    % Mixed numeric-only and name strings, extract numeric IDs
        isDigits = false(numel(vals),1);
        for ii = 1:numel(vals)
            isDigits(ii) = ~isempty(regexp(vals(ii), '^\d+$', 'once'));
        end
        ids = vals(isDigits);
        names = vals(~isDigits);
        recIdFromNames = strings(0,1);
        for ii = 1:numel(names)
            tok = regexp(names(ii), '(\d+)', 'tokens', 'once');
            if ~isempty(tok)
                recIdFromNames(end+1,1) = string(tok{1}); %#ok<AGROW>
            end
        end
        blackListIds = unique([ids; recIdFromNames]);
        blackListNames = unique("shhs1-" + blackListIds);
        fprintf('Loaded blacklist: %d entries. Will skip these records.\n', numel(blackListIds));
    catch ME
        fprintf('Failed to load blacklist (no skipping): %s\n', ME.message);
    end
else
    fprintf('Blacklist not found (no skipping): %s\n', blacklistFile);
end

% ===================== Enumerate *_info.mat =====================
infoFiles = dir(fullfile(edfDir, 'shhs1-*_*info.mat')); % compatible with misnaming
infoFiles2 = dir(fullfile(edfDir, 'shhs1-*_info.mat'));
% Deduplicate and merge
allPaths = unique([arrayfun(@(d) fullfile(d.folder,d.name), infoFiles, 'UniformOutput', false), ...
                   arrayfun(@(d) fullfile(d.folder,d.name), infoFiles2, 'UniformOutput', false)]);
if isempty(allPaths)
    error('No *_info.mat files found in: %s', edfDir);
end
allPaths = sort(allPaths);
if processFirstN > 0
    allPaths = allPaths(1:min(processFirstN, numel(allPaths)));
end
fprintf('Planned to process %d records (info MAT).\n', numel(allPaths));

% ===================== Iterate records and build features =====================
allRowTables = cell(0,1);
numSkippedQuality = 0; % Count drops by quality thresholds

for iFile = 1:numel(allPaths)
    infoPath = allPaths{iFile};
    [~, baseName, ~] = fileparts(infoPath);
    recBase = erase(baseName, '_info');
    % Blacklist filtering: support '200002' ID or 'shhs1-200002' name
    recIdTok = regexp(recBase, 'shhs1-(\d+)$', 'tokens', 'once');
    recIdStr = "";
    if ~isempty(recIdTok)
        recIdStr = string(recIdTok{1});
    end
    if any(blackListNames == string(recBase)) || (strlength(recIdStr) > 0 && any(blackListIds == recIdStr))
        fprintf('  Skip: in blacklist -> %s\n', recBase);
        continue;
    end
    fprintf('\n--- [%d/%d] Loading %s ...\n', iFile, numel(allPaths), [baseName '.mat']);
    try
        S = load(infoPath);
    catch ME
        fprintf('  Skip: load failed (%s)\n', ME.message);
        continue;
    end

    requiredFields = {'predPVCIndices','patientVital','fs','recordNumSamples','rGlobalAll','isPVCBeat','qrs_dur_vec','r_amp_vec','sqi_vec','t_amp_vec','tGlobalIndices'};
    missing = {};
    for ff = 1:numel(requiredFields)
        if ~isfield(S, requiredFields{ff}) || isempty(S.(requiredFields{ff}))
            missing{end+1} = requiredFields{ff}; %#ok<AGROW>
        end
    end
    if ~isempty(missing)
        fprintf('  Skip: missing fields in info MAT: %s\n', strjoin(missing, ', '));
        continue;
    end

    patientVital = double(S.patientVital); % 0=Dead, 1=Alive, may be NaN
    if ~isfinite(patientVital)
        fprintf('  Skip: patientVital is missing.\n');
        continue;
    end

    % Raw per-beat vectors (no raw ECG)
    fs = double(S.fs);
    N  = double(S.recordNumSamples);
    rGlobalAll = double(S.rGlobalAll(:));
    isPVCBeat  = logical(S.isPVCBeat(:));
    qrs_dur_vec = double(S.qrs_dur_vec(:));
    r_amp_vec   = double(S.r_amp_vec(:)); %#ok<NASGU>
    sqi_vec     = logical(S.sqi_vec(:));
    t_amp_vec   = double(S.t_amp_vec(:));
    tGlobalIdx  = double(S.tGlobalIndices(:));
    if any([numel(isPVCBeat), numel(qrs_dur_vec), numel(t_amp_vec), numel(tGlobalIdx), numel(sqi_vec)] ~= numel(rGlobalAll))
        fprintf('  Skip: vector lengths inconsistent.\n');
        continue;
    end

    % PVC global R indices (sample points)
    pvcRidx = double(S.predPVCIndices(:));
    pvcRidx = pvcRidx(isfinite(pvcRidx));
    pvcRidx = sort(pvcRidx);
    numPVC  = numel(pvcRidx);
    fprintf('  Beats=%d, fs=%g Hz, PVC=%d\n', numel(rGlobalAll), fs, numPVC);
    if numPVC == 0
    % Quality threshold: no PVC -> drop
        fprintf('  Skip by quality: PVC=0 (<%d)\n', params.minPVCPerRecord);
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    % Pre-compute RR vector (interval (j-1)->j maps to index j)
    numBeats = numel(rGlobalAll);
    rr_between = nan(numBeats,1);
    if numBeats >= 2
        rr_between(2:end) = (rGlobalAll(2:end) - rGlobalAll(1:end-1)) / fs;
    end

    % Map the PVC R-sample to the nearest beat index
    idxPVC_all_sorted = round(interp1(rGlobalAll, 1:numel(rGlobalAll), pvcRidx, 'nearest', 'extrap'));
    idxPVC_all_sorted = max(1, min(numBeats, idxPVC_all_sorted));

    % Record-level ratio of good SQI (counted on available non-PVC beats)
    try
        j_all = (2:numBeats).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & isfinite(rr_between(j_all));
        sqi_ratio = mean(double(sqi_vec(j_all(mask_all))), 'omitnan');
    catch
        sqi_ratio = NaN;
    end
    % Quality thresholds: PVC count and SQI ratio
    if numPVC < params.minPVCPerRecord || ~(isfinite(sqi_ratio) && sqi_ratio >= params.minSQIRatio)
        fprintf('  Skip by quality: PVC=%d, SQIratio=%.2f (minPVC=%d, minSQI=%.2f)\n', numPVC, sqi_ratio, params.minPVCPerRecord, params.minSQIRatio);
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    % ========== For each PVC: compute "variable-window recovery dynamics" features ==========
    % Per-PVC baseline stats, recovery half-lives, time constant, overshoot,
    % oscillations, instability counts, AUC, etc.
    recMetrics = struct();
    recMetrics.numPVC = numPVC;

    % Per-PVC containers
    halflife_rr = nan(numPVC,1);
    halflife_rr_30 = nan(numPVC,1);
    halflife_rr_10 = nan(numPVC,1);
    tau_rr = nan(numPVC,1);
    auc_rr = nan(numPVC,1);
    overshoot_rr = nan(numPVC,1);
    oscill_rr = nan(numPVC,1);
    stalls_rr = nan(numPVC,1);
    notRecovered_rr = false(numPVC,1);

    halflife_tamp = nan(numPVC,1);
    auc_tamp = nan(numPVC,1);

    halflife_qtc = nan(numPVC,1);
    auc_qtc = nan(numPVC,1);

    hrt_to = nan(numPVC,1);
    hrt_ts = nan(numPVC,1);

    coupling_ratio = nan(numPVC,1); % RR_pre / muRR
    hr_peak_accel_bpm = nan(numPVC,1); % Peak of (1/RR_post1 - 1/muRR)*60 (first 2 beats)

    % Longest "late instability variance" after PVC (>5s segment variance of dRR)
    late_var_rr = nan(numPVC,1);

    % -- Additional high-discriminative/advanced PVC-level features --
    early_mean_dev_rr = nan(numPVC,1);   % Early (0â€“5s) mean of dev
    late_mean_dev_rr  = nan(numPVC,1);   % Late (5â€“15s) mean of dev
    slope_rr_dev_0_5s = nan(numPVC,1);   % 0â€“5s linear slope
    slope_rr_dev_5_15s= nan(numPVC,1);   % 5â€“15s linear slope
    sampen_rr         = nan(numPVC,1);   % Sample entropy of RR dev (m=2, r=0.2*std)
    lzc_rr            = nan(numPVC,1);   % LZ complexity of the sign sequence of RR dev
    poincare_ratio_pp = nan(numPVC,1);   % Post(<=10s) SD1/SD2 divided by Pre SD1/SD2
    dev_rr_q95        = nan(numPVC,1);   % 95th percentile of dev
    tamp_rr_corr      = nan(numPVC,1);   % Correlation between dev_rr and dev_tamp

    qtc_over_frac     = nan(numPVC,1);   % Fraction of QTc deviation > 10%
    qtc_over_mag      = nan(numPVC,1);   % Max magnitude of QTc overshoot (>10%)

    for kk = 1:numPVC
        pvcSample = pvcRidx(kk);
        idxPVC = idxPVC_all_sorted(kk);

    % Observation window end (next PVC / upper bound seconds)
        nextPVCsample = inf;
        if kk < numPVC
            nextPVCsample = pvcRidx(kk+1);
        end
        obsEnd = min([double(N), double(pvcSample) + round(params.maxObsSec*fs), double(nextPVCsample)-1]);
        if ~(isfinite(obsEnd) && obsEnd > pvcSample)
            notRecovered_rr(kk) = true; % treat as censored
            continue;
        end

    % --- Pre-PVC baseline RR/T/QTc (non-PVC & good SQI) ---
        [muRR, sigRR, baseRR_vec, muTamp, muQTc] = local_baseline_stats(idxPVC, pvcSample, rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, qrs_dur_vec, fs, params);
        if ~isfinite(muRR) || muRR <= 0
            notRecovered_rr(kk) = true; % Invalid baseline; skip this PVC
            continue;
        end

    % Pre-/post-PVC RR
        rr_pre = nan; rr_post1 = nan; rr_post2 = nan;
        if idxPVC >= 2 && isfinite(rr_between(idxPVC))
            rr_pre = rr_between(idxPVC);
        end
        if idxPVC+1 <= numBeats && isfinite(rr_between(idxPVC+1))
            rr_post1 = rr_between(idxPVC+1);
        end
        if idxPVC+2 <= numBeats && isfinite(rr_between(idxPVC+2))
            rr_post2 = rr_between(idxPVC+2);
        end

    % HRT indices
        if isfinite(rr_pre) && isfinite(rr_post1) && rr_pre>0
            hrt_to(kk) = (rr_post1 - rr_pre) / rr_pre;
        end
        hrt_ts(kk) = local_approx_ts(idxPVC, pvcSample, obsEnd, rGlobalAll, rr_between, isPVCBeat, sqi_vec);

    % Coupling ratio and peak HR acceleration (within first 2 beats)
        if isfinite(rr_pre)
            coupling_ratio(kk) = rr_pre / muRR;
        end
        try
            hr0 = 60/max(muRR, eps);
            hr1 = 60/max(rr_post1, eps);
            hr2 = 60/max(rr_post2, eps);
            hr_peak_accel_bpm(kk) = max([hr1-hr0, hr2-hr0]);
        catch
            hr_peak_accel_bpm(kk) = NaN;
        end

    % Post-PVC valid RR/T/QTc sequence (only non-PVC & good SQI & within window)
        j_after = (idxPVC+1):numBeats;
        keep = false(size(j_after));
        for jj = 1:numel(j_after)
            j = j_after(jj);
            if rGlobalAll(j) > obsEnd, break; end
            if j>=2 && ~isPVCBeat(j) && ~isPVCBeat(j-1) && sqi_vec(j) && sqi_vec(j-1) && isfinite(rr_between(j))
                rrj = rr_between(j);
                if rrj>=params.minRR && rrj<=params.maxRR
                    keep(jj) = true;
                end
            end
        end
        j_after = j_after(keep);
        if isempty(j_after)
            notRecovered_rr(kk) = true;
            continue;
        end

    % Timestamps (seconds)
        t_after = (double(rGlobalAll(j_after)) - double(pvcSample)) / fs;
        rr_seq = rr_between(j_after);

    % Normalized deviation (relative to baseline)
        dev_rr = abs(rr_seq - muRR) ./ max(muRR, eps);

    % Recovery half-life (50%/30%/10%), requires consecutive satisfaction of params.consecStableBeats
        halflife_rr(kk)   = local_halflife(dev_rr, t_after, 0.50, params.consecStableBeats);
        halflife_rr_30(kk)= local_halflife(dev_rr, t_after, 0.30, params.consecStableBeats);
        halflife_rr_10(kk)= local_halflife(dev_rr, t_after, 0.10, params.consecStableBeats);
        if ~isfinite(halflife_rr(kk)) && t_after(end) < params.maxObsSec
            notRecovered_rr(kk) = true;
        end

    % Time constant (linear fit on ln(dev), robustly using first K points)
        tau_rr(kk) = local_time_constant(dev_rr, t_after, 8);

    % AUC: âˆ« dev_rr dt (normalize time by baselineSec to avoid long-record bias)
        auc_rr(kk) = local_auc(dev_rr, t_after, params.baselineSec);

    % Overshoot and oscillations/instability
        overshoot_rr(kk) = max(dev_rr) - min(dev_rr(1), dev_rr(1)); %#ok<NASGU>
        oscill_rr(kk)    = local_oscillation_index(dev_rr);
    stalls_rr(kk)    = local_recovery_stalls(dev_rr, 0.20, params.consecStableBeats); % Times of relapse after first reaching 20% threshold

    % Late instability (>5s segment variance of dev)
        late_mask = t_after >= 5.0;
        if any(late_mask)
            late_var_rr(kk) = var(dev_rr(late_mask), 'omitnan');
        end

    % T-amplitude recovery (normalized to baseline)
        tamp_seq = t_amp_vec(j_after);
        if isfinite(muTamp) && muTamp > 0
            dev_tamp = abs(tamp_seq - muTamp) ./ muTamp;
            halflife_tamp(kk) = local_halflife(dev_tamp, t_after, 0.50, params.consecStableBeats);
            auc_tamp(kk)      = local_auc(dev_tamp, t_after, params.baselineSec);
        else
            dev_tamp = nan(size(t_after));
        end

    % Approximate QTc recovery
        qtc_seq = local_qtc_series(j_after, rGlobalAll, tGlobalIdx, qrs_dur_vec, rr_between, fs);
        if ~isempty(qtc_seq) && all(isnan(qtc_seq))==false && isfinite(muQTc) && muQTc>0
            dev_qtc = abs(qtc_seq - muQTc) ./ muQTc;
            halflife_qtc(kk) = local_halflife(dev_qtc, t_after(1:numel(qtc_seq)), 0.50, params.consecStableBeats);
            auc_qtc(kk)      = local_auc(dev_qtc, t_after(1:numel(qtc_seq)), params.baselineSec);
            try
                qtc_over_frac(kk) = mean(dev_qtc > 0.10, 'omitnan');
            catch
                qtc_over_frac(kk) = NaN;
            end
            try
                qtc_over_mag(kk) = max(max(dev_qtc) - 0.10, 0);
            catch
                qtc_over_mag(kk) = NaN;
            end
        end

    % -- Compute additional "advanced/complexity" features --
        try
            % Early/Late means and piecewise slopes
            early_mask = (t_after >= 0) & (t_after <= 5);
            late_mask2 = (t_after > 5) & (t_after <= 15);
            if any(early_mask)
                early_mean_dev_rr(kk) = mean(dev_rr(early_mask), 'omitnan');
                slope_rr_dev_0_5s(kk) = local_linear_slope(t_after(early_mask), dev_rr(early_mask));
            end
            if any(late_mask2)
                late_mean_dev_rr(kk)  = mean(dev_rr(late_mask2), 'omitnan');
                slope_rr_dev_5_15s(kk)= local_linear_slope(t_after(late_mask2), dev_rr(late_mask2));
            end
        catch
        end
        try
            % Sample entropy (tolerant to short series)
            sampen_rr(kk) = local_sampen(dev_rr, 2, 0.2*std(dev_rr,'omitnan'));
        catch
            sampen_rr(kk) = NaN;
        end
        try
            % LZ complexity: based on the sign sequence of first differences
            d1 = diff(dev_rr(:));
            s = double(d1 >= 0); % Non-negative as 1, negative as 0
            lzc_rr(kk) = local_lzc_binary(s);
        catch
            lzc_rr(kk) = NaN;
        end
        try
            % PoincarÃ© SD1/SD2 ratio: Post<=10s vs Pre baseline
            [SD1_pre, SD2_pre] = local_poincare_sd(baseRR_vec);
            post_mask10 = t_after <= 10;
            if any(post_mask10)
                [SD1_post, SD2_post] = local_poincare_sd(rr_seq(post_mask10));
                r_pre  = SD1_pre/max(SD2_pre, eps);
                r_post = SD1_post/max(SD2_post, eps);
                if isfinite(r_pre) && r_pre>0 && isfinite(r_post)
                    poincare_ratio_pp(kk) = r_post / r_pre;
                end
            end
        catch
            poincare_ratio_pp(kk) = NaN;
        end
        try
            dev_rr_q95(kk) = quantile(dev_rr, 0.95);
        catch
            dev_rr_q95(kk) = NaN;
        end
        try
            if exist('dev_tamp','var') && numel(dev_tamp)==numel(dev_rr)
                C = corr(dev_rr(:), dev_tamp(:), 'type','Spearman','rows','pairwise');
                tamp_rr_corr(kk) = C;
            end
        catch
            tamp_rr_corr(kk) = NaN;
        end
    end

    % ========== Record-level aggregation ==========
    recDurationHr = (N/fs)/3600;
    pvcPerHour = numPVC / max(recDurationHr, eps);

    % Recovery failure/censoring ratio
    recovery_failure_ratio = mean(double(notRecovered_rr));

    % Aggregation helpers
    agg = @(v, f) f(v(isfinite(v)));
    safe_mean = @(v) agg(v, @mean); safe_med = @(v) agg(v, @median);

    % Assemble record-level features
    R = struct();
    R.record = string(recBase);
    R.patientVital = patientVital;
    R.PVCs_per_hour = pvcPerHour;
    R.PVC_count = numPVC;
    R.recovery_failure_ratio = recovery_failure_ratio;
    R.halflife_rr_median = safe_med(halflife_rr);
    R.halflife_rr30_median = safe_med(halflife_rr_30);
    R.halflife_rr10_median = safe_med(halflife_rr_10);
    R.tau_rr_median = safe_med(tau_rr);
    R.auc_rr_mean = safe_mean(auc_rr);
    R.oscill_rr_median = safe_med(oscill_rr);
    R.stalls_rr_mean = safe_mean(stalls_rr);
    R.late_var_rr_median = safe_med(late_var_rr);
    R.HRT_TO_abnormal_frac = mean(double(isfinite(hrt_to) & (hrt_to>=0)), 'omitnan');
    R.HRT_TS_low_frac = mean(double(isfinite(hrt_ts) & (hrt_ts<=params.hrt_ts_low_thr)), 'omitnan');
    R.halflife_tamp_median = safe_med(halflife_tamp);
    R.auc_tamp_mean = safe_mean(auc_tamp);
    R.halflife_qtc_median = safe_med(halflife_qtc);
    R.auc_qtc_mean = safe_mean(auc_qtc);
    R.QRS_Dur_PVC_mean = safe_mean(qrs_dur_vec(idxPVC_all_sorted));
    % Baseline HRV RMSSD (aggregated over PVC-specific baseline RR)
    try
        rmssd_base = local_rmssd_from_base_perPVC(idxPVC_all_sorted, pvcRidx, rGlobalAll, rr_between, isPVCBeat, sqi_vec, fs, params);
        R.RR_Pre_RMSSD_mean = mean(rmssd_base(isfinite(rmssd_base)), 'omitnan');
    catch
        R.RR_Pre_RMSSD_mean = NaN;
    end
    R.coupling_ratio_mean = safe_mean(coupling_ratio);
    R.hr_peak_accel_bpm_mean = safe_mean(hr_peak_accel_bpm);
    R.sqi_good_ratio = sqi_ratio;

    % -- Additional record-level aggregates (high-discriminative/advanced) --
    R.rr_dev_early_mean_med   = safe_med(early_mean_dev_rr);
    R.rr_dev_late_mean_med    = safe_med(late_mean_dev_rr);
    R.rr_dev_late_minus_early = R.rr_dev_late_mean_med - R.rr_dev_early_mean_med;
    R.rr_dev_slope_0_5s_med   = safe_med(slope_rr_dev_0_5s);
    R.rr_dev_slope_5_15s_med  = safe_med(slope_rr_dev_5_15s);
    R.rr_dev_sampen_med       = safe_med(sampen_rr);
    R.rr_dev_lzc_med          = safe_med(lzc_rr);
    R.poincare_ratio_postpre_med = safe_med(poincare_ratio_pp);
    R.rr_dev_q95_med          = safe_med(dev_rr_q95);
    % Max slope within post-PVC 0â€“5s window (take per-PVC max, then median)
    try
    % If not computed per PVC above, approximate fluctuation strength via rr_dev_q95.
    % For more precise estimates: compute max slope_rr_dev_0_5s per PVC in the loop above, then aggregate here.
    % To keep current structure, use the median of slope_rr_dev_0_5s as the representative.
        R.rr_dev_maxslope_0_5s_med = safe_med(slope_rr_dev_0_5s);
    catch
        R.rr_dev_maxslope_0_5s_med = NaN;
    end
    R.tamp_rr_corr_med        = safe_med(tamp_rr_corr);
    R.qtc_overshoot_frac_mean = safe_mean(qtc_over_frac);
    R.qtc_overshoot_mag_mean  = safe_mean(qtc_over_mag);

    % Cross-PVC population instability
    R.coupling_ratio_cv       = local_coeffvar(coupling_ratio);
    try
        pvc_spacings = diff(double(pvcRidx))/fs; pvc_spacings = pvc_spacings(isfinite(pvc_spacings) & pvc_spacings>0);
        R.pvc_spacing_cv = local_coeffvar(pvc_spacings);
    catch
        R.pvc_spacing_cv = NaN;
    end
    % Recovery rate (mean of 1/half-life; i.e., harmonic-mean-based rate)
    try
        hl = halflife_rr(isfinite(halflife_rr) & halflife_rr>0);
        if ~isempty(hl)
            R.recovery_rate_hmean = mean(1./hl);
        else
            R.recovery_rate_hmean = NaN;
        end
    catch
        R.recovery_rate_hmean = NaN;
    end

    % Additional demographic/follow-up covariates
    try
    % Extract nsrrid (e.g., shhs1-200002 -> 200002)
        recIdTok = regexp(string(recBase), 'shhs1-(\d+)$', 'tokens', 'once');
        nsrrid = "";
        if ~isempty(recIdTok)
            nsrrid = string(recIdTok{1});
        end
        R.nsrrid = nsrrid;
        val = [];
        if strlength(nsrrid)>0 && isKey(covarMap, char(nsrrid))
            val = covarMap(char(nsrrid));
        end
        if ~isempty(val)
            R.censdate = val.censdate;
            R.gender   = val.gender;   % 1: Male, 2: Female
            R.race     = val.race;     % 1: White, 2: Black, 3: Other
            R.age_s1   = val.age_s1;   % Age (>=90 truncated to 90)
        else
            R.censdate = NaN;
            R.gender   = NaN;
            R.race     = NaN;
            R.age_s1   = NaN;
        end
    catch
        R.nsrrid  = "";
        R.censdate= NaN; R.gender=NaN; R.race=NaN; R.age_s1=NaN;
    end

    allRowTables{end+1,1} = struct2table(R);
end

if isempty(allRowTables)
    error('No features generated.');
end

featureTable = vertcat(allRowTables{:});
fprintf('\nCollected %d records (rows).\n', height(featureTable));
fprintf('Skipped by quality filter (minPVC=%d, minSQI=%.2f): %d records.\n', params.minPVCPerRecord, params.minSQIRatio, numSkippedQuality);

% Save full table (with NaNs)
try
    save(featOutFile, 'featureTable');
    fprintf('âœ“ Saved features: %s\n', featOutFile);
catch ME
    fprintf('Failed to save feature table: %s\n', ME.message);
end

% ===================== Imputation and split =====================
% Select numeric predictors (exclude record / patientVital)
names = featureTable.Properties.VariableNames;
isNum = false(size(names));
for i = 1:numel(names)
    isNum(i) = isnumeric(featureTable.(names{i})) && isvector(featureTable.(names{i}));
end
% Exclude leakage or undesirable predictors (e.g., censdate = follow-up time, would be hindsight)
excludePredictors = {'patientVital','censdate'};
predictorMask = isNum & ~ismember(names, excludePredictors);
predictorNames = names(predictorMask);
responseName = 'patientVital';

% Impute missing values with column median
for i = 1:numel(predictorNames)
    col = featureTable.(predictorNames{i});
    if ~isnumeric(col), continue; end
    nanMask = isnan(col);
    if any(nanMask)
        medv = median(col(~nanMask));
        if isempty(medv) || isnan(medv), medv = 0; end
        col(nanMask) = medv;
        featureTable.(predictorNames{i}) = col;
    end
end

% Group-stratified split by patient ID (avoid same patient in both train and test)
y = featureTable.(responseName);
if iscell(y), y = cell2mat(y); end
if islogical(y), y = double(y); end
recNames = string(featureTable.record);
patientId = strings(height(featureTable),1);
for ii = 1:height(featureTable)
    nm = recNames(ii);
    tok = regexp(nm, 'shhs1-(\d+)$', 'tokens', 'once');
    if ~isempty(tok)
        patientId(ii) = string(tok{1});
    else
    % Fallback: extract the numeric substring
        tok2 = regexp(nm, '(\d+)', 'tokens', 'once');
        if ~isempty(tok2)
            patientId(ii) = string(tok2{1});
        else
            patientId(ii) = nm; % If parsing fails, keep the full name
        end
    end
end
[trainMask, testMask] = local_group_stratified_split(patientId, y, splitRatioTest, randomSeed);

trainingFeatureTable = featureTable(trainMask,:);
testingFeatureTable  = featureTable(testMask,:);
fprintf('Train=%d, Test=%d\n', height(trainingFeatureTable), height(testingFeatureTable));

try
    save(trainTblFile, 'trainingFeatureTable'); fprintf('âœ“ Saved training table\n');
catch ME
    fprintf('Failed to save training table: %s\n', ME.message);
end
try
    save(testTblFile,  'testingFeatureTable');  fprintf('âœ“ Saved testing table\n');
catch ME
    fprintf('Failed to save testing table: %s\n', ME.message);
end

% ===================== Feature effectiveness (univariate) =====================
fprintf('\n=== Univariate feature effectiveness (Train/Test, Dead positive) ===\n');
[featStatsTrain] = local_feature_effectiveness(trainingFeatureTable, predictorNames, responseName, 'Train');
[featStatsTest]  = local_feature_effectiveness(testingFeatureTable,  predictorNames, responseName, 'Test');
% Print Top-20 (sorted by Test AUC desc)
try
    Tdisp = sortrows(featStatsTest, 'AUC', 'descend');
    Tdisp = Tdisp(1:min(20,height(Tdisp)), {'Feature','AUC','CohensD','SpearmanR','p_ranksum'});
    disp(Tdisp);
catch
end
% Save CSVs for tidy-data inspection
try
    writetable(featStatsTrain, fullfile(resultsDir,'feature_effectiveness_train_v3.csv'));
catch
end
try
    writetable(featStatsTest, fullfile(resultsDir,'feature_effectiveness_test_v3.csv'));
catch
end

% ===================== Low-discriminative feature filtering (train-based) =====================
if isfield(modelOptions,'filterLowDiscriminative') && modelOptions.filterLowDiscriminative
    try
        [keepNames_auto, dropNames_auto, dropTable_auto] = local_filter_low_discriminative(featStatsTrain, modelOptions);
        pred_before = predictorNames; %#ok<NASGU>
        predictorNames = predictorNames(ismember(predictorNames, keepNames_auto));
        fprintf('Low-disc filter: removed %d/%d features before MRMR.\n', numel(setdiff(pred_before, predictorNames)), numel(pred_before));
        try
            writetable(dropTable_auto, fullfile(resultsDir,'low_disc_features_v3.csv'));
        catch
        end
catch ME
        fprintf('Low-disc filtering failed (skip): %s\n', ME.message);
    end
end

% ===================== Feature selection via MRMR + CV =====================
if isfield(modelOptions,'enableFeatureSelection') && modelOptions.enableFeatureSelection
    try
        [selectedPredictorNames, mrmrRankingTable, bestK] = local_select_features_mrmr(trainingFeatureTable, predictorNames, responseName, modelOptions);
        fprintf('Selected top-%d features by MRMR+CV.\n', bestK);
    % Correlation de-redundancy (respect MRMR ranking order)
        if isfield(modelOptions,'maxFeatureCorrelation') && isfinite(modelOptions.maxFeatureCorrelation)
            predictorNames_filtered = local_filter_correlated_features(trainingFeatureTable, selectedPredictorNames, modelOptions.maxFeatureCorrelation);
            fprintf('After correlation filtering (thr=%.2f): %d -> %d features.\n', modelOptions.maxFeatureCorrelation, numel(selectedPredictorNames), numel(predictorNames_filtered));
            predictorNames = predictorNames_filtered; %#ok<NASGU>
        else
            predictorNames = selectedPredictorNames; %#ok<NASGU>
        end
        try
            % Save ranking
            save(fullfile(resultsDir,'mrmr_feature_ranking_v3.mat'), 'mrmrRankingTable', 'bestK');
        catch
        end
catch ME
        fprintf('Feature selection failed (use all predictors): %s\n', ME.message);
    end
end

% ===================== Train model (RUSBoost + cost + threshold) =====================
[trainedClassifier, valAcc] = local_train_model(trainingFeatureTable, predictorNames, responseName, modelOptions);
fprintf('Cross-validated accuracy (train-internal): %.2f%%\n', 100*valAcc);

% Threshold tuning (maximize Dead(0)-class F1) and store in Options.pvcThreshold
if modelOptions.enableThresholdMoving
    trainedClassifier = local_threshold_tuning(trainedClassifier, trainingFeatureTable, predictorNames, responseName, modelOptions);
end

% ===================== Evaluation (English confusion matrices) =====================
[train_predVital, ~] = local_predict_with_threshold(trainedClassifier, trainingFeatureTable, predictorNames);
train_yTrue = trainingFeatureTable.(responseName); if iscell(train_yTrue), train_yTrue = cell2mat(train_yTrue); end
local_print_confusion('Train', train_yTrue, train_predVital);

[predVital, scoresDead] = local_predict_with_threshold(trainedClassifier, testingFeatureTable, predictorNames); %#ok<ASGLU>
yTrue = testingFeatureTable.(responseName); if iscell(yTrue), yTrue = cell2mat(yTrue); end
local_print_confusion('Test', yTrue, predVital);

% AUC (Dead positive)
try
    yTrueDead = 1 - yTrue; % 1=Dead
    [~,~,~,aucTest] = perfcurve(yTrueDead, scoresDead, 1);
catch
    aucTest = NaN;
end

% AUPRC and Brier score (Dead positive, using calibrated prob if available)
try
    % If Platt-calibrated, local_predict_with_threshold already applied the threshold to calibrated probs.
    % For metrics here, prefer calibrated probabilities over raw scores:
    scoresProb_eval = scoresDead;
    if isfield(trainedClassifier,'Options') && isfield(trainedClassifier.Options,'calibrationBeta') && numel(trainedClassifier.Options.calibrationBeta)==2
        try
            scoresProb_eval = glmval(trainedClassifier.Options.calibrationBeta, scoresDead, 'logit');
        catch
            scoresProb_eval = scoresDead;
        end
    end
    [recall, precision, ~, aucPR] = perfcurve(yTrueDead, scoresProb_eval, 1, 'XCrit','reca','YCrit','prec'); %#ok<ASGLU>
catch
    aucPR = NaN;
end
try
    brier = mean((scoresProb_eval - yTrueDead).^2);
catch
    brier = NaN;
end
fprintf('Test AUC=%.3f | AUPRC=%.3f | Brier=%.4f\n', aucTest, aucPR, brier);

% ===================== Ablation: demographics-only / physio-only / combined =====================
try
    demoVars = intersect(predictorNames, {'age_s1','gender','race'});
    physVars = setdiff(predictorNames, demoVars);
    sets = {demoVars, physVars, predictorNames};
    setNames = {'demographic_only','physio_only','combined'};
    abRows = strings(0,1);
    AUCs = []; AUPRCs = []; Briers = []; F1ds = [];
    for si = 1:numel(sets)
        varsS = sets{si};
        if isempty(varsS)
            abRows(end+1,1) = string(setNames{si}); %#ok<AGROW>
            AUCs(end+1,1) = NaN; AUPRCs(end+1,1) = NaN; Briers(end+1,1) = NaN; F1ds(end+1,1) = NaN; %#ok<AGROW>
            continue;
        end
    % In ablation, reuse the main model's selected method and hyperparams to avoid re-searching
        abOptions = modelOptions;
        if isfield(modelOptions,'disableAutoInAblation') && modelOptions.disableAutoInAblation
            if isfield(trainedClassifier,'Options')
                abOptions = trainedClassifier.Options;
            end
            if isfield(abOptions,'selectedMethod')
                abOptions.Method = abOptions.selectedMethod; % Force using the selected method
            end
            if ischar(abOptions.Method) || isstring(abOptions.Method)
                if strcmpi(char(abOptions.Method),'auto')
                    abOptions.Method = 'GentleBoost';
                end
            end
            abOptions.autoSelectVerbose = false; % Silent
        end
        [mdlS, ~] = local_train_model(trainingFeatureTable, varsS, responseName, abOptions);
        mdlS = local_threshold_tuning(mdlS, trainingFeatureTable, varsS, responseName, abOptions);
        [yhatS, scoreS] = local_predict_with_threshold(mdlS, testingFeatureTable, varsS);
        yTrueS = testingFeatureTable.(responseName); if iscell(yTrueS), yTrueS = cell2mat(yTrueS); end
        yDeadS = 1 - yTrueS;
        try
            [~,~,~,aucS] = perfcurve(yDeadS, scoreS, 1);
        catch
            aucS = NaN;
        end
        try
            probS = scoreS;
            if isfield(mdlS,'Options') && isfield(mdlS.Options,'calibrationBeta')
                probS = glmval(mdlS.Options.calibrationBeta, scoreS, 'logit');
            end
            [~,~,~,auprcS] = perfcurve(yDeadS, probS, 1, 'XCrit','reca','YCrit','prec');
            brierS = mean((probS - yDeadS).^2);
        catch
            auprcS = NaN; brierS = NaN;
        end
        % F1(Dead)
        yHatDeadS = 1 - yhatS;
        TP = sum((yDeadS==1) & (yHatDeadS==1));
        FP = sum((yDeadS==0) & (yHatDeadS==1));
        FN = sum((yDeadS==1) & (yHatDeadS==0));
        prec = TP / max(TP+FP, eps); rec = TP / max(TP+FN, eps);
        F1d = (prec+rec>0) * (2*prec*rec/(prec+rec));

        abRows(end+1,1) = string(setNames{si}); %#ok<AGROW>
        AUCs(end+1,1) = aucS; AUPRCs(end+1,1) = auprcS; Briers(end+1,1) = brierS; F1ds(end+1,1) = F1d; %#ok<AGROW>
    end
    abTable = table(abRows, AUCs, AUPRCs, Briers, F1ds, 'VariableNames', {'Setting','AUC','AUPRC','Brier','F1_Dead'});
    disp(abTable);
    try
        writetable(abTable, fullfile(resultsDir,'ablation_demographic_physio_combined_v3.csv'));
    catch
    end
catch ME
    fprintf('Ablation failed: %s\n', ME.message);
end

% ===================== Model importance and permutation importance =====================
modelImportance = struct();
try
    imp = predictorImportance(trainedClassifier.ClassificationEnsemble);
    modelImportance.gain = array2table(imp(:)','VariableNames', trainedClassifier.RequiredVariables);
catch
end
try
    thrUsed = trainedClassifier.Options.pvcThreshold;
catch
    thrUsed = 0.5;
end
try
    permImp = local_permutation_importance(trainedClassifier, testingFeatureTable, predictorNames, responseName, thrUsed, 20);
    try
        writetable(permImp, fullfile(resultsDir,'permutation_importance_test_v3.csv'));
    catch
    end
catch
    permImp = table();
end

% ===================== Save model package =====================
trainedModelPackage = struct();
trainedModelPackage.trainedClassifier = trainedClassifier;
trainedModelPackage.requiredVariables = predictorNames;
trainedModelPackage.responseName = responseName;
trainedModelPackage.options = modelOptions;
trainedModelPackage.validationAccuracy = valAcc;
trainedModelPackage.testAUC = aucTest;
trainedModelPackage.testAUPRC = aucPR;
trainedModelPackage.testBrier = brier;
trainedModelPackage.featureEffectivenessTrain = featStatsTrain;
trainedModelPackage.featureEffectivenessTest  = featStatsTest;
trainedModelPackage.modelImportance = modelImportance;
trainedModelPackage.permutationImportanceTest = permImp;
trainedModelPackage.savedAt = datetime("now","Format","yyyy-MM-dd HH:mm:ss");

try
    save(modelOutFile, 'trainedClassifier', 'predictorNames', 'responseName', 'modelOptions', 'trainedModelPackage');
    fprintf('âœ“ Saved model: %s\n', modelOutFile);
catch ME
    fprintf('Failed to save model: %s\n', ME.message);
end

fprintf('\nAll done.\n');

% ===================== Local functions in this file =====================

function Trec = local_build_empty_record(recBase, patientVital, ~, ~)
    R = struct();
    R.record = string(recBase);
    R.patientVital = patientVital;
    R.PVCs_per_hour = 0;
    R.recovery_failure_ratio = 1;
    R.halflife_rr_median = NaN;
    R.halflife_rr30_median = NaN;
    R.halflife_rr10_median = NaN;
    R.tau_rr_median = NaN;
    R.auc_rr_mean = NaN;
    R.oscill_rr_median = NaN;
    R.stalls_rr_mean = NaN;
    R.late_var_rr_median = NaN;
    R.HRT_TO_abnormal_frac = NaN;
    R.HRT_TS_low_frac = NaN;
    R.halflife_tamp_median = NaN;
    R.auc_tamp_mean = NaN;
    R.halflife_qtc_median = NaN;
    R.auc_qtc_mean = NaN;
    R.QRS_Dur_PVC_mean = NaN;
    R.RR_Pre_RMSSD_mean = NaN;
    R.coupling_ratio_mean = NaN;
    R.hr_peak_accel_bpm_mean = NaN;
    % Additional features (keep table schema consistent)
    R.rr_dev_early_mean_med   = NaN;
    R.rr_dev_late_mean_med    = NaN;
    R.rr_dev_late_minus_early = NaN;
    R.rr_dev_slope_0_5s_med   = NaN;
    R.rr_dev_slope_5_15s_med  = NaN;
    R.rr_dev_sampen_med       = NaN;
    R.rr_dev_lzc_med          = NaN;
    R.poincare_ratio_postpre_med = NaN;
    R.rr_dev_q95_med          = NaN;
    R.tamp_rr_corr_med        = NaN;
    R.qtc_overshoot_frac_mean = NaN;
    R.qtc_overshoot_mag_mean  = NaN;
    R.coupling_ratio_cv       = NaN;
    R.pvc_spacing_cv          = NaN;
    R.recovery_rate_hmean     = NaN;
    % Covariate placeholders
    R.nsrrid  = "";
    R.censdate= NaN;
    R.gender  = NaN;
    R.race    = NaN;
    R.age_s1  = NaN;
    Trec = struct2table(R);
end

function [muRR, sigRR, baseRR_vec, muTamp, muQTc] = local_baseline_stats(~, pvcSample, rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, qrs_dur_vec, fs, params)
    baseA = max(1, double(pvcSample) - round(params.baselineSec*fs));
    baseB = max(1, double(pvcSample) - 1);
    j_in = find(rGlobalAll >= baseA & rGlobalAll <= baseB);
    j_in = j_in(:);
    j_in = j_in(j_in>=2);
    if ~isempty(j_in)
        mask_ok = ~isPVCBeat(j_in) & ~isPVCBeat(j_in-1) & sqi_vec(j_in) & sqi_vec(j_in-1) & isfinite(rr_between(j_in));
        baseRR_vec = rr_between(j_in(mask_ok));
    else
        baseRR_vec = [];
    end
    baseRR_vec = baseRR_vec(isfinite(baseRR_vec) & baseRR_vec>=params.minRR & baseRR_vec<=params.maxRR);
    if numel(baseRR_vec) < max(3, params.baselineMinBeats)
        j_all = (2:numel(rGlobalAll)).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & sqi_vec(j_all) & sqi_vec(j_all-1) & isfinite(rr_between(j_all));
        rr_all = rr_between(j_all(mask_all)); rr_all = rr_all(isfinite(rr_all));
        if isempty(rr_all)
            muRR = median(rr_between(isfinite(rr_between)));
            sigRR = std(rr_between(isfinite(rr_between)));
        else
            muRR = median(rr_all);
            sigRR = std(rr_all);
        end
    else
        muRR = mean(baseRR_vec);
        sigRR = std(baseRR_vec);
    end
    if ~isfinite(muRR) || muRR<=0
        muRR = median(rr_between(isfinite(rr_between)));
    end
    if ~isfinite(sigRR), sigRR = 0; end

    % Baseline T amplitude and QTc approximation
    muTamp = NaN; muQTc = NaN;
    try
    % T amplitude
        if ~isempty(j_in)
            j_ok = j_in(~isnan(t_amp_vec(j_in)) & isfinite(t_amp_vec(j_in)));
            t_base = t_amp_vec(j_ok);
            if ~isempty(t_base)
                muTamp = mean(abs(t_base));
            end
        end
    catch
        muTamp = NaN;
    end
    try
    % QTc approximation (consistent with main flow)
        j_ok = j_in;
        keep = false(size(j_ok));
        for mm = 1:numel(j_ok)
            j = j_ok(mm);
            if isfinite(tGlobalIdx(j)) && tGlobalIdx(j) > rGlobalAll(j) && isfinite(rr_between(j)) && rr_between(j)>0
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                if isfinite(QT_approx) && QT_approx>=0.20 && QT_approx<=0.60
                    keep(mm)=true;
                end
            end
        end
        j_ok = j_ok(keep);
        if ~isempty(j_ok)
            qtc_vals = nan(numel(j_ok),1);
            for k = 1:numel(j_ok)
                j = j_ok(k);
                rrj = rr_between(j);
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                qtc_vals(k) = QT_approx / (rrj^(1/3));
            end
            qtc_vals = qtc_vals(isfinite(qtc_vals) & qtc_vals>=0.30 & qtc_vals<=0.70);
            if ~isempty(qtc_vals)
                muQTc = mean(qtc_vals);
            end
        end
    catch
        muQTc = NaN;
    end
end

function ts_val = local_approx_ts(idxPVC, pvcSample, obsEnd, rGlobalAll, rr_between, isPVCBeat, sqi_vec)
    % Approximate TS: max positive slope of any 5-consecutive RRs among beats 5â€“15 after PVC with good SQI and non-PVC
    numBeats = numel(rGlobalAll);
    j_after = (idxPVC+1):min(numBeats, idxPVC+20);
    j_after = j_after(:);
    mask_rr_ok = true(size(j_after));
    mask_rr_ok = mask_rr_ok & (rGlobalAll(j_after) <= obsEnd) & (rGlobalAll(j_after-1) >= pvcSample);
    mask_rr_ok = mask_rr_ok & ~isPVCBeat(j_after) & ~isPVCBeat(j_after-1) & sqi_vec(j_after) & sqi_vec(j_after-1) & isfinite(rr_between(j_after));
    j_after = j_after(mask_rr_ok);
    ts_val = NaN;
    if numel(j_after) >= 7
        rr_seq = rr_between(j_after);
        maxSlope = -inf;
        for uu = 1:(numel(rr_seq)-4)
            slope = (rr_seq(uu+4) - rr_seq(uu)) / 4.0;
            if slope > maxSlope
                maxSlope = slope;
            end
        end
        ts_val = maxSlope;
    end
end

function t50 = local_halflife(dev, t, thr, consecBeats)
    % Find the first time index where consecBeats consecutive points satisfy dev <= thr
    t50 = NaN;
    if isempty(dev) || isempty(t), return; end
    dev = dev(:); t = t(:);
    hit = (dev <= thr);
    if ~any(hit), return; end
    run = 0;
    for i = 1:numel(hit)
        if hit(i), run = run + 1; else, run = 0; end
        if run >= consecBeats
            t50 = t(i - consecBeats + 1); return;
        end
    end
end

function tau = local_time_constant(dev, t, K)
    % Fit ln on the first K points: dev(t) ~ exp(-t/tau)
    tau = NaN;
    if isempty(dev) || isempty(t), return; end
    dev = dev(:); t = t(:);
    K = min([K, numel(dev), numel(t)]);
    y = log(max(dev(1:K), eps));
    x = t(1:K);
    if numel(unique(x)) < 2, return; end
    p = polyfit(x, y, 1); % y ~ a*x + b, tau = -1/a
    a = p(1);
    if a < 0
        tau = -1/a;
    else
        tau = NaN;
    end
end

function A = local_auc(dev, t, normTime)
    % Trapezoidal integral âˆ« dev dt, normalize time by normTime
    A = NaN;
    if numel(dev) < 2 || numel(t) < 2, A = 0; return; end
    dev = dev(:); t = t(:);
    dt = diff(t) ./ max(normTime, 1);
    if any(isnan(dev))
    % Forward-fill missing points
        for i = 2:numel(dev)
            if isnan(dev(i)), dev(i) = dev(i-1); end
        end
        if isnan(dev(1)), dev(1) = 0; end
    end
    A = sum(0.5*(dev(1:end-1)+dev(2:end)).*dt);
    if ~isfinite(A), A = NaN; end
end

function oi = local_oscillation_index(dev)
    % Oscillation index based on sign-change rate of first differences
    oi = NaN;
    if numel(dev) < 3, oi = 0; return; end
    d1 = diff(dev(:));
    s = sign(d1);
    s(s==0) = 1; % Treat zero difference as no change
    chg = sum(abs(diff(s))>0);
    oi = chg / max(numel(d1)-1, 1);
end

function maxSlope = local_max_slope(t, x)
    % Maximum adjacent-point slope (absolute value)
    maxSlope = NaN;
    t = t(:); x = x(:);
    ok = isfinite(t) & isfinite(x);
    t = t(ok); x = x(ok);
    if numel(t) < 2, maxSlope = NaN; return; end
    dt = diff(t); dx = diff(x);
    s = dx ./ max(dt, eps);
    maxSlope = max(abs(s));
end

function cnt = local_recovery_stalls(dev, thr, consecBeats)
    % Number of re-instabilities after reaching threshold (segments under thr minus 1)
    cnt = 0;
    if isempty(dev), return; end
    under = dev(:) <= thr;
    if ~any(under), return; end
    % Compress into run-length representation
    transitions = [true; diff(under)~=0; true];
    % Count entries into the under region
    enterIdx = find([false; (~under(1:end-1) & under(2:end))]);
    % Only count under-runs lasting at least consecBeats consecutively
    i = 1; validSegments = 0;
    while i <= numel(enterIdx)
        startPos = enterIdx(i) + 1;
        len = 0; j = startPos;
        while j <= numel(under) && under(j)
            len = len + 1; j = j + 1;
        end
        if len >= consecBeats
            validSegments = validSegments + 1;
        end
        i = i + 1;
    end
    cnt = max(validSegments - 1, 0);
end

function qtc_seq = local_qtc_series(j_after, rGlobalAll, tGlobalIdx, qrs_dur_vec, rr_between, fs)
    qtc = nan(numel(j_after),1);
    for i = 1:numel(j_after)
        j = j_after(i);
        if isfinite(tGlobalIdx(j)) && tGlobalIdx(j) > rGlobalAll(j) && isfinite(rr_between(j)) && rr_between(j)>0
            QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
            if isfinite(QT_approx) && QT_approx>=0.20 && QT_approx<=0.60
                qtc(i) = QT_approx / (rr_between(j)^(1/3));
            end
        end
    end
    qtc_seq = qtc(isfinite(qtc));
end

function rmssd_vec = local_rmssd_from_base_perPVC(idxPVC_all_sorted, pvcRidx, rGlobalAll, rr_between, isPVCBeat, sqi_vec, fs, params)
    numPVC = numel(idxPVC_all_sorted);
    rmssd_vec = nan(numPVC,1);
    for kk = 1:numPVC
        pvcSample = pvcRidx(kk);
        baseA = max(1, double(pvcSample) - round(params.baselineSec*fs));
        baseB = max(1, double(pvcSample) - 1);
        j_in = find(rGlobalAll >= baseA & rGlobalAll <= baseB);
        j_in = j_in(:);
        j_in = j_in(j_in>=2);
        if ~isempty(j_in)
            mask_ok = ~isPVCBeat(j_in) & ~isPVCBeat(j_in-1) & sqi_vec(j_in) & sqi_vec(j_in-1) & isfinite(rr_between(j_in));
            rr_base = rr_between(j_in(mask_ok));
            rr_base = rr_base(isfinite(rr_base));
            if numel(rr_base) >= 2
                d = diff(rr_base);
                rmssd_vec(kk) = sqrt(mean(d.^2));
            end
        end
    end
end

function [trainedClassifier, validationAccuracy] = local_train_model(trainingTable, predictorNames, responseName, modelOptions)
    predictors = trainingTable(:, predictorNames);
    response = trainingTable.(responseName); if iscell(response), response = cell2mat(response); end

    % Compute sub-feature sampling parameter
    numVars = modelOptions.NumVariablesToSample;
    if ischar(numVars) || isstring(numVars)
        numVarsStr = char(numVars);
        if strcmpi(numVarsStr,'all')
            numVarsArgGlobal = 'all';
        elseif strcmpi(numVarsStr,'sqrt')
            p = width(predictors); numVarsArgGlobal = max(1, floor(sqrt(max(1,double(p)))));
        else
            numVarsArgGlobal = 'all';
        end
    else
        numVarsArgGlobal = max(1, round(double(numVars)));
    end

    % If Method is not 'auto', train with the specified method (omit LearnRate for Bag)
    if isfield(modelOptions,'Method') && ~strcmpi(char(modelOptions.Method), 'auto')
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits', maxSplits, 'NumVariablesToSample', numVarsArgGlobal, 'MinLeafSize', modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits', maxSplits, 'NumVariablesToSample', numVarsArgGlobal);
        end
        fitArgs = {predictors, response, 'Method', modelOptions.Method, 'NumLearningCycles', modelOptions.NumLearningCycles, 'Learners', template};
        if ~strcmpi(char(modelOptions.Method), 'bag') && isfield(modelOptions,'LearnRate')
            fitArgs = [fitArgs, {'LearnRate', modelOptions.LearnRate}]; %#ok<AGROW>
        end
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost', C}]; %#ok<AGROW>
        end
        cls = fitcensemble(fitArgs{:});
        try
            cv = crossval(cls, 'KFold', modelOptions.cvKFold);
            validationAccuracy = 1 - kfoldLoss(cv);
        catch
            validationAccuracy = NaN;
        end
        trainedClassifier = struct();
        trainedClassifier.ClassificationEnsemble = cls;
        trainedClassifier.RequiredVariables = predictorNames;
        trainedClassifier.Options = modelOptions;
        return;
    end

    % =============== Automatic method/parameter search (maximize F1 of Dead class) ===============
    if ~isfield(modelOptions, 'candidateMethods') || isempty(modelOptions.candidateMethods)
        modelOptions.candidateMethods = {'LogitBoost','GentleBoost','Bag'};
    end
    if ~isfield(modelOptions, 'searchGrid') || isempty(modelOptions.searchGrid)
        modelOptions.searchGrid = struct();
    end
    % Grid defaults
    if ~isfield(modelOptions.searchGrid,'NumLearningCycles'),    modelOptions.searchGrid.NumLearningCycles = [200, 300]; end
    if ~isfield(modelOptions.searchGrid,'LearnRate'),            modelOptions.searchGrid.LearnRate = [0.03, 0.05, 0.10]; end
    if ~isfield(modelOptions.searchGrid,'MaxNumSplits'),         modelOptions.searchGrid.MaxNumSplits = [40, 60, 100]; end
    if ~isfield(modelOptions.searchGrid,'MinLeafSize'),          modelOptions.searchGrid.MinLeafSize = [8, 12, 20]; end
    if ~isfield(modelOptions.searchGrid,'NumVariablesToSample'), modelOptions.searchGrid.NumVariablesToSample = {'sqrt'}; end

    bestF1 = -inf; bestInfo = struct(); bestCls = []; bestValAcc = NaN;
    verbose = isfield(modelOptions,'autoSelectVerbose') && modelOptions.autoSelectVerbose;
    % Precisely count total combinations (per method, include/exclude LearnRate accordingly)
    totalComb = 0;
    for miCnt = 1:numel(modelOptions.candidateMethods)
        mth = char(modelOptions.candidateMethods{miCnt});
        base = numel(modelOptions.searchGrid.NumLearningCycles) * numel(modelOptions.searchGrid.MaxNumSplits) * numel(modelOptions.searchGrid.MinLeafSize) * numel(modelOptions.searchGrid.NumVariablesToSample);
        if ~strcmpi(mth,'bag')
            base = base * numel(modelOptions.searchGrid.LearnRate);
        end
        totalComb = totalComb + base;
    end
    tried = 0;
    for mi = 1:numel(modelOptions.candidateMethods)
        methodName = char(modelOptions.candidateMethods{mi});
        for nlc = modelOptions.searchGrid.NumLearningCycles
            for mns = modelOptions.searchGrid.MaxNumSplits
                for mls = modelOptions.searchGrid.MinLeafSize
                    for nvs = 1:numel(modelOptions.searchGrid.NumVariablesToSample)
                        nvsVal = modelOptions.searchGrid.NumVariablesToSample{nvs};
                        % Compute NumVariablesToSample argument
                        if ischar(nvsVal) || isstring(nvsVal)
                            nvsStr = char(nvsVal);
                            if strcmpi(nvsStr,'all')
                                numVarsArg = 'all';
                            elseif strcmpi(nvsStr,'sqrt')
                                p = width(predictors); numVarsArg = max(1, floor(sqrt(max(1,double(p)))));
                            else
                                numVarsArg = 'all';
                            end
                        else
                            numVarsArg = max(1, round(double(nvsVal)));
                        end
                        % Tree template
                        if ~isempty(mls)
                            template = templateTree('MaxNumSplits', mns, 'NumVariablesToSample', numVarsArg, 'MinLeafSize', mls);
                        else
                            template = templateTree('MaxNumSplits', mns, 'NumVariablesToSample', numVarsArg);
                        end
                        % Assemble fit arguments
                        fitArgs = {predictors, response, 'Method', methodName, 'NumLearningCycles', nlc, 'Learners', template};
                        if ~strcmpi(methodName,'bag')
                            for lr = modelOptions.searchGrid.LearnRate
                                fitArgs2 = fitArgs;
                                fitArgs2 = [fitArgs2, {'LearnRate', lr}]; %#ok<AGROW>
                                if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
                                    C = [0, modelOptions.costFP; modelOptions.costFN, 0];
                                    fitArgs2 = [fitArgs2, {'Cost', C}]; %#ok<AGROW>
                                end
                                tried = tried + 1;
                                if verbose
                                    fprintf('  [Auto %d/%d] Method=%s | NLC=%d | LR=%.3f | MaxSplits=%d | MinLeaf=%d | NumVars=%s\n', tried, totalComb, methodName, nlc, lr, mns, mls, mat2str(numVarsArg));
                                end
                                try
                                    cls = fitcensemble(fitArgs2{:});
                                catch
                                    continue; % Some combinations may not be supported
                                end
                                % CV evaluation of F1(Dead) (with Platt calibration and threshold scan)
                                try
                                    cvMdl = crossval(cls, 'KFold', modelOptions.cvKFold);
                                    valAcc = 1 - kfoldLoss(cvMdl);
                                    [~, cvScores] = kfoldPredict(cvMdl);
                                    try
                                        clsNames = cvMdl.ClassNames;
                                    catch
                                        clsNames = [0;1];
                                    end
                                    if iscell(clsNames)
                                        try
                                            clsNames = cell2mat(clsNames);
                                        catch
                                            clsNames = [0;1];
                                        end
                                    end
                                    col0 = find(clsNames==0, 1, 'first'); if isempty(col0), col0 = 1; end
                                    scoresDead = cvScores(:, col0);
                                    yDead = 1 - response;
                                    % Platt calibration
                                    try
                                        beta = glmfit(scoresDead, yDead, 'binomial', 'link','logit');
                                        scoresProb = glmval(beta, scoresDead, 'logit');
                                    catch
                                        scoresProb = scoresDead;
                                    end
                                    % Threshold scan
                                    bestF1Local = -inf; bestThrLocal = 0.5;
                                    for thr = modelOptions.thrCandidates
                                        f1 = local_f1_dead_from_scores(yDead, scoresProb, thr);
                                        if f1 > bestF1Local, bestF1Local = f1; bestThrLocal = thr; end
                                    end
                                    if bestF1Local > bestF1
                                        bestF1 = bestF1Local; bestCls = cls; bestValAcc = valAcc;
                                        bestInfo = struct('Method', methodName, 'NumLearningCycles', nlc, 'LearnRate', lr, 'MaxNumSplits', mns, 'MinLeafSize', mls, 'NumVariablesToSample', nvsVal, 'BestThr', bestThrLocal);
                                    end
                                catch
                                end
                            end
                        else
                            % Bagging does not support LearnRate
                            if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
                                C = [0, modelOptions.costFP; modelOptions.costFN, 0];
                                fitArgs = [fitArgs, {'Cost', C}]; %#ok<AGROW>
                            end
                            tried = tried + 1;
                            if verbose
                                fprintf('  [Auto %d/%d] Method=%s | NLC=%d | MaxSplits=%d | MinLeaf=%d | NumVars=%s\n', tried, totalComb, methodName, nlc, mns, mls, mat2str(numVarsArg));
                            end
                            try
                                cls = fitcensemble(fitArgs{:});
                            catch
                                continue;
                            end
                            try
                                cvMdl = crossval(cls, 'KFold', modelOptions.cvKFold);
                                valAcc = 1 - kfoldLoss(cvMdl);
                                [~, cvScores] = kfoldPredict(cvMdl);
                                try
                                    clsNames = cvMdl.ClassNames;
                                catch
                                    clsNames = [0;1];
                                end
                                if iscell(clsNames)
                                    try
                                        clsNames = cell2mat(clsNames);
                                    catch
                                        clsNames = [0;1];
                                    end
                                end
                                col0 = find(clsNames==0, 1, 'first'); if isempty(col0), col0 = 1; end
                                scoresDead = cvScores(:, col0);
                                yDead = 1 - response;
                                try
                                    beta = glmfit(scoresDead, yDead, 'binomial', 'link','logit');
                                    scoresProb = glmval(beta, scoresDead, 'logit');
                                catch
                                    scoresProb = scoresDead;
                                end
                                bestF1Local = -inf; bestThrLocal = 0.5;
                                for thr = modelOptions.thrCandidates
                                    f1 = local_f1_dead_from_scores(yDead, scoresProb, thr);
                                    if f1 > bestF1Local, bestF1Local = f1; bestThrLocal = thr; end
                                end
                                if bestF1Local > bestF1
                                    bestF1 = bestF1Local; bestCls = cls; bestValAcc = valAcc;
                                    bestInfo = struct('Method', methodName, 'NumLearningCycles', nlc, 'LearnRate', NaN, 'MaxNumSplits', mns, 'MinLeafSize', mls, 'NumVariablesToSample', nvsVal, 'BestThr', bestThrLocal);
                                end
                            catch
                            end
                        end
                    end
                end
            end
        end
    end

    if verbose && ~isempty(fieldnames(bestInfo))
        try
            fprintf('Auto-selected: Method=%s | NLC=%d | LR=%s | MaxSplits=%d | MinLeaf=%d | NumVars=%s | CV-F1(Dead)=%.3f\n', string(bestInfo.Method), bestInfo.NumLearningCycles, mat2str(bestInfo.LearnRate), bestInfo.MaxNumSplits, bestInfo.MinLeafSize, mat2str(bestInfo.NumVariablesToSample), bestF1);
        catch
        end
    end

    % Fallback: if auto search fails, use default GentleBoost
    if isempty(bestCls)
        if verbose, fprintf('Auto-selection failed or no model succeeded. Fallback to GentleBoost default.\n'); end
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits', maxSplits, 'NumVariablesToSample', numVarsArgGlobal, 'MinLeafSize', modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits', maxSplits, 'NumVariablesToSample', numVarsArgGlobal);
        end
        fitArgs = {predictors, response, 'Method', 'GentleBoost', 'NumLearningCycles', modelOptions.NumLearningCycles, 'Learners', template, 'LearnRate', modelOptions.LearnRate};
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost', C}]; %#ok<AGROW>
        end
        bestCls = fitcensemble(fitArgs{:});
        try
            cv = crossval(bestCls, 'KFold', modelOptions.cvKFold);
            bestValAcc = 1 - kfoldLoss(cv);
        catch
            bestValAcc = NaN;
        end
        minLeafVal = [];
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            minLeafVal = modelOptions.MinLeafSize;
        end
        bestInfo = struct('Method','GentleBoost','NumLearningCycles',modelOptions.NumLearningCycles,'LearnRate',modelOptions.LearnRate,'MaxNumSplits',maxSplits,'MinLeafSize',minLeafVal,'NumVariablesToSample',numVars,'BestThr',0.5);
    end

    trainedClassifier = struct();
    trainedClassifier.ClassificationEnsemble = bestCls;
    trainedClassifier.RequiredVariables = predictorNames;
    % Record the selected method and parameters
    modelOptions.selectedMethod = string(bestInfo.Method);
    modelOptions.Method = string(bestInfo.Method);
    modelOptions.NumLearningCycles = double(bestInfo.NumLearningCycles);
    if ~isnan(bestInfo.LearnRate)
        modelOptions.LearnRate = double(bestInfo.LearnRate);
    end
    modelOptions.MaxNumSplits = double(bestInfo.MaxNumSplits);
    modelOptions.MinLeafSize  = double(bestInfo.MinLeafSize);
    modelOptions.NumVariablesToSample = bestInfo.NumVariablesToSample;
    trainedClassifier.Options = modelOptions;
    validationAccuracy = bestValAcc;
end

function model = local_threshold_tuning(model, Ttrain, predictorNames, responseName, modelOptions)
    % On the training set, perform K-fold CV and scan thresholds to maximize F1 of the Dead (0) class
    rng(0);
    try
        cvMdl = crossval(model.ClassificationEnsemble, 'KFold', modelOptions.cvKFold);
        [~, cvScores] = kfoldPredict(cvMdl);
        try
            cls = cvMdl.ClassNames;
        catch
            cls = [0;1];
        end
        if iscell(cls)
            try
                cls = cell2mat(cls);
            catch
                cls = [0;1];
            end
        end
        col0 = find(cls==0, 1, 'first'); if isempty(col0), col0 = 1; end
        scoresDead = cvScores(:, col0);
    catch
    [~, scoresDead] = local_predict_with_threshold(model, Ttrain, predictorNames); % Fallback: training prediction scores
    end
    yTrain = Ttrain.(responseName); if iscell(yTrain), yTrain = cell2mat(yTrain); end
    yDead = 1 - yTrain; % 1=Dead
    % Platt probability calibration (logistic), fit on CV scores
    calibBeta = [];
    try
        calibBeta = glmfit(scoresDead, yDead, 'binomial', 'link','logit');
        scoresProb = glmval(calibBeta, scoresDead, 'logit');
    catch
        scoresProb = scoresDead;
        calibBeta = [];
    end

    bestThr = 0.5; bestF1 = -inf;
    for thr = modelOptions.thrCandidates
        yHatDead = double(scoresProb >= thr);
        TP = sum((yHatDead==1) & (yDead==1));
        FP = sum((yHatDead==1) & (yDead==0));
        FN = sum((yHatDead==0) & (yDead==1));
        prec = TP / max(TP + FP, eps);
        rec  = TP / max(TP + FN, eps);
        if (prec + rec) > 0
            F1 = 2 * prec * rec / (prec + rec);
        else
            F1 = 0;
        end
        if F1 > bestF1
            bestF1 = F1; bestThr = thr;
        end
    end
    model.Options.pvcThreshold = bestThr;
    if ~isempty(calibBeta)
        model.Options.calibrationBeta = calibBeta;
        fprintf('Applied Platt calibration: beta=[%.4f, %.4f].\n', calibBeta(1), calibBeta(2));
    end
    fprintf('Best threshold (CV, maximize F1[Dead]): thr=%.2f, F1=%.3f\n', bestThr, bestF1);
end

function [predVital, scoresDead] = local_predict_with_threshold(model, T, predictorNames)
    X = T(:, predictorNames);
    try
        [~, rawScores] = predict(model.ClassificationEnsemble, X);
    catch
        [~, rawScores] = predict(model.ClassificationEnsemble, X{:,:});
    end
    try
        cls = model.ClassificationEnsemble.ClassNames;
    catch
        cls = [0;1];
    end
    if iscell(cls)
        try
            cls = cell2mat(cls);
        catch
            cls = [0;1];
        end
    end
    col0 = find(cls==0, 1, 'first'); if isempty(col0), col0 = 1; end
    scoresDead = rawScores(:, col0);
    % Probability calibration (if present)
    scoresProb = scoresDead;
    if isfield(model,'Options') && isfield(model.Options,'calibrationBeta') && numel(model.Options.calibrationBeta)==2 && all(isfinite(model.Options.calibrationBeta))
        try
            scoresProb = glmval(model.Options.calibrationBeta, scoresDead, 'logit');
        catch
            scoresProb = scoresDead;
        end
    end
    thr = 0.5;
    if isfield(model,'Options') && isfield(model.Options,'enableThresholdMoving') && model.Options.enableThresholdMoving
        if isfield(model.Options,'pvcThreshold') && isfinite(model.Options.pvcThreshold)
            thr = model.Options.pvcThreshold;
        end
    end
    yHatDead = double(scoresProb >= thr);
    predVital = 1 - yHatDead; % 1=Alive, 0=Dead
end

function local_print_confusion(tag, yTrueVital, yHatVital)
    yTrueVital = double(yTrueVital(:));
    yHatVital = double(yHatVital(:));
    yTrueDead = 1 - yTrueVital; % 1=Dead
    yHatDead  = 1 - yHatVital;  % 1=Pred Dead
    TP = sum((yTrueDead==1) & (yHatDead==1));
    FN = sum((yTrueDead==1) & (yHatDead==0));
    FP = sum((yTrueDead==0) & (yHatDead==1));
    TN = sum((yTrueDead==0) & (yHatDead==0));
    acc = mean(yTrueVital == yHatVital);
    if TP+FN>0, sens = TP/(TP+FN); else, sens = NaN; end % Dead recall
    if TN+FP>0, spec = TN/(TN+FP); else, spec = NaN; end % Alive specificity
    if (TP+FP)>0, prec = TP/(TP+FP); else, prec = NaN; end % Dead precision
    if (prec+sens)>0, f1 = 2*prec*sens/(prec+sens); else, f1 = NaN; end

    fprintf('\n--- %s Confusion Matrix (English) ---\n', tag);
    hdr = sprintf('%-14s%12s%12s', 'Actual\\Pred', 'Alive', 'Dead');
    fprintf('%s\n', hdr);
    fprintf('%s\n', repmat('-',1,length(hdr)));
    fprintf('%-14s%12d%12d\n', 'Alive', TN, FP);
    fprintf('%-14s%12d%12d\n', 'Dead',  FN, TP);
    fprintf('Accuracy: %.2f%% | Recall(Dead): %.2f%% | Specificity(Alive): %.2f%% | Precision(Dead): %.2f%% | F1(Dead): %.3f\n', 100*acc, 100*sens, 100*spec, 100*prec, f1);
    try
        figure('Name',[tag ' Confusion Matrix'],'NumberTitle','off');
        confMat = [TN, FP; FN, TP];
        confusionchart(confMat, {'Alive','Dead'}, 'RowSummary','row-normalized','ColumnSummary','column-normalized');
        title([tag ' Confusion Matrix'])
    catch
    end
end


function [keepNames, dropNames, dropTable] = local_filter_low_discriminative(featStatsTrain, modelOptions)
    % Filter features "weaker than thresholds" based on train-set univariate stats
    AUC = featStatsTrain.AUC; D = featStatsTrain.CohensD; P = featStatsTrain.p_ranksum; MI = featStatsTrain.MI_bits;
    F  = string(featStatsTrain.Feature);
    if isempty(AUC) || isempty(F)
        keepNames = F; dropNames = strings(0,1); dropTable = table(); return;
    end
    aucMargin = modelOptions.filter_auc_margin;    % |AUC-0.5| >= aucMargin
    dMinAbs   = modelOptions.filter_min_abs_d;     % |d| >= dMinAbs
    miMin     = modelOptions.filter_min_mi_bits;   % MI >= miMin
    pMax      = modelOptions.filter_max_p;         % p <= pMax (significant)

    good_auc = abs(AUC - 0.5) >= aucMargin;
    good_d   = abs(D) >= dMinAbs;
    good_mi  = MI >= miMin;
    good_p   = P <= pMax;
    good_any = good_auc | good_d | good_mi | good_p; % Keep if any condition holds (loose)

    keepMask = good_any;
    keepNames = F(keepMask);
    dropNames = F(~keepMask);
    dropTable = table(dropNames, AUC(~keepMask), D(~keepMask), MI(~keepMask), P(~keepMask), 'VariableNames', {'Feature','AUC','CohensD','MI_bits','p_ranksum'});
end

function [trainMask, testMask] = local_group_stratified_split(groupIds, y, testRatio, seed)
    % Group-stratified: ensure a group's samples appear only in train or test
    rng(seed);
    groupIds = string(groupIds(:));
    y = double(y(:));
    ug = unique(groupIds);
    % Count class distribution for each group
    G = struct('id', [], 'n0', 0, 'n1', 0, 'idx', []);
    G = repmat(G, numel(ug), 1);
    for i = 1:numel(ug)
        gid = ug(i);
        idx = find(groupIds==gid);
        yi = y(idx);
        G(i).id = gid;
        G(i).n0 = sum(yi==0);
        G(i).n1 = sum(yi==1);
        G(i).idx = idx;
    end
    % Greedy assign to test set to match class ratios
    target0 = sum(y==0) * testRatio;
    target1 = sum(y==1) * testRatio;
    cur0 = 0; cur1 = 0;
    ord = randperm(numel(G));
    testGroups = false(numel(G),1);
    for k = ord
        g = G(k);
    % If both under target, prefer adding the group that improves closeness to targets
        gain0 = abs((cur0+g.n0) - target0) - abs(cur0 - target0);
        gain1 = abs((cur1+g.n1) - target1) - abs(cur1 - target1);
        if (cur0 < target0 || cur1 < target1) && ((gain0 <= 0) || (gain1 <= 0))
            testGroups(k) = true; cur0 = cur0 + g.n0; cur1 = cur1 + g.n1; %#ok<AGROW>
        end
    end
    % If still insufficient, randomly fill to approach targets
    k = 1;
    while (cur0 < target0 || cur1 < target1) && k<=numel(G)
        if ~testGroups(k)
            g = G(k); testGroups(k)=true; cur0=cur0+g.n0; cur1=cur1+g.n1; %#ok<AGROW>
        end
        k = k + 1;
    end
    testMask = false(numel(y),1);
    for i = 1:numel(G)
        if testGroups(i)
            testMask(G(i).idx) = true;
        end
    end
    trainMask = ~testMask;
    % Ensure both classes appear
    if sum(y(testMask)==1)==0
        idxMove = find(trainMask & (y==1), 1, 'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(testMask)==0)==0
        idxMove = find(trainMask & (y==0), 1, 'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(trainMask)==1)==0
        idxMove = find(testMask & (y==1), 1, 'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
    if sum(y(trainMask)==0)==0
        idxMove = find(testMask & (y==0), 1, 'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
end

function kept = local_filter_correlated_features(T, featList, thr)
    % Add features in order; skip those with Spearman |rho|>=thr to any kept feature
    X = T(:, featList);
    kept = strings(0,1);
    for i = 1:numel(featList)
        f = featList{i};
        accept = true;
        for j = 1:numel(kept)
            g = kept(j);
            try
                r = corr(T.(f), T.(g), 'type','Spearman','rows','pairwise');
            catch
                r = NaN;
            end
            if isfinite(r) && abs(r) >= thr
                accept = false; break;
            end
        end
        if accept
            kept(end+1,1) = string(f); %#ok<AGROW>
        end
    end
end


function slope = local_linear_slope(t, x)
    slope = NaN;
    t = t(:); x = x(:);
    ok = isfinite(t) & isfinite(x);
    t = t(ok); x = x(ok);
    if numel(t) < 2, return; end
    p = polyfit(t, x, 1);
    slope = p(1);
end

function val = local_coeffvar(v)
        v = v(:);
    v = v(isfinite(v));
    if isempty(v), val = NaN; return; end
    mu = mean(v);
    if abs(mu) < eps, val = NaN; return; end
    val = std(v) / abs(mu);
end

function [SD1, SD2] = local_poincare_sd(rr)
    SD1 = NaN; SD2 = NaN;
    rr = rr(:);
    rr = rr(isfinite(rr));
    if numel(rr) < 3, return; end
    d = diff(rr);
    sd_rr = std(rr);
    sd_d  = std(d);
    SD1 = sqrt(0.5) * sd_d;
    tmp = 2*sd_rr^2 - 0.5*sd_d^2;
    if tmp <= 0, SD2 = NaN; else, SD2 = sqrt(tmp); end
end

function s = local_sampen(x, m, r)
    s = NaN;
    x = x(:);
    x = x(isfinite(x));
    n = numel(x);
    if n < m + 2 || ~isfinite(r) || r <= 0
        return;
    end
    % Compute the proportion of template pairs with distance <= r, SampEn(m,r,n) = -ln( A_{m+1} / A_{m} )
    % Simplified, robust implementation (O(n^2)), suitable for short-to-medium sequences
    count_m   = 0;
    count_m1  = 0;
    for i = 1:(n - m)
        Xi = x(i:(i+m-1));
        for j = (i+1):(n - m + 1)
            Xj = x(j:(j+m-1));
            if max(abs(Xi - Xj)) <= r
                count_m = count_m + 1;
                if j <= n - m
                    if abs(x(i+m) - x(j+m)) <= r
                        count_m1 = count_m1 + 1;
    end
end
        end
    end
    end
    if count_m == 0 || count_m1 == 0
        return;
    end
    s = -log(count_m1 / count_m);
end

function c = local_lzc_binary(b)
    % Lempel-Ziv complexity (binary sequence), output normalized value c_norm = c * log2(n) / n
    c = NaN;
    b = b(:);
    b = b(isfinite(b));
    if isempty(b), return; end
    b = double(b ~= 0);
    n = numel(b);
    if n < 2, c = 0; return; end
    % Convert to char to adapt to the classic LZ scanning algorithm
    s = char(b+'0').';
    i = 1; c_raw = 1; l = 1; k = 1; k_max = 1; nlen = length(s);
    while true
        if i + k > nlen || l + k > nlen
            c_raw = c_raw + 1; break;
        end
        if s(i + k) == s(l + k)
            k = k + 1;
            if k > k_max
                k_max = k;
            end
        else
            if k > k_max
                k_max = k;
            end
            if k_max == 1
                c_raw = c_raw + 1; l = l + 1; i = 1; k = 1; k_max = 1;
            else
                i = i + 1;
                if i == l
                    c_raw = c_raw + 1; l = l + k_max; i = 1; k = 1; k_max = 1;
                else
                    k = 1;
        end
    end
end
        if l > nlen
            break;
        end
    end
    c = c_raw * (log2(n)) / n;
end

function [featStats] = local_feature_effectiveness(T, predictorNames, responseName, tag)
    yVital = T.(responseName); if iscell(yVital), yVital = cell2mat(yVital); end
    yDead = 1 - double(yVital(:));
    rows = numel(predictorNames);
    Feature   = strings(rows,1);
    AUC       = nan(rows,1);
    CohensD   = nan(rows,1);
    SpearmanR = nan(rows,1);
    p_ranksum = nan(rows,1);
    MI_bits   = nan(rows,1);
    for i = 1:rows
        name = predictorNames{i}; Feature(i) = string(name);
        x = T.(name); x = double(x(:));
        ok = isfinite(x) & isfinite(yDead);
        x = x(ok); y = yDead(ok);
        if numel(unique(y))<2 || numel(x) < 5
            continue;
        end
    % AUC (direction-agnostic)
        try
            [~,~,~,auc] = perfcurve(y, x, 1);
            if ~isfinite(auc), auc = NaN; end
            if auc < 0.5, auc = 1 - auc; end
            AUC(i) = auc;
    catch
            AUC(i) = NaN;
        end
    % Cohen's d (Dead vs Alive)
        try
            xd = x(y==1); xa = x(y==0);
            md = mean(xd); ma = mean(xa);
            sd_d = std(xd); sd_a = std(xa);
            n_d = numel(xd); n_a = numel(xa);
            s_p = sqrt(((n_d-1)*sd_d^2 + (n_a-1)*sd_a^2) / max(n_d+n_a-2,1));
            if s_p>0
                CohensD(i) = (md - ma) / s_p;
            else
                CohensD(i) = NaN;
            end
        catch
            CohensD(i) = NaN;
        end
    % Spearman correlation
        try
            SpearmanR(i) = corr(x, y, 'type','Spearman','rows','pairwise');
        catch
            SpearmanR(i) = NaN;
        end
    % Rank-sum test
        try
            xd = x(y==1); xa = x(y==0);
            if numel(xd)>=2 && numel(xa)>=2
                p_ranksum(i) = ranksum(xd, xa);
            end
        catch
            p_ranksum(i) = NaN;
        end
    % Mutual information (binned)
        try
            bins = local_quantile_bins(x, 10);
            MI_bits(i) = local_mutual_info_bits(bins, y);
        catch
            MI_bits(i) = NaN;
        end
    end
    featStats = table(Feature, AUC, CohensD, SpearmanR, p_ranksum, MI_bits);
    try
        fprintf('  [%s] median AUC=%.3f | top AUC=%.3f (%s)\n', tag, median(AUC,'omitnan'), max(AUC), string(Feature(AUC==max(AUC))));
    catch
    end
end

function bins = local_quantile_bins(x, numBins)
    x = x(:);
    ok = isfinite(x);
    x = x(ok);
    if isempty(x)
        bins = nan(size(ok)); return;
    end
    edges = quantile(x, linspace(0,1,numBins+1));
    edges(1) = -inf; edges(end) = inf;
    bins_full = nan(numel(ok),1);
    bins_full(ok) = discretize(x, edges);
    bins = bins_full;
end

function mi = local_mutual_info_bits(xBins, yBin)
    % xBins: 1..K bins; yBin: 0/1
    mi = NaN;
    if numel(xBins) ~= numel(yBin), return; end
    ok = isfinite(xBins) & isfinite(yBin);
    xBins = xBins(ok); yBin = yBin(ok);
    if isempty(xBins), mi = NaN; return; end
    K = max(xBins);
    if K<=1, mi = 0; return; end
    N = numel(xBins);
    px = accumarray(xBins, 1, [K,1], @sum, 0) / N;
    py = [sum(yBin==0); sum(yBin==1)] / N;
    pxy = zeros(K,2);
    for k=1:K
        sel = (xBins==k);
        Nk = sum(sel);
        if Nk==0, continue; end
        pxy(k,1) = sum(sel & (yBin==0)) / N;
        pxy(k,2) = sum(sel & (yBin==1)) / N;
    end
    % MI = sum p(x,y) log2( p(x,y) / (p(x)p(y)) )
    mi_val = 0;
    for k=1:K
        for j=1:2
            if pxy(k,j) > 0 && px(k)>0 && py(j)>0
                mi_val = mi_val + pxy(k,j) * log2( pxy(k,j) / (px(k)*py(j)) );
        end
    end
end
    mi = mi_val;
end

function [selectedPredictorNames, rankingTable, bestK] = local_select_features_mrmr(trainingTable, predictorNames, responseName, modelOptions)
    Xtbl = trainingTable(:, predictorNames);
    y = trainingTable.(responseName); if iscell(y), y = cell2mat(y); end
    % MRMR ranking
    try
        [idx, scores] = fscmrmr(Xtbl, y);
    catch ME
        error('fscmrmr failed: %s', ME.message);
    end
    ranking = predictorNames(idx);
    rankingTable = table(string(ranking(:)), scores(:), (1:numel(ranking)).', 'VariableNames', {'Feature','Score','Rank'});
    % CV selection of K (maximize mean F1 of Dead class)
    Klist = modelOptions.featureKList;
    cp = cvpartition(numel(y), 'KFold', modelOptions.cvKFold);
    f1Means = nan(numel(Klist),1);
    for ii = 1:numel(Klist)
        K = min(Klist(ii), numel(ranking));
        featsK = ranking(1:K);
        f1Fold = nan(cp.NumTestSets,1);
        for f = 1:cp.NumTestSets
            tr = training(cp, f); te = test(cp, f);
            TrTbl = trainingTable(tr, :);
            TeTbl = trainingTable(te, :);
            % Train base learner (same as main training; when Method='auto', use GentleBoost for stable K evaluation)
            template = templateTree('MaxNumSplits', modelOptions.MaxNumSplits, 'NumVariablesToSample', max(1, floor(sqrt(numel(featsK)))), 'MinLeafSize', modelOptions.MinLeafSize);
            methodUsed = modelOptions.Method;
            if ischar(methodUsed) || isstring(methodUsed)
                if strcmpi(char(methodUsed), 'auto')
                    methodUsed = 'GentleBoost';
                end
            end
            fitArgs = {TrTbl(:,featsK), TrTbl.(responseName), 'Method', methodUsed, 'NumLearningCycles', modelOptions.NumLearningCycles, 'Learners', template};
            if ~strcmpi(char(methodUsed), 'bag')
                fitArgs = [fitArgs, {'LearnRate', modelOptions.LearnRate}]; %#ok<AGROW>
            end
            if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
                C = [0, modelOptions.costFP; modelOptions.costFN, 0];
                fitArgs = [fitArgs, {'Cost', C}];
            end
            mdl = fitcensemble(fitArgs{:});
            % Optimize threshold on training fold
            [~, scoresTr] = predict(mdl, TrTbl(:,featsK));
            cls = mdl.ClassNames;
            if iscell(cls)
                try
                    cls = cell2mat(cls);
                    catch
                        cls = [0;1];
                    end
                    end
                    col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
            scoresDeadTr = scoresTr(:, col0);
            yTr = TrTbl.(responseName); if iscell(yTr), yTr = cell2mat(yTr); end
            yDeadTr = 1 - yTr;
            bestThr = 0.5; bestF1 = -inf;
            for thr = modelOptions.thrCandidates
                f1 = local_f1_dead_from_scores(yDeadTr, scoresDeadTr, thr);
                if f1 > bestF1, bestF1=f1; bestThr=thr; end
            end
            % Evaluate F1 on validation fold
            [~, scoresTe] = predict(mdl, TeTbl(:,featsK));
            scoresDeadTe = scoresTe(:, col0);
            yTe = TeTbl.(responseName); if iscell(yTe), yTe = cell2mat(yTe); end
            yDeadTe = 1 - yTe;
            f1Fold(f) = local_f1_dead_from_scores(yDeadTe, scoresDeadTe, bestThr);
        end
        f1Means(ii) = mean(f1Fold, 'omitnan');
    end
    [~, idxBest] = max(f1Means);
    bestK = min(Klist(idxBest), numel(ranking));
    selectedPredictorNames = ranking(1:bestK);
end

function f1 = local_f1_dead_from_scores(yDead, scoresDead, thr)
    yHatDead = double(scoresDead >= thr);
    TP = sum((yHatDead==1) & (yDead==1));
    FP = sum((yHatDead==1) & (yDead==0));
    FN = sum((yHatDead==0) & (yDead==1));
    prec = TP / max(TP + FP, eps);
    rec  = TP / max(TP + FN, eps);
    if (prec + rec) > 0
        f1 = 2 * prec * rec / (prec + rec);
    else
        f1 = 0;
    end
end

function permImpTbl = local_permutation_importance(model, Ttest, predictorNames, responseName, thr, nRepeat)
    rng(0);
    % Baseline F1
    [~, scores] = predict(model.ClassificationEnsemble, Ttest(:,predictorNames));
    try
        cls = model.ClassificationEnsemble.ClassNames;
                catch
        cls = [0;1];
                end
    if iscell(cls)
        try
            cls = cell2mat(cls);
        catch
            cls = [0;1];
            end
        end
    col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
    scoresDead = scores(:, col0);
    yVital = Ttest.(responseName); if iscell(yVital), yVital = cell2mat(yVital); end
    yDead = 1 - yVital;
    baseF1 = local_f1_dead_from_scores(yDead, scoresDead, thr);
    % Per-feature permutation
    nF = numel(predictorNames);
    Feature = strings(nF,1);
    DeltaF1_mean = nan(nF,1);
    DeltaF1_std  = nan(nF,1);
    for i = 1:nF
        fname = predictorNames{i}; Feature(i) = string(fname);
        deltas = nan(nRepeat,1);
        for r = 1:nRepeat
            Tperm = Ttest;
            vals = Tperm.(fname);
            idx = randperm(height(Tperm));
            Tperm.(fname) = vals(idx);
            [~, scoresP] = predict(model.ClassificationEnsemble, Tperm(:,predictorNames));
            scoresDeadP = scoresP(:, col0);
            f1p = local_f1_dead_from_scores(yDead, scoresDeadP, thr);
            deltas(r) = baseF1 - f1p;
        end
        DeltaF1_mean(i) = mean(deltas, 'omitnan');
        DeltaF1_std(i)  = std(deltas, 'omitnan');
    end
    permImpTbl = table(Feature, DeltaF1_mean, DeltaF1_std);
    try
        permImpTbl = sortrows(permImpTbl, 'DeltaF1_mean', 'descend');
                catch
                end
end

