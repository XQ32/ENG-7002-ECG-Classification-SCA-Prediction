% =================================================================================================
% File: generate_sca_classifier_v3.m
% Version: v3 (Refactored Layout)
% Purpose:
%   Based on *_info.mat metadata generated by predict_shhs1_all.m (no longer reads EDF raw signals):
%     1) Construct a "variable-window" recovery dynamics feature system for PVC → sinus recovery
%     2) Train an imbalance-friendly binary model (Alive=1 / Dead=0)
%     3) Output feature table, train/test splits, model, evaluation results, and publication-grade figures
%
% Feature focus:
%   - Post-PVC RR / T / approximate QTc recovery half-life and time constant
%   - Overshoot, oscillation index, stage stalls, censoring ratio
%   - Complexity/nonlinearity: sample entropy, LZ complexity, piecewise slopes, Poincaré ratio
%   - PVC coupling, peak heart-rate acceleration, QTc deviation fraction
%
% Input requirements (from *_info.mat):
%   predPVCIndices, patientVital, fs, recordNumSamples, rGlobalAll, isPVCBeat,
%   qrs_dur_vec, r_amp_vec, sqi_vec, t_amp_vec, tGlobalIndices
%
% Outputs:
%   results/post_ectopic_features_v3.mat        Record-level feature table (with NaN)
%   results/SCA_trainingFeatureTable_v3.mat     Training split (missing imputed + normalized)
%   results/SCA_testingFeatureTable_v3.mat      Testing split
%   results/sca_classifier_v3.mat               Trained model package
%   results/feature_effectiveness_*_v3.csv      Univariate discriminative power
%   results/roc / pr / calibration / decision_curve PNGs
%   results/permutation_importance_test_v3.csv  Permutation importance
%   results/ablation_demographic_physio_combined_v3.csv  Ablation study
%
% Main pipeline:
%   1) Parameter configuration and path setup
%   2) Load covariates (optional) + blacklist (optional)
%   3) Enumerate *_info.mat and extract PVC-level features per record → aggregate to record-level
%   4) Grouped stratified split (by patient ID) + missing imputation + MinMax normalization
%   5) Univariate evaluation
%   6) Model training (auto-parameter selection optional) + threshold/probability calibration
%   7) Evaluation and figure export
%   8) Ablation / importance / save model package
%
% Constraints and extensions:
%   - If more precise morphology metrics for QT, ST, P waves, etc. are needed later,
%     add fields during the *_info.mat generation stage
%   - Removed unused legacy functions in this script (local_build_confmat_text / local_build_empty_record, etc.)
%
% Changelog:
%   2025-09-04: v3 initial
%   2025-10-08: Refactor reordering: centralized config, move local functions to end, streamlined comments
% =================================================================================================

clc; clear; close all;

%% ================================================================================================
%  Global configuration (centralized for easy tuning and version tracking)
% ================================================================================================

% Run control
processFirstN          = 0;        % Process first N records (<=0 = all)
splitRatioTest         = 0.20;     % Test split ratio
randomSeed             = 42;       % Random seed
enableBlacklist        = false;    % Whether blacklist is enabled
includeDemographics    = false;    % Whether to include demographic covariates
rng(randomSeed);

% Paths
rootDir   = pwd;
edfDir    = fullfile(rootDir, 'shhs','polysomnography','edfs','shhs1');
resultsDir= fullfile(rootDir, 'results');
if ~isfolder(edfDir), error('EDF (info MAT) directory not found: %s', edfDir); end
if ~isfolder(resultsDir), mkdir(resultsDir); end
addpath(genpath(rootDir));

% Input covariate CSV (optional)
covarCsv = fullfile(rootDir, 'shhs','datasets','shhs-cvd-summary-dataset-0.21.0.csv');

% Recovery/baseline/decision parameters
params = struct();
params.baselineSec        = 50;    % Baseline window lookback before PVC (seconds)
params.baselineMinBeats   = 10;    % Minimum valid sinus beats for baseline
params.maxObsSec          = 50;    % Max observation window after PVC (seconds)
params.consecStableBeats  = 10;    % Consecutive stable beats required to declare recovery
params.rrTolFrac          = 0.08;  % RR deviation threshold: |RR-μRR| <= max(frac*μRR, sigma*σRR)
params.rrTolSigma         = 1.5;
params.hrt_ts_low_thr     = 0.0;   % HRT TS low threshold (smaller = more abnormal)
params.minRR              = 0.30;  % Reasonable RR lower bound (s)
params.maxRR              = 2.50;  % Reasonable RR upper bound (s)
params.minPVCPerRecord    = 10;    % Minimum PVC count per record
params.minSQIRatio        = 0.60;  % Threshold for proportion of good sinus + SQI

% Model options / imbalance & thresholding
modelOptions = struct();
% Best hyperparams from auto-search (15-feature optimized: AdaBoostM1 NLC=250 LR=0.015 MaxSplits=20 MinLeaf=60)
modelOptions.Method                = 'AdaBoostM1';  % Best method from auto-search
modelOptions.NumLearningCycles     = 250;           % Best value (CV-AUPRC=0.402)
modelOptions.LearnRate             = 0.015;         % Best learning rate
modelOptions.MaxNumSplits          = 20;            % Reduce tree complexity to improve generalization (30→20)
modelOptions.MinLeafSize           = 60;            % Stronger leaf regularization (50→60)
modelOptions.NumVariablesToSample  = 'sqrt';        % Maintain randomness
modelOptions.enableCostMatrix      = true;
modelOptions.costFP                = 2;
modelOptions.costFN                = 8;             % Increase FN penalty (was 6)
modelOptions.enableThresholdMoving = true;
modelOptions.pvcThreshold          = 0.22;          % Best threshold (CV F1=0.443)
modelOptions.cvKFold               = 10;
modelOptions.thrCandidates         = 0.02:0.01:0.98;
modelOptions.calibrationMethod     = 'platt';       % Explicitly use Platt calibration
modelOptions.candidateMethods      = {'LogitBoost','GentleBoost','AdaBoostM1','RUSBoost'};
% Narrower/safer search grid (reduce overfitting)
modelOptions.searchGrid = struct();
modelOptions.searchGrid.NumLearningCycles    = [250, 300];
modelOptions.searchGrid.LearnRate            = [0.01, 0.015, 0.02];
modelOptions.searchGrid.MaxNumSplits         = [20, 30, 40];
modelOptions.searchGrid.MinLeafSize          = [50, 60];  % Larger range
modelOptions.searchGrid.NumVariablesToSample = {'sqrt'};
modelOptions.autoSelectVerbose               = true;
modelOptions.disableAutoInAblation           = true;

% Output filenames
featOutFile  = fullfile(resultsDir, 'post_ectopic_features_v3.mat');
trainTblFile = fullfile(resultsDir, 'SCA_trainingFeatureTable_v3.mat');
testTblFile  = fullfile(resultsDir, 'SCA_testingFeatureTable_v3.mat');
modelOutFile = fullfile(resultsDir, 'sca_classifier_v3.mat');

fprintf('=== SCA Risk (v3) Variable-Recovery Feature Pipeline (Refactored) ===\n');
fprintf('Info MAT directory: %s\n', edfDir);

%% ================================================================================================
%  Load covariates (optional)
% ================================================================================================
covarMap = containers.Map('KeyType','char','ValueType','any');
if exist(covarCsv,'file')
    try
        Tcov = readtable(covarCsv);
        if any(strcmpi(Tcov.Properties.VariableNames,'nsrrid'))
            idStr = string(Tcov.nsrrid);
        else
            idStr = string(Tcov{:,1});
        end
        has_gender = any(strcmpi(Tcov.Properties.VariableNames,'gender'));
        has_race   = any(strcmpi(Tcov.Properties.VariableNames,'race'));
        has_age    = any(strcmpi(Tcov.Properties.VariableNames,'age_s1'));
        for ii = 1:height(Tcov)
            key = char(strtrim(idStr(ii)));
            val = struct();
            if has_gender, val.gender = double(Tcov.gender(ii)); else, val.gender = NaN; end
            if has_race,   val.race   = double(Tcov.race(ii));   else, val.race   = NaN; end
            if has_age,    val.age_s1 = double(Tcov.age_s1(ii)); else, val.age_s1 = NaN; end
            covarMap(key) = val;
        end
        fprintf('Loaded covariates (%d rows)\n', height(Tcov));
    catch ME
        fprintf('Covariate load failed: %s\n', ME.message);
    end
else
    fprintf('Covariate CSV not found, skip.\n');
end

%% ================================================================================================
%  Blacklist (optional)
% ================================================================================================
blackListIds = string([]);
blackListNames = string([]);
if enableBlacklist
    blacklistFile = fullfile(resultsDir, 'badsignallist.csv');
    if exist(blacklistFile,'file')
        try
            C = readcell(blacklistFile);
            vals = string(C(:));
            vals = strtrim(replace(replace(vals, '"',''),'''',''));
            vals = vals(vals~="");
            isDigits = ~cellfun('isempty', regexp(cellstr(vals), '^\d+$', 'once'));
            ids = vals(isDigits);
            names = vals(~isDigits);
            recIdFromNames = strings(0,1);
            for ii = 1:numel(names)
                tok = regexp(names(ii),'(\d+)','tokens','once');
                if ~isempty(tok)
                    recIdFromNames(end+1,1) = string(tok{1});
                end
            end
            blackListIds   = unique([ids; recIdFromNames]);
            blackListNames = unique("shhs1-" + blackListIds);
            fprintf('Blacklist loaded: %d entries\n', numel(blackListIds));
        catch ME
            fprintf('Blacklist load failed: %s\n', ME.message);
        end
    else
        fprintf('Blacklist file missing, skip.\n');
    end
else
    fprintf('Blacklist disabled.\n');
end

%% ================================================================================================
%  Enumerate *_info.mat
% ================================================================================================
infoFilesA = dir(fullfile(edfDir, 'shhs1-*_*info.mat'));
infoFilesB = dir(fullfile(edfDir, 'shhs1-*_info.mat'));
allPaths = unique([ ...
    arrayfun(@(d) fullfile(d.folder,d.name), infoFilesA,'UniformOutput',false), ...
    arrayfun(@(d) fullfile(d.folder,d.name), infoFilesB,'UniformOutput',false) ]);
if isempty(allPaths), error('No *_info.mat found under %s', edfDir); end
allPaths = sort(allPaths);
if processFirstN > 0
    allPaths = allPaths(1:min(processFirstN,numel(allPaths)));
end
fprintf('Planned records: %d\n', numel(allPaths));

%% ================================================================================================
%  Iterate records → PVC-level features → aggregate to record-level
% ================================================================================================
allRowTables = cell(0,1);
numSkippedQuality = 0;

requiredFields = {'predPVCIndices','patientVital','fs','recordNumSamples','rGlobalAll', ...
    'isPVCBeat','qrs_dur_vec','r_amp_vec','sqi_vec','t_amp_vec','tGlobalIndices'};

for iFile = 1:numel(allPaths)
    infoPath = allPaths{iFile};
    [~, baseName] = fileparts(infoPath);
    recBase = erase(baseName, '_info');
    recIdTok = regexp(recBase, 'shhs1-(\d+)$','tokens','once');
    recIdStr = "";
    if ~isempty(recIdTok), recIdStr = string(recIdTok{1}); end

    if enableBlacklist && (any(blackListNames==string(recBase)) || (strlength(recIdStr)>0 && any(blackListIds==recIdStr)))
        fprintf('  [%d/%d] Skip blacklist: %s\n', iFile, numel(allPaths), recBase);
        continue;
    end

    fprintf('\n--- [%d/%d] Load %s\n', iFile, numel(allPaths), [baseName '.mat']);
    try
        S = load(infoPath);
    catch ME
        fprintf('  Load failed: %s\n', ME.message);
        continue;
    end

    miss = requiredFields(~cellfun(@(f) isfield(S,f) && ~isempty(S.(f)), requiredFields));
    if ~isempty(miss)
        fprintf('  Skip missing fields: %s\n', strjoin(miss, ', '));
        continue;
    end

    patientVital = double(S.patientVital);
    if ~isfinite(patientVital)
        fprintf('  Skip: patientVital invalid.\n');
        continue;
    end

    fs  = double(S.fs);
    N   = double(S.recordNumSamples);
    rGlobalAll = double(S.rGlobalAll(:));
    isPVCBeat  = logical(S.isPVCBeat(:));
    qrs_dur_vec= double(S.qrs_dur_vec(:));
    r_amp_vec  = double(S.r_amp_vec(:)); %#ok<NASGU>
    sqi_vec    = logical(S.sqi_vec(:));
    t_amp_vec  = double(S.t_amp_vec(:));
    tGlobalIdx = double(S.tGlobalIndices(:));
    if any([numel(isPVCBeat),numel(qrs_dur_vec),numel(t_amp_vec),numel(tGlobalIdx),numel(sqi_vec)] ~= numel(rGlobalAll))
        fprintf('  Skip: vector length mismatch.\n');
        continue;
    end

    pvcRidx = sort(double(S.predPVCIndices(:)));
    pvcRidx = pvcRidx(isfinite(pvcRidx));
    numPVC  = numel(pvcRidx);
    fprintf('  Beats=%d fs=%.1f PVC=%d\n', numel(rGlobalAll), fs, numPVC);
    if numPVC == 0
        fprintf('  Skip quality: PVC=0\n');
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    numBeats = numel(rGlobalAll);
    rr_between = nan(numBeats,1);
    if numBeats>=2
        rr_between(2:end) = diff(rGlobalAll)/fs;
    end

    idxPVC_all_sorted = round(interp1(rGlobalAll, 1:numel(rGlobalAll), pvcRidx, 'nearest','extrap'));
    idxPVC_all_sorted = max(1, min(numBeats, idxPVC_all_sorted));

    % Record-level SQI window quality
    try
        j_all = (2:numBeats).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & isfinite(rr_between(j_all));
        sqi_ratio = mean(double(sqi_vec(j_all(mask_all))), 'omitnan');
    catch
        sqi_ratio = NaN;
    end
    if numPVC < params.minPVCPerRecord || !(isfinite(sqi_ratio) && sqi_ratio>=params.minSQIRatio)
        fprintf('  Skip quality: PVC=%d SQIratio=%.2f (minPVC=%d minSQI=%.2f)\n', ...
            numPVC, sqi_ratio, params.minPVCPerRecord, params.minSQIRatio);
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    % --- PVC-level containers (only intermediates required by the 15 core features) ---
    halflife_rr       = nan(numPVC,1);  % For Feature 6 (CV), Feature 13 (fast recovery ratio)
    halflife_rr_30    = nan(numPVC,1);  % For Feature 5 (early/late ratio)
    halflife_rr_10    = nan(numPVC,1);  % For Feature 5 (early/late ratio), Feature 15 (composite score)
    tau_rr            = nan(numPVC,1);  % For Feature 2 (time constant median)
    oscill_rr         = nan(numPVC,1);  % For Feature 3 (oscillation median), multiple derived features
    hrt_to            = nan(numPVC,1);  % For Feature 4 (HRT abnormal fraction)

    % PVC loop
    for kk = 1:numPVC
        pvcSample = pvcRidx(kk);
        idxPVC    = idxPVC_all_sorted(kk);

        nextPVCsample = inf;
        if kk < numPVC
            nextPVCsample = pvcRidx(kk+1);
        end
        obsEnd = min([double(N), double(pvcSample)+round(params.maxObsSec*fs), double(nextPVCsample)-1]);
        if ~(isfinite(obsEnd) && obsEnd > pvcSample)
            continue;
        end

        [muRR, ~, ~, ~, ~] = local_baseline_stats(idxPVC, pvcSample, ...
            rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, ...
            qrs_dur_vec, fs, params);
        if ~isfinite(muRR) || muRR<=0
            continue;
        end

        % Compute HRT TO (for Feature 4: HRT_TurbOnset_AbnormalFrac)
        rr_pre=nan; rr_post1=nan;
        if idxPVC>=2,          rr_pre   = rr_between(idxPVC); end
        if idxPVC+1<=numBeats, rr_post1 = rr_between(idxPVC+1); end
        if isfinite(rr_pre) && isfinite(rr_post1) && rr_pre>0
            hrt_to(kk) = (rr_post1 - rr_pre) / rr_pre;
        end

        % Post-PVC non-PVC + good-SQI sequence
        j_after = (idxPVC+1):numBeats;
        keep = false(size(j_after));
        for jj = 1:numel(j_after)
            j = j_after(jj);
            if rGlobalAll(j) > obsEnd, break; end
            if j>=2 && ~isPVCBeat(j) && ~isPVCBeat(j-1) && sqi_vec(j) && sqi_vec(j-1) && ...
                    isfinite(rr_between(j)) && rr_between(j)>=params.minRR && rr_between(j)<=params.maxRR
                keep(jj)=true;
            end
        end
        j_after = j_after(keep);
        if isempty(j_after)
            continue;
        end
        t_after = (double(rGlobalAll(j_after)) - double(pvcSample))/fs;
        rr_seq  = rr_between(j_after);
        dev_rr  = abs(rr_seq - muRR)./max(muRR,eps);

        % === Core feature computations (only those required by the 15 features) ===
        % Half-life (used by Features 5, 6, 13, 15)
        halflife_rr(kk)    = local_halflife(dev_rr, t_after, 0.50, params.consecStableBeats);
        halflife_rr_30(kk) = local_halflife(dev_rr, t_after, 0.30, params.consecStableBeats);
        halflife_rr_10(kk) = local_halflife(dev_rr, t_after, 0.10, params.consecStableBeats);

        % Time constant (for Feature 2: RRI_Recovery_TimeConst_Median)
        tau_rr(kk) = local_time_constant(dev_rr, t_after, 8);

        % Oscillation index (for Features 3, 7, 9, 10, 11, 12: multiple oscillation-related features)
        oscill_rr(kk) = local_oscillation_index(dev_rr);
    end % end PVC loop

    % --- Record-level aggregation (15 core features) ---
    recDurationHr = (N/fs)/3600;
    agg_med = @(v) median(v(isfinite(v)));

    R = struct();
    R.record       = string(recBase);
    R.patientVital = patientVital;

    % ========== Feature 1: PVC_FreqPerHour ==========
    % PVC frequency per hour — basic feature
    R.PVC_FreqPerHour = numPVC / max(recDurationHr,eps);

    % ========== Feature 2: RRI_Recovery_TimeConst_Median ==========
    % Median time constant of recovery
    R.RRI_Recovery_TimeConst_Median = agg_med(tau_rr);

    % ========== Feature 3: RRI_PostPVC_OscillAmp_Median ==========
    % Median oscillation amplitude post-PVC
    R.RRI_PostPVC_OscillAmp_Median = agg_med(oscill_rr);

    % ========== Feature 4: HRT_TurbOnset_AbnormalFrac ==========
    % Abnormal fraction of HRT turbulence onset
    R.HRT_TurbOnset_AbnormalFrac = mean(double(isfinite(hrt_to) & (hrt_to>=0)),'omitnan');

    % ========== Half-life–based derived features ==========
    
    % ========== Feature 5: RRI_Recovery_EarlyLate_Ratio ==========
    % Early (10%) vs mid (30%) recovery speed ratio
    hl10_med = agg_med(halflife_rr_10);
    hl30_med = agg_med(halflife_rr_30);
    R.RRI_Recovery_EarlyLate_Ratio = hl10_med / max(hl30_med, eps);
    
    % ========== Feature 6: RRI_Recovery_TimeVariability_CV ==========
    % Coefficient of variation of recovery time (consistency of recovery)
    try
        hl_valid = halflife_rr(isfinite(halflife_rr) & halflife_rr > 0);
        if numel(hl_valid) >= 3
            R.RRI_Recovery_TimeVariability_CV = std(hl_valid) / max(mean(hl_valid), eps);
        else
            R.RRI_Recovery_TimeVariability_CV = NaN;
        end
    catch
        R.RRI_Recovery_TimeVariability_CV = NaN;
    end
    
    % Fast recovery ratio (used by Feature 13)
    try
        hl_valid = halflife_rr(isfinite(halflife_rr));
        fast_recovery_ratio = mean(double(hl_valid <= 10));
    catch
        fast_recovery_ratio = NaN;
    end
    
    % ========== Feature 7: RRI_PostPVC_OscillAmp_Log ==========
    % Log transform of oscillation amplitude
    R.RRI_PostPVC_OscillAmp_Log = log(max(R.RRI_PostPVC_OscillAmp_Median, eps));
    
    % ========== Feature 8: PVC_FreqPerHour_Sqrt ==========
    % Square-root transform of PVC frequency
    R.PVC_FreqPerHour_Sqrt = sqrt(max(R.PVC_FreqPerHour, 0));
    
    % ========== Feature 9: RRI_Oscill_x_PVC_Freq ==========
    % Oscillation amplitude × PVC frequency (synergistic risk)
    R.RRI_Oscill_x_PVC_Freq = R.RRI_PostPVC_OscillAmp_Median * R.PVC_FreqPerHour;
    
    % ========== Feature 10: RRI_Oscill_x_RecoveryCV ==========
    % Oscillation amplitude × recovery variability (composite instability)
    R.RRI_Oscill_x_RecoveryCV = R.RRI_PostPVC_OscillAmp_Median * R.RRI_Recovery_TimeVariability_CV;
    
    % ========== Feature 11: RRI_OscillLog_x_PVC_Freq ==========
    % Log oscillation × PVC frequency
    R.RRI_OscillLog_x_PVC_Freq = R.RRI_PostPVC_OscillAmp_Log * R.PVC_FreqPerHour;
    
    % ========== Feature 12: PVC_Burden_NormIndex ==========
    % Normalized PVC burden index (frequency × oscillation, clipped to 0–2)
    try
        pvc_burden = (R.PVC_FreqPerHour / 50) * (R.RRI_PostPVC_OscillAmp_Median / 0.5);
        R.PVC_Burden_NormIndex = min(pvc_burden, 2.0);
    catch
        R.PVC_Burden_NormIndex = NaN;
    end
    
    % ========== Feature 13: RRI_Recovery_CapacityIndex ==========
    % Recovery capacity index (fast recovery ratio / early-late ratio)
    try
        if isfinite(fast_recovery_ratio) && isfinite(R.RRI_Recovery_EarlyLate_Ratio) && R.RRI_Recovery_EarlyLate_Ratio > 0
            R.RRI_Recovery_CapacityIndex = fast_recovery_ratio / R.RRI_Recovery_EarlyLate_Ratio;
        else
            R.RRI_Recovery_CapacityIndex = NaN;
        end
    catch
        R.RRI_Recovery_CapacityIndex = NaN;
    end
    
    % ========== Feature 14: RRI_Oscill_Freq_LogRatio ==========
    % Ratio of log-oscillation to log-frequency
    try
        pvc_freq_log = log(max(R.PVC_FreqPerHour, 1));
        if isfinite(R.RRI_PostPVC_OscillAmp_Log) && isfinite(pvc_freq_log)
            R.RRI_Oscill_Freq_LogRatio = R.RRI_PostPVC_OscillAmp_Log / max(pvc_freq_log, eps);
        else
            R.RRI_Oscill_Freq_LogRatio = NaN;
        end
    catch
        R.RRI_Oscill_Freq_LogRatio = NaN;
    end
    
    % ========== Feature 15: Clinical_CompositeRisk_Score ==========
    % Composite risk score (based on SHAP weights: oscill(0.40) + pvc_freq(0.30) + halflife10(0.30))
    try
        oscill_norm = min(R.RRI_PostPVC_OscillAmp_Median / 0.5, 1.0);
        pvc_norm    = min(R.PVC_FreqPerHour / 50, 1.0);
        hl10_norm   = min(hl10_med / 30, 1.0);
        R.Clinical_CompositeRisk_Score = 0.40*oscill_norm + 0.30*pvc_norm + 0.30*hl10_norm;
    catch
        R.Clinical_CompositeRisk_Score = NaN;
    end

    % Covariates
    try
        recIdTok = regexp(string(recBase), 'shhs1-(\d+)$', 'tokens','once');
        nsrrid = "";
        if ~isempty(recIdTok), nsrrid = string(recIdTok{1}); end
        R.nsrrid = nsrrid;
        val = [];
        if strlength(nsrrid)>0 && isKey(covarMap, char(nsrrid))
            val = covarMap(char(nsrrid));
        end
        if ~isempty(val)
            R.age_s1 = val.age_s1;
        else
            R.age_s1 = NaN;
        end
    catch
        R.nsrrid = ""; R.age_s1 = NaN;
    end

    allRowTables{end+1,1} = struct2table(R,'AsArray',true);
end

if isempty(allRowTables)
    error('No usable records → featureTable empty.');
end
featureTable = vertcat(allRowTables{:});
fprintf('\nCollected %d records. Skipped (quality)=%d\n', height(featureTable), numSkippedQuality);

% Save raw aggregation (with NaN)
try
    save(featOutFile, 'featureTable');
    fprintf('✓ Saved feature table: %s\n', featOutFile);
catch ME
    fprintf('Save feature table failed: %s\n', ME.message);
end

%% ================================================================================================
%  Missing imputation + grouped stratified split (by patient ID) + normalization
% ================================================================================================
% ===== Feature selection: keep only the 15 most core features (in computation order) =====
% List of 15 core features (in the actual computation order):
coreFeatures = {
    'PVC_FreqPerHour';                   % Feature 1: PVC frequency per hour — basic feature
    'RRI_Recovery_TimeConst_Median';     % Feature 2: Recovery time constant median
    'RRI_PostPVC_OscillAmp_Median';      % Feature 3: Post-PVC oscillation amplitude median
    'HRT_TurbOnset_AbnormalFrac';        % Feature 4: HRT turbulence onset abnormal fraction
    'RRI_Recovery_EarlyLate_Ratio';      % Feature 5: Early/Late recovery ratio
    'RRI_Recovery_TimeVariability_CV';   % Feature 6: CV of recovery time
    'RRI_PostPVC_OscillAmp_Log';         % Feature 7: Log of oscillation amplitude
    'PVC_FreqPerHour_Sqrt';              % Feature 8: Square root of PVC frequency
    'RRI_Oscill_x_PVC_Freq';             % Feature 9: Oscillation × PVC frequency
    'RRI_Oscill_x_RecoveryCV';           % Feature 10: Oscillation × recovery variability
    'RRI_OscillLog_x_PVC_Freq';          % Feature 11: Log-oscillation × PVC frequency
    'PVC_Burden_NormIndex';              % Feature 12: Normalized PVC burden index
    'RRI_Recovery_CapacityIndex';        % Feature 13: Recovery capacity index
    'RRI_Oscill_Freq_LogRatio';          % Feature 14: Log-oscillation/log-frequency ratio
    'Clinical_CompositeRisk_Score'       % Feature 15: Clinical composite risk score
};

names = featureTable.Properties.VariableNames;
isNum = cellfun(@(c) isnumeric(featureTable.(c)) && isvector(featureTable.(c)), names);

if includeDemographics
    excludePredictors = {'patientVital', 'nsrrid', 'record', 'age_s1'};
else
    excludePredictors = {'patientVital', 'nsrrid', 'record', 'age_s1', 'gender', 'race'};
end

% Keep only core features
predictorNames = coreFeatures;
responseName   = 'patientVital';

fprintf('\n=== Feature Selection ===\n');
fprintf('Final core feature count: %d\n', numel(predictorNames));
fprintf('Core feature list:\n');
for i = 1:numel(predictorNames)
    fprintf('  %2d. %s\n', i, predictorNames{i});
end

% Median imputation for missing
for i = 1:numel(predictorNames)
    col = featureTable.(predictorNames{i});
    if ~isnumeric(col), continue; end
    nanMask = isnan(col);
    if any(nanMask)
        medv = median(col(~nanMask));
        if isempty(medv) || isnan(medv), medv = 0; end
        col(nanMask) = medv;
        featureTable.(predictorNames{i}) = col;
    end
end

% Build group (patient ID)
recNames  = string(featureTable.record);
patientId = strings(height(featureTable),1);
for ii = 1:height(featureTable)
    nm = recNames(ii);
    tok = regexp(nm, 'shhs1-(\d+)$','tokens','once');
    if ~isempty(tok)
        patientId(ii) = string(tok{1});
    else
        tok2 = regexp(nm,'(\d+)','tokens','once');
        if ~isempty(tok2)
            patientId(ii) = string(tok2{1});
        else
            patientId(ii) = nm;
        end
    end
end
y = featureTable.(responseName);
if iscell(y), y = cell2mat(y); end
if islogical(y), y = double(y); end

[trainMask, testMask] = local_group_stratified_split(patientId, y, splitRatioTest, randomSeed);
trainingFeatureTable = featureTable(trainMask,:);
testingFeatureTable  = featureTable(testMask,:);
fprintf('Train=%d  Test=%d\n', height(trainingFeatureTable), height(testingFeatureTable));

% Min-Max normalization (based on training set)
try
    scaler = struct();
    scaler.method = 'minmax';
    scaler.featureNames = predictorNames;
    scaler.min = nan(numel(predictorNames),1);
    scaler.max = nan(numel(predictorNames),1);
    for i = 1:numel(predictorNames)
        fn = predictorNames{i};
        tr = double(trainingFeatureTable.(fn));
        mn = min(tr,[],'omitnan');
        mx = max(tr,[],'omitnan');
        if ~isfinite(mn), mn = 0; end
        if ~isfinite(mx), mx = mn + 1; end
        denom = mx - mn;
        if ~(isfinite(denom) && denom>0)
            trScaled = zeros(size(tr));
            teScaled = zeros(size(testingFeatureTable.(fn)));
        else
            trScaled = (tr - mn)./denom;
            te = double(testingFeatureTable.(fn));
            teScaled = (te - mn)./denom;
            trScaled = min(max(trScaled,0),1);
            teScaled = min(max(teScaled,0),1);
        end
        trainingFeatureTable.(fn) = trScaled;
        testingFeatureTable.(fn)  = teScaled;
        scaler.min(i)=mn; scaler.max(i)=mx;
    end
    fprintf('Applied Min-Max normalization.\n');
catch ME
    fprintf('Normalization failed: %s\n', ME.message);
end

% Save training / testing tables
try, save(trainTblFile,'trainingFeatureTable'); fprintf('✓ Saved training table\n'); catch ME, fprintf('Train table save failed: %s\n', ME.message); end
try, save(testTblFile,'testingFeatureTable');   fprintf('✓ Saved testing table\n');  catch ME, fprintf('Test table save failed: %s\n', ME.message); end

%% ================================================================================================
%  Univariate feature effectiveness (AUC / d / MI / rank-sum)
% ================================================================================================
fprintf('\n=== Univariate Feature Effectiveness (Train/Test, Dead Positive) ===\n');
featStatsTrain = local_feature_effectiveness(trainingFeatureTable, predictorNames, responseName, 'Train');
featStatsTest  = local_feature_effectiveness(testingFeatureTable,  predictorNames, responseName, 'Test');

try
    writetable(featStatsTrain, fullfile(resultsDir,'feature_effectiveness_train_v3.csv'));
    writetable(featStatsTest,  fullfile(resultsDir,'feature_effectiveness_test_v3.csv'));
catch
end

%% ================================================================================================
%  Model training + threshold tuning + evaluation
% ================================================================================================
[trainedClassifier, valAcc] = local_train_model(trainingFeatureTable, predictorNames, responseName, modelOptions);
fprintf('Cross-validated accuracy (internal) = %.2f%%\n', 100*valAcc);

if modelOptions.enableThresholdMoving
    trainedClassifier = local_threshold_tuning(trainedClassifier, trainingFeatureTable, predictorNames, responseName, modelOptions);
end

fprintf('\n=== Evaluation (English Confusion Matrices) ===\n');
try
    optsEval = trainedClassifier.Options;
catch
    optsEval = modelOptions;
end
fprintf('Method=%s | NLC=%d | LR=%.3f | MaxSplits=%d | MinLeaf=%d | NumVars=%s\n', ...
    char(optsEval.Method), optsEval.NumLearningCycles, optsEval.LearnRate, ...
    optsEval.MaxNumSplits, optsEval.MinLeafSize, mat2str(optsEval.NumVariablesToSample));

[train_predVital, ~] = local_predict_with_threshold(trainedClassifier, trainingFeatureTable, predictorNames);
train_yTrue = trainingFeatureTable.(responseName); if iscell(train_yTrue), train_yTrue = cell2mat(train_yTrue); end
local_print_confusion('Train', train_yTrue, train_predVital);

[predVital, scoresDead] = local_predict_with_threshold(trainedClassifier, testingFeatureTable, predictorNames);
yTrue = testingFeatureTable.(responseName); if iscell(yTrue), yTrue = cell2mat(yTrue); end
local_print_confusion('Test', yTrue, predVital);

% AUC / AUPRC / Brier (Dead positive)
try
    yTrueDead = 1 - yTrue;
    [~,~,~,aucTest] = perfcurve(yTrueDead, scoresDead, 1);
catch
    aucTest = NaN;
end
try
    scoresProb_eval = scoresDead;
    if isfield(trainedClassifier.Options,'calibrationBeta')
        scoresProb_eval = glmval(trainedClassifier.Options.calibrationBeta, scoresDead, 'logit');
    end
    [~,~,~,aucPR] = perfcurve(yTrueDead, scoresProb_eval, 1, 'XCrit','reca','YCrit','prec');
catch
    aucPR = NaN;
end
try
    brier = mean((scoresProb_eval - yTrueDead).^2);
catch
    brier = NaN;
end
fprintf('Test AUC=%.3f | AUPRC=%.3f | Brier=%.4f\n', aucTest, aucPR, brier);

% Evaluation plots
try
    local_generate_publication_plots(trainedClassifier, trainingFeatureTable, testingFeatureTable, predictorNames, responseName, resultsDir);
catch ME
    fprintf('Plot generation failed: %s\n', ME.message);
end

%% ================================================================================================
%  Ablation (Demographic / Physio / Combined)
% ================================================================================================
try
    demoVars = intersect(predictorNames, {'age_s1','gender','race'});
    physVars = setdiff(predictorNames, demoVars);
    sets     = {demoVars, physVars, predictorNames};
    setNames = {'demographic_only','physio_only','combined'};
    abRows = strings(0,1); AUCs=[]; AUPRCs=[]; Briers=[]; F1ds=[];
    for si = 1:numel(sets)
        varsS = sets{si};
        if isempty(varsS)
            abRows(end+1,1)=string(setNames{si}); AUCs(end+1,1)=NaN; AUPRCs(end+1,1)=NaN; Briers(end+1,1)=NaN; F1ds(end+1,1)=NaN;
            continue;
        end
        abOptions = modelOptions;
        if isfield(modelOptions,'disableAutoInAblation') && modelOptions.disableAutoInAblation
            if isfield(trainedClassifier,'Options')
                abOptions = trainedClassifier.Options;
            end
            if isfield(abOptions,'selectedMethod'), abOptions.Method = abOptions.selectedMethod; end
            if strcmpi(char(abOptions.Method),'auto'), abOptions.Method='GentleBoost'; end
            abOptions.autoSelectVerbose = false;
        end
        [mdlS, ~] = local_train_model(trainingFeatureTable, varsS, responseName, abOptions);
        mdlS = local_threshold_tuning(mdlS, trainingFeatureTable, varsS, responseName, abOptions);
        [yhatS, scoreS] = local_predict_with_threshold(mdlS, testingFeatureTable, varsS);
        yTrueS = testingFeatureTable.(responseName); if iscell(yTrueS), yTrueS = cell2mat(yTrueS); end
        yDeadS = 1 - yTrueS;
        try
            [~,~,~,aucS] = perfcurve(yDeadS, scoreS, 1);
        catch
            aucS = NaN;
        end
        try
            probS = scoreS;
            if isfield(mdlS.Options,'calibrationBeta')
                probS = glmval(mdlS.Options.calibrationBeta, scoreS, 'logit');
            end
            [~,~,~,auprcS] = perfcurve(yDeadS, probS, 1, 'XCrit','reca','YCrit','prec');
            brierS = mean((probS - yDeadS).^2);
        catch
            auprcS = NaN; brierS = NaN;
        end
        yHatDeadS = 1 - yhatS;
        TP = sum((yDeadS==1)&(yHatDeadS==1));
        FP = sum((yDeadS==0)&(yHatDeadS==1));
        FN = sum((yDeadS==1)&(yHatDeadS==0));
        prec = TP/max(TP+FP,eps); rec = TP/max(TP+FN,eps);
        F1d = (prec+rec>0) * (2*prec*rec/(prec+rec));
        abRows(end+1,1)=string(setNames{si});
        AUCs(end+1,1)=aucS; AUPRCs(end+1,1)=auprcS; Briers(end+1,1)=brierS; F1ds(end+1,1)=F1d;
    end
    abTable = table(abRows,AUCs,AUPRCs,Briers,F1ds,'VariableNames',{'Setting','AUC','AUPRC','Brier','F1_Dead'});
    disp(abTable);
    try, writetable(abTable, fullfile(resultsDir,'ablation_demographic_physio_combined_v3.csv')); catch, end
catch ME
    fprintf('Ablation failed: %s\n', ME.message);
end

%% ================================================================================================
%  Feature importance (multi-perspective)
% ================================================================================================
fprintf('\n=== Feature Importance Analysis ===\n');

% --- 1. Model intrinsic importance (Gini gain) ---
modelImportance = struct();
try
    imp = predictorImportance(trainedClassifier.ClassificationEnsemble);
    modelImportance.gain = array2table(imp(:)','VariableNames', trainedClassifier.RequiredVariables);
    fprintf('✓ Computed Gini-based importance\n');
catch ME
    fprintf('Gini importance failed: %s\n', ME.message);
end

% --- 2. SHAP values (Shapley Additive Explanations) ---
shapTable = table();
try
    fprintf('Computing SHAP values (this may take a while)...\n');
    % Use the test set to compute SHAP values
    X_test = testingFeatureTable(:, predictorNames);
    
    % Compute Shapley values (MATLAB R2021a+ supported)
    % Compute per-sample Shapley values, then use mean absolute value as feature importance
    numSamples = min(100, height(testingFeatureTable));  % Limit sample count for speed
    sampledIdx = randperm(height(testingFeatureTable), numSamples);
    X_sampled = testingFeatureTable(sampledIdx, predictorNames);
    
    % Use a custom function to compute SHAP (TreeSHAP for ensemble models)
    [shapValues, shapImportance] = local_compute_shap_treebased(...
        trainedClassifier.ClassificationEnsemble, X_sampled, predictorNames);
    
    modelImportance.shap_values = shapValues;  % Detailed SHAP value matrix
    shapTable = table(string(predictorNames(:)), shapImportance(:), ...
        'VariableNames', {'Feature', 'SHAP_Importance'});
    shapTable = sortrows(shapTable, 'SHAP_Importance', 'descend');
    
    try, writetable(shapTable, fullfile(resultsDir,'shap_importance_v3.csv')); catch, end
    fprintf('✓ Computed SHAP values\n');
catch ME
    fprintf('SHAP computation failed: %s\n', ME.message);
    fprintf('  (Note: SHAP requires MATLAB R2021a+ or custom implementation)\n');
end

% --- 3. Print importance rankings ---
fprintf('\n=== Feature Importance Rankings ===\n\n');

% 3.1 SHAP importance (recommended)
if ~isempty(shapTable) && height(shapTable) > 0
    fprintf('[1] SHAP Importance (Mean |SHAP|):\n');
    fprintf('     (Higher = stronger impact on predictions)\n');
    fprintf('     Top 15 Features:\n');
    for i = 1:min(15, height(shapTable))
        fprintf('       %2d. %-40s  SHAP=%.4f\n', ...
            i, char(shapTable.Feature(i)), ...
            shapTable.SHAP_Importance(i));
    end
end

% 3.2 Model Gain
if isfield(modelImportance, 'gain') && ~isempty(modelImportance.gain)
    fprintf('\n[2] Gini Gain-based Importance:\n');
    fprintf('     (Higher = more splits in decision trees)\n');
    gains = table2array(modelImportance.gain);
    [sortedGains, sortIdx] = sort(gains, 'descend');
    varNames = modelImportance.gain.Properties.VariableNames;
    fprintf('     Top 15 Features:\n');
    for i = 1:min(15, numel(sortIdx))
        idx = sortIdx(i);
        fprintf('       %2d. %-40s  Gain=%.4f\n', ...
            i, char(varNames{idx}), sortedGains(i));
    end
end

% 3.3 Univariate discriminative power (from existing results)
if exist('featStatsTest', 'var') && ~isempty(featStatsTest)
    fprintf('\n[3] Univariate Discriminative Power (Test Set):\n');
    fprintf('     (AUC: 0.5=random, 1.0=perfect)\n');
    featStatsTest_sorted = sortrows(featStatsTest, 'AUC', 'descend');
    fprintf('     Top 15 Features by AUC:\n');
    for i = 1:min(15, height(featStatsTest_sorted))
        fprintf('       %2d. %-40s  AUC=%.3f | Cohen_d=%+.2f | MI=%.3f bits\n', ...
            i, char(featStatsTest_sorted.Feature(i)), ...
            featStatsTest_sorted.AUC(i), ...
            featStatsTest_sorted.CohensD(i), ...
            featStatsTest_sorted.MI_bits(i));
    end
end

% 3.4 Statistical significance
if exist('featStatsTest', 'var') && ~isempty(featStatsTest)
    featStatsSig = featStatsTest(featStatsTest.p_ranksum < 0.05, :);
    featStatsSig = sortrows(featStatsSig, 'p_ranksum', 'ascend');
    if height(featStatsSig) > 0
        fprintf('\n[4] Statistical Significance (Wilcoxon Rank-Sum, p<0.05):\n');
        fprintf('     Significant features: %d/%d\n', height(featStatsSig), numel(predictorNames));
        fprintf('     Top 10 Most Significant:\n');
        for i = 1:min(10, height(featStatsSig))
            fprintf('       %2d. %-40s  p=%.2e\n', ...
                i, char(featStatsSig.Feature(i)), featStatsSig.p_ranksum(i));
        end
    end
end

%% ================================================================================================
%  Save model package
% ================================================================================================
trainedModelPackage = struct();
trainedModelPackage.trainedClassifier           = trainedClassifier;
trainedModelPackage.requiredVariables           = predictorNames;
trainedModelPackage.responseName                = responseName;
trainedModelPackage.options                     = modelOptions;
trainedModelPackage.validationAccuracy          = valAcc;
trainedModelPackage.testAUC                     = aucTest;
trainedModelPackage.testAUPRC                   = aucPR;
trainedModelPackage.testBrier                   = brier;
trainedModelPackage.featureEffectivenessTrain   = featStatsTrain;
trainedModelPackage.featureEffectivenessTest    = featStatsTest;
trainedModelPackage.modelImportance             = modelImportance;
trainedModelPackage.shapImportance              = shapTable;  % SHAP importance
trainedModelPackage.savedAt                     = datetime("now","Format","yyyy-MM-dd HH:mm:ss");

try
    save(modelOutFile, 'trainedClassifier','predictorNames','responseName','modelOptions','trainedModelPackage');
    fprintf('✓ Saved model: %s\n', modelOutFile);
catch ME
    fprintf('Save model failed: %s\n', ME.message);
end

fprintf('\nAll done.\n');

%% =================================================================================================
%  Local functions in this file (grouped by theme)
% =================================================================================================

% ---------------------------------------------------------------------------------
% Baseline and recovery core
% ---------------------------------------------------------------------------------
function [muRR, sigRR, baseRR_vec, muTamp, muQTc] = local_baseline_stats(~, pvcSample, rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, qrs_dur_vec, fs, params)
    baseA = max(1, double(pvcSample) - round(params.baselineSec*fs));
    baseB = max(1, double(pvcSample) - 1);
    j_in = find(rGlobalAll >= baseA & rGlobalAll <= baseB);
    j_in = j_in(:); j_in = j_in(j_in>=2);
    if ~isempty(j_in)
        mask_ok = ~isPVCBeat(j_in) & ~isPVCBeat(j_in-1) & sqi_vec(j_in) & sqi_vec(j_in-1) & isfinite(rr_between(j_in));
        baseRR_vec = rr_between(j_in(mask_ok));
    else
        baseRR_vec = [];
    end
    baseRR_vec = baseRR_vec(isfinite(baseRR_vec) & baseRR_vec>=params.minRR & baseRR_vec<=params.maxRR);

    if numel(baseRR_vec) < max(3, params.baselineMinBeats)
        j_all = (2:numel(rGlobalAll)).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & sqi_vec(j_all) & sqi_vec(j_all-1) & isfinite(rr_between(j_all));
        rr_all = rr_between(j_all(mask_all)); rr_all = rr_all(isfinite(rr_all));
        if isempty(rr_all)
            muRR = median(rr_between(isfinite(rr_between)));
            sigRR= std(rr_between(isfinite(rr_between)));
        else
            muRR = median(rr_all);
            sigRR= std(rr_all);
        end
    else
        muRR = mean(baseRR_vec);
        sigRR= std(baseRR_vec);
    end
    if ~isfinite(muRR) || muRR<=0, muRR = median(rr_between(isfinite(rr_between))); end
    if ~isfinite(sigRR), sigRR = 0; end

    muTamp = NaN; muQTc = NaN;
    try
        if ~isempty(j_in)
            j_ok = j_in(~isnan(t_amp_vec(j_in)) & isfinite(t_amp_vec(j_in)));
            t_base = t_amp_vec(j_ok);
            if ~isempty(t_base), muTamp = mean(abs(t_base)); end
        end
    catch, muTamp = NaN; end
    try
        j_ok = j_in;
        keep = false(size(j_ok));
        for mm = 1:numel(j_ok)
            j = j_ok(mm);
            if isfinite(tGlobalIdx(j)) && tGlobalIdx(j) > rGlobalAll(j) && isfinite(rr_between(j)) && rr_between(j)>0
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                if isfinite(QT_approx) && QT_approx>=0.20 && QT_approx<=0.60
                    keep(mm)=true;
                end
            end
        end
        j_ok = j_ok(keep);
        if ~isempty(j_ok)
            qtc_vals = nan(numel(j_ok),1);
            for k = 1:numel(j_ok)
                j = j_ok(k);
                rrj = rr_between(j);
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                qtc_vals(k)= QT_approx / (rrj^(1/3));
            end
            qtc_vals = qtc_vals(isfinite(qtc_vals) & qtc_vals>=0.30 & qtc_vals<=0.70);
            if ~isempty(qtc_vals), muQTc = mean(qtc_vals); end
        end
    catch, muQTc = NaN; end
end

function t50 = local_halflife(dev, t, thr, consecBeats)
    t50 = NaN;
    if isempty(dev)||isempty(t), return; end
    dev = dev(:); t = t(:);
    hit = dev <= thr;
    run = 0;
    for i = 1:numel(hit)
        if hit(i), run=run+1; else, run=0; end
        if run>=consecBeats
            t50 = t(i - consecBeats + 1); return;
        end
    end
end

function tau = local_time_constant(dev, t, K)
    tau = NaN;
    if isempty(dev)||isempty(t), return; end
    dev = dev(:); t = t(:);
    K = min([K, numel(dev), numel(t)]);
    y = log(max(dev(1:K), eps));
    x = t(1:K);
    if numel(unique(x))<2, return; end
    p = polyfit(x,y,1);
    a = p(1);
    if a < 0, tau = -1/a; end
end

function A = local_auc(dev, t, normTime)
    A=NaN;
    if numel(dev)<2 || numel(t)<2, A=0; return; end
    dev = dev(:); t = t(:);
    dt = diff(t)./max(normTime,1);
    for i=2:numel(dev)
        if isnan(dev(i)), dev(i)=dev(i-1); end
    end
    if isnan(dev(1)), dev(1)=0; end
    A = sum(0.5*(dev(1:end-1)+dev(2:end)).*dt);
    if ~isfinite(A), A = NaN; end
end

function oi = local_oscillation_index(dev)
    oi = NaN;
    if numel(dev)<3, oi=0; return; end
    d1 = diff(dev(:));
    s  = sign(d1); s(s==0)=1;
    oi = sum(abs(diff(s))>0) / max(numel(d1)-1,1);
end

% ---------------------------------------------------------------------------------
% Model / prediction / threshold
% ---------------------------------------------------------------------------------
function [trainedClassifier, validationAccuracy] = local_train_model(trainingTable, predictorNames, responseName, modelOptions)
    predictors = trainingTable(:, predictorNames);
    response = trainingTable.(responseName); if iscell(response), response = cell2mat(response); end

    % Compute number of variables to sample per split
    numVars = modelOptions.NumVariablesToSample;
    if ischar(numVars) || isstring(numVars)
        if strcmpi(char(numVars),'all')
            numVarsArgGlobal = 'all';
        elseif strcmpi(char(numVars),'sqrt')
            p = width(predictors);
            numVarsArgGlobal = max(1, floor(sqrt(max(1,double(p)))));
        else
            numVarsArgGlobal = 'all';
        end
    else
        numVarsArgGlobal = max(1, round(double(numVars)));
    end

    % Non-auto mode
    if isfield(modelOptions,'Method') && ~strcmpi(char(modelOptions.Method),'auto')
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal,'MinLeafSize',modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal);
        end
        fitArgs = {predictors, response,'Method',modelOptions.Method,'NumLearningCycles',modelOptions.NumLearningCycles,'Learners',template};
        if ~strcmpi(char(modelOptions.Method),'bag') && isfield(modelOptions,'LearnRate')
            fitArgs = [fitArgs, {'LearnRate', modelOptions.LearnRate}];
        end
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost',C}];
        end
        cls = fitcensemble(fitArgs{:});
        try
            cv = crossval(cls,'KFold',modelOptions.cvKFold);
            validationAccuracy = 1 - kfoldLoss(cv);
        catch
            validationAccuracy = NaN;
        end
        trainedClassifier = struct();
        trainedClassifier.ClassificationEnsemble = cls;
        trainedClassifier.RequiredVariables = predictorNames;
        trainedClassifier.Options = modelOptions;
        return;
    end

    % Auto-search
    if ~isfield(modelOptions,'candidateMethods') || isempty(modelOptions.candidateMethods)
        modelOptions.candidateMethods = {'LogitBoost','GentleBoost','AdaBoostM1','RUSBoost'};
    end
    if ~isfield(modelOptions,'searchGrid') || isempty(modelOptions.searchGrid)
        modelOptions.searchGrid = struct();
    end
    sg = modelOptions.searchGrid;
    if ~isfield(sg,'NumLearningCycles'),    sg.NumLearningCycles    = [200,300]; end
    if ~isfield(sg,'LearnRate'),            sg.LearnRate            = [0.03,0.05,0.10]; end
    if ~isfield(sg,'MaxNumSplits'),         sg.MaxNumSplits         = [40,60,100]; end
    if ~isfield(sg,'MinLeafSize'),          sg.MinLeafSize          = [8,12,20]; end
    if ~isfield(sg,'NumVariablesToSample'), sg.NumVariablesToSample = {'sqrt','all'}; end

    bestAUPRC = -inf; bestInfo = struct(); bestCls = []; bestValAcc = NaN;
    verbose = isfield(modelOptions,'autoSelectVerbose') && modelOptions.autoSelectVerbose;

    numMethods = numel(modelOptions.candidateMethods);
    combPerMethod = numel(sg.NumLearningCycles)*numel(sg.LearnRate)*numel(sg.MaxNumSplits)* ...
                    numel(sg.MinLeafSize)*numel(sg.NumVariablesToSample);
    totalComb = numMethods * combPerMethod;
    tried = 0;

    for mi = 1:numMethods
        methodName = char(modelOptions.candidateMethods{mi});
        for nlc = sg.NumLearningCycles
            for mns = sg.MaxNumSplits
                for mls = sg.MinLeafSize
                    for nvs = 1:numel(sg.NumVariablesToSample)
                        nvsVal = sg.NumVariablesToSample{nvs};
                        if ischar(nvsVal)||isstring(nvsVal)
                            if strcmpi(char(nvsVal),'all')
                                numVarsArg = 'all';
                            elseif strcmpi(char(nvsVal),'sqrt')
                                p = width(predictors);
                                numVarsArg = max(1,floor(sqrt(max(1,double(p)))));
                            else
                                numVarsArg = 'all';
                            end
                        else
                            numVarsArg = max(1, round(double(nvsVal)));
                        end
                        if ~isempty(mls)
                            template = templateTree('MaxNumSplits',mns,'NumVariablesToSample',numVarsArg,'MinLeafSize',mls);
                        else
                            template = templateTree('MaxNumSplits',mns,'NumVariablesToSample',numVarsArg);
                        end
                        for lr = sg.LearnRate
                            fitArgs = {predictors,response,'Method',methodName,'NumLearningCycles',nlc,'Learners',template,'LearnRate',lr};
                            if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
                                C = [0, modelOptions.costFP; modelOptions.costFN, 0];
                                fitArgs = [fitArgs, {'Cost',C}];
                            end
                            tried = tried + 1;
                            if verbose
                                fprintf('  [Auto %d/%d] %s NLC=%d LR=%.3f MaxSplits=%d MinLeaf=%d NumVars=%s\n', ...
                                    tried, totalComb, methodName, nlc, lr, mns, mls, mat2str(numVarsArg));
                            end
                            try
                                cls = fitcensemble(fitArgs{:});
                            catch
                                continue;
                            end
                            % CV AUPRC (Dead)
                            try
                                cvMdl = crossval(cls,'KFold',modelOptions.cvKFold);
                                valAcc = 1 - kfoldLoss(cvMdl);
                                [~, cvScores] = kfoldPredict(cvMdl);
                                try
                                    clsNames = cvMdl.ClassNames;
                                catch
                                    clsNames = [0;1];
                                end
                                if iscell(clsNames)
                                    try, clsNames = cell2mat(clsNames); catch, clsNames=[0;1]; end
                                end
                                col0 = find(clsNames==0,1,'first'); if isempty(col0), col0=1; end
                                scoresDead = cvScores(:,col0);
                                yDead = 1 - response;
                                try
                                    beta = glmfit(scoresDead, yDead, 'binomial','link','logit');
                                    scoresProb = glmval(beta, scoresDead, 'logit');
                                catch
                                    scoresProb = scoresDead;
                                end
                                try
                                    [~,~,~,auprcLocal] = perfcurve(yDead, scoresProb, 1, 'XCrit','reca','YCrit','prec');
                                catch
                                    [~,~,~,auprcLocal] = perfcurve(yDead, scoresDead, 1, 'XCrit','reca','YCrit','prec');
                                end
                                if isfinite(auprcLocal) && auprcLocal > bestAUPRC
                                    bestAUPRC = auprcLocal; bestCls = cls; bestValAcc = valAcc;
                                    bestInfo = struct('Method',methodName,'NumLearningCycles',nlc,'LearnRate',lr, ...
                                        'MaxNumSplits',mns,'MinLeafSize',mls,'NumVariablesToSample',nvsVal);
                                end
                            catch
                            end
                        end
                    end
                end
            end
        end
    end

    if verbose && ~isempty(fieldnames(bestInfo))
        fprintf('Auto-selected: %s NLC=%d LR=%.3f MaxSplits=%d MinLeaf=%d NumVars=%s | CV-AUPRC(Dead)=%.3f\n', ...
            string(bestInfo.Method), bestInfo.NumLearningCycles, bestInfo.LearnRate, ...
            bestInfo.MaxNumSplits, bestInfo.MinLeafSize, mat2str(bestInfo.NumVariablesToSample), bestAUPRC);
    end

    if isempty(bestCls)
        if verbose, fprintf('Auto-selection failed → fallback GentleBoost.\n'); end
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal,'MinLeafSize',modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal);
        end
        fitArgs = {predictors,response,'Method','GentleBoost','NumLearningCycles',modelOptions.NumLearningCycles,'Learners',template,'LearnRate',modelOptions.LearnRate};
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost',C}];
        end
        bestCls = fitcensemble(fitArgs{:});
        try
            cv = crossval(bestCls,'KFold',modelOptions.cvKFold);
            bestValAcc = 1 - kfoldLoss(cv);
        catch
            bestValAcc = NaN;
        end
        bestInfo = struct('Method','GentleBoost','NumLearningCycles',modelOptions.NumLearningCycles, ...
            'LearnRate',modelOptions.LearnRate,'MaxNumSplits',maxSplits,'MinLeafSize',modelOptions.MinLeafSize, ...
            'NumVariablesToSample',numVars);
    end

    trainedClassifier = struct();
    trainedClassifier.ClassificationEnsemble = bestCls;
    trainedClassifier.RequiredVariables = predictorNames;
    % Record selected hyperparameters
    modelOptions.selectedMethod          = string(bestInfo.Method);
    modelOptions.Method                  = string(bestInfo.Method);
    modelOptions.NumLearningCycles       = double(bestInfo.NumLearningCycles);
    modelOptions.LearnRate               = double(bestInfo.LearnRate);
    modelOptions.MaxNumSplits            = double(bestInfo.MaxNumSplits);
    modelOptions.MinLeafSize             = double(bestInfo.MinLeafSize);
    modelOptions.NumVariablesToSample    = bestInfo.NumVariablesToSample;
    trainedClassifier.Options            = modelOptions;
    validationAccuracy = bestValAcc;
end

function model = local_threshold_tuning(model, Ttrain, predictorNames, responseName, modelOptions)
    rng(0);
    try
        cvMdl = crossval(model.ClassificationEnsemble,'KFold',modelOptions.cvKFold);
        [~, cvScores] = kfoldPredict(cvMdl);
        try
            cls = cvMdl.ClassNames;
        catch
            cls = [0;1];
        end
        if iscell(cls)
            try, cls = cell2mat(cls); catch, cls=[0;1]; end
        end
        col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
        scoresDead = cvScores(:,col0);
    catch
        [~, scoresDead] = local_predict_with_threshold(model, Ttrain, predictorNames);
    end
    yTrain = Ttrain.(responseName); if iscell(yTrain), yTrain = cell2mat(yTrain); end
    yDead = 1 - yTrain;
    calibBeta = [];
    try
        calibBeta = glmfit(scoresDead, yDead, 'binomial','link','logit');
        scoresProb = glmval(calibBeta, scoresDead, 'logit');
    catch
        scoresProb = scoresDead;
        calibBeta = [];
    end

    bestThr = 0.5; bestF1 = -inf;
    for thr = modelOptions.thrCandidates
        yHatDead = double(scoresProb >= thr);
        TP = sum((yHatDead==1)&(yDead==1));
        FP = sum((yHatDead==1)&(yDead==0));
        FN = sum((yHatDead==0)&(yDead==1));
        prec = TP/max(TP+FP,eps); rec = TP/max(TP+FN,eps);
        F1 = (prec+rec>0) * (2*prec*rec/(prec+rec));
        if F1 > bestF1
            bestF1 = F1; bestThr = thr;
        end
    end
    model.Options.pvcThreshold = bestThr;
    if ~isempty(calibBeta)
        model.Options.calibrationBeta = calibBeta;
        fprintf('Platt calibration: beta=[%.4f %.4f]\n', calibBeta(1), calibBeta(2));
    end
    fprintf('Best threshold (CV F1[Dead]): %.2f (F1=%.3f)\n', bestThr, bestF1);
end

function [predVital, scoresDead] = local_predict_with_threshold(model, T, predictorNames)
    X = T(:, predictorNames);
    try
        [~, rawScores] = predict(model.ClassificationEnsemble, X);
    catch
        [~, rawScores] = predict(model.ClassificationEnsemble, X{:,:});
    end
    try
        cls = model.ClassificationEnsemble.ClassNames;
    catch
        cls = [0;1];
    end
    if iscell(cls)
        try, cls = cell2mat(cls); catch, cls=[0;1]; end
    end
    col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
    scoresDead = rawScores(:,col0);
    scoresProb = scoresDead;
    if isfield(model,'Options') && isfield(model.Options,'calibrationBeta') && numel(model.Options.calibrationBeta)==2 && all(isfinite(model.Options.calibrationBeta))
        try
            scoresProb = glmval(model.Options.calibrationBeta, scoresDead, 'logit');
        catch
            scoresProb = scoresDead;
        end
    end
    thr = 0.5;
    if isfield(model,'Options') && isfield(model.Options,'enableThresholdMoving') && model.Options.enableThresholdMoving
        if isfield(model.Options,'pvcThreshold') && isfinite(model.Options.pvcThreshold)
            thr = model.Options.pvcThreshold;
        end
    end
    yHatDead = double(scoresProb >= thr);
    predVital = 1 - yHatDead;
end

function local_print_confusion(tag, yTrueVital, yHatVital)
    yTrueVital = double(yTrueVital(:));
    yHatVital  = double(yHatVital(:));
    yTrueDead  = 1 - yTrueVital;
    yHatDead   = 1 - yHatVital;
    TP = sum((yTrueDead==1)&(yHatDead==1));
    FN = sum((yTrueDead==1)&(yHatDead==0));
    FP = sum((yTrueDead==0)&(yHatDead==1));
    TN = sum((yTrueDead==0)&(yHatDead==0));
    acc = mean(yTrueVital==yHatVital);
    sens = TP/max(TP+FN,eps);
    spec = TN/max(TN+FP,eps);
    prec = TP/max(TP+FP,eps);
    f1   = (prec+sens>0)*(2*prec*sens/(prec+sens));

    fprintf('\n--- %s Confusion Matrix (English) ---\n', tag);
    hdr = sprintf('%-14s%12s%12s','Actual\\Pred','Alive','Dead');
    fprintf('%s\n', hdr);
    fprintf('%s\n', repmat('-',1,length(hdr)));
    fprintf('%-14s%12d%12d\n','Alive',TN,FP);
    fprintf('%-14s%12d%12d\n','Dead', FN,TP);
    fprintf('Accuracy=%.2f%% | Recall(Dead)=%.2f%% | Specificity(Alive)=%.2f%% | Precision(Dead)=%.2f%% | F1(Dead)=%.3f\n', ...
        100*acc, 100*sens, 100*spec, 100*prec, f1);
    try
        figure;
        confMat = [TN FP; FN TP];
        confusionchart(confMat, {'Alive','Dead'}, 'RowSummary','row-normalized','ColumnSummary','column-normalized');
        title([tag ' Confusion']);
    catch
    end
end

% ---------------------------------------------------------------------------------
% Data split / linear helpers
% ---------------------------------------------------------------------------------
function [trainMask, testMask] = local_group_stratified_split(groupIds, y, testRatio, seed)
    rng(seed);
    groupIds = string(groupIds(:));
    y = double(y(:));
    ug = unique(groupIds);
    G = struct('id',[], 'n0',0, 'n1',0, 'idx',[]);
    G = repmat(G, numel(ug),1);
    for i=1:numel(ug)
        gid = ug(i);
        idx = find(groupIds==gid);
        yi  = y(idx);
        G(i).id = gid; G(i).n0 = sum(yi==0); G(i).n1=sum(yi==1); G(i).idx=idx;
    end
    target0 = sum(y==0)*testRatio;
    target1 = sum(y==1)*testRatio;
    cur0=0; cur1=0; ord = randperm(numel(G));
    testGroups = false(numel(G),1);
    for k = ord
        g = G(k);
        gain0 = abs((cur0+g.n0)-target0) - abs(cur0-target0);
        gain1 = abs((cur1+g.n1)-target1) - abs(cur1-target1);
        if (cur0<target0 || cur1<target1) && ((gain0<=0)||(gain1<=0))
            testGroups(k)=true; cur0=cur0+g.n0; cur1=cur1+g.n1;
        end
    end
    k=1;
    while (cur0<target0 || cur1<target1) && k<=numel(G)
        if ~testGroups(k)
            g=G(k); testGroups(k)=true; cur0=cur0+g.n0; cur1=cur1+g.n1;
        end
        k=k+1;
    end
    testMask = false(numel(y),1);
    for i=1:numel(G)
        if testGroups(i)
            testMask(G(i).idx)=true;
        end
    end
    trainMask = ~testMask;
    % Ensure both classes exist
    if sum(y(testMask)==1)==0
        idxMove = find(trainMask&(y==1),1,'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(testMask)==0)==0
        idxMove = find(trainMask&(y==0),1,'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(trainMask)==1)==0
        idxMove = find(testMask&(y==1),1,'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
    if sum(y(trainMask)==0)==0
        idxMove = find(testMask&(y==0),1,'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
end

% ---------------------------------------------------------------------------------
% Statistical helpers
% ---------------------------------------------------------------------------------

function [featStats] = local_feature_effectiveness(T, predictorNames, responseName, tag)
    yVital = T.(responseName); if iscell(yVital), yVital = cell2mat(yVital); end
    yDead = 1 - double(yVital(:));
    rows = numel(predictorNames);
    Feature=strings(rows,1); AUC=nan(rows,1); CohensD=nan(rows,1);
    SpearmanR=nan(rows,1); p_ranksum=nan(rows,1); MI_bits=nan(rows,1);
    for i=1:rows
        name = predictorNames{i}; Feature(i)=string(name);
        x = double(T.(name)); x = x(:);
        ok = isfinite(x)&isfinite(yDead);
        x = x(ok); y = yDead(ok);
        if numel(unique(y))<2 || numel(x)<5, continue; end
        try
            [~,~,~,auc] = perfcurve(y,x,1);
            if ~isfinite(auc), auc=NaN; end
            if auc<0.5, auc = 1 - auc; end
            AUC(i)=auc;
        catch
            AUC(i)=NaN;
        end
        try
            xd = x(y==1); xa = x(y==0);
            md=mean(xd); ma=mean(xa);
            sd_d=std(xd); sd_a=std(xa);
            n_d=numel(xd); n_a=numel(xa);
            s_p = sqrt(((n_d-1)*sd_d^2 + (n_a-1)*sd_a^2)/max(n_d+n_a-2,1));
            if s_p>0, CohensD(i)=(md-ma)/s_p; end
        catch, CohensD(i)=NaN; end
        try
            SpearmanR(i)=corr(x,y,'type','Spearman','rows','pairwise');
        catch, SpearmanR(i)=NaN; end
        try
            xd=x(y==1); xa=x(y==0);
            if numel(xd)>=2 && numel(xa)>=2
                p_ranksum(i)=ranksum(xd,xa);
            end
        catch, p_ranksum(i)=NaN; end
        try
            bins = local_quantile_bins(x,10);
            MI_bits(i) = local_mutual_info_bits(bins,y);
        catch, MI_bits(i)=NaN; end
    end
    featStats = table(Feature,AUC,CohensD,SpearmanR,p_ranksum,MI_bits);
    try
        fprintf('  [%s] median AUC=%.3f | top AUC=%.3f (%s)\n', tag, median(AUC,'omitnan'), max(AUC), string(Feature(AUC==max(AUC))));
    catch
    end
end

function bins = local_quantile_bins(x, numBins)
    x=x(:); ok=isfinite(x); x=x(ok);
    if isempty(x), bins=nan(size(ok)); return; end
    edges = quantile(x, linspace(0,1,numBins+1));
    edges(1)=-inf; edges(end)=inf;
    bins_full = nan(numel(ok),1);
    bins_full(ok) = discretize(x, edges);
    bins = bins_full;
end

function mi = local_mutual_info_bits(xBins, yBin)
    mi=NaN;
    if numel(xBins)~=numel(yBin), return; end
    ok=isfinite(xBins)&isfinite(yBin);
    xBins=xBins(ok); yBin=yBin(ok);
    if isempty(xBins), mi=NaN; return; end
    K = max(xBins);
    if K<=1, mi=0; return; end
    N=numel(xBins);
    px = accumarray(xBins,1,[K,1],@sum,0)/N;
    py = [sum(yBin==0); sum(yBin==1)]/N;
    pxy=zeros(K,2);
    for k=1:K
        sel = (xBins==k);
        pxy(k,1) = sum(sel & (yBin==0))/N;
        pxy(k,2) = sum(sel & (yBin==1))/N;
    end
    mi_val=0;
    for k=1:K
        for j=1:2
            if pxy(k,j)>0 && px(k)>0 && py(j)>0
                mi_val = mi_val + pxy(k,j)*log2(pxy(k,j)/(px(k)*py(j)));
            end
        end
    end
    mi=mi_val;
end

% ---------------------------------------------------------------------------------
% Publication-grade plots
% ---------------------------------------------------------------------------------
function local_generate_publication_plots(model, Ttrain, Ttest, predictorNames, responseName, resultsDir)
    try
        [~, rawScoresTr] = predict(model.ClassificationEnsemble, Ttrain(:,predictorNames));
        try
            cls = model.ClassificationEnsemble.ClassNames;
        catch
            cls = [0;1];
        end
        if iscell(cls)
            try, cls=cell2mat(cls); catch, cls=[0;1]; end
        end
        col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
        scoresDeadTr = rawScoresTr(:,col0);
        scoresProbTr = scoresDeadTr;
        if isfield(model,'Options') && isfield(model.Options,'calibrationBeta')
            try
                scoresProbTr = glmval(model.Options.calibrationBeta, scoresDeadTr, 'logit');
            catch
            end
        end
        yVitalTr = Ttrain.(responseName); if iscell(yVitalTr), yVitalTr = cell2mat(yVitalTr); end
        yDeadTr = 1 - double(yVitalTr(:));

        [~, rawScoresTe] = predict(model.ClassificationEnsemble, Ttest(:,predictorNames));
        scoresDeadTe = rawScoresTe(:,col0);
        scoresProbTe = scoresDeadTe;
        if isfield(model,'Options') && isfield(model.Options,'calibrationBeta')
            try
                scoresProbTe = glmval(model.Options.calibrationBeta, scoresDeadTe, 'logit');
            catch
            end
        end
        yVitalTe = Ttest.(responseName); if iscell(yVitalTe), yVitalTe = cell2mat(yVitalTe); end
        yDeadTe = 1 - double(yVitalTe(:));

        % ROC
        try
            [Xtr,Ytr,~,aucTr] = perfcurve(yDeadTr, scoresProbTr,1);
        catch
            [Xtr,Ytr,~,aucTr] = perfcurve(yDeadTr, scoresDeadTr,1);
        end
        try
            [Xte,Yte,~,aucTe] = perfcurve(yDeadTe, scoresProbTe,1);
        catch
            [Xte,Yte,~,aucTe] = perfcurve(yDeadTe, scoresDeadTe,1);
        end
        figure; plot(Xte,Yte,'r-','LineWidth',2); hold on;
        plot(Xtr,Ytr,'b--','LineWidth',2); plot([0 1],[0 1],'k:','LineWidth',1.2);
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('False Positive Rate'); ylabel('True Positive Rate');
        title('ROC Curve (Dead=Positive)');
        legend({sprintf('Test (AUC=%.3f)',aucTe), sprintf('Train (AUC=%.3f)',aucTr), 'Chance'}, 'Location','southeast');
        try, exportgraphics(gcf, fullfile(resultsDir,'roc_curve_train_test.png'),'Resolution',300); catch, end

        % PR
        try
            [Rtr,Ptr,~,auprcTr] = perfcurve(yDeadTr, scoresProbTr,1,'XCrit','reca','YCrit','prec');
        catch
            [Rtr,Ptr,~,auprcTr] = perfcurve(yDeadTr, scoresDeadTr,1,'XCrit','reca','YCrit','prec');
        end
        try
            [Rte,Pte,~,auprcTe] = perfcurve(yDeadTe, scoresProbTe,1,'XCrit','reca','YCrit','prec');
        catch
            [Rte,Pte,~,auprcTe] = perfcurve(yDeadTe, scoresDeadTe,1,'XCrit','reca','YCrit','prec');
        end
        prevalTe = mean(yDeadTe);
        figure; plot(Rte,Pte,'r-','LineWidth',2); hold on;
        plot(Rtr,Ptr,'b--','LineWidth',2);
        yline(prevalTe,'k:','LineWidth',1.2,'Label','Prevalence (Test)','LabelHorizontalAlignment','left');
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('Recall'); ylabel('Precision');
        title('Precision-Recall (Dead=Positive)');
        legend({sprintf('Test (AUPRC=%.3f)',auprcTe), sprintf('Train (AUPRC=%.3f)',auprcTr), 'Baseline'}, 'Location','southwest');
        try, exportgraphics(gcf, fullfile(resultsDir,'pr_curve_train_test.png'),'Resolution',300); catch, end

        % Calibration
        numBins=10; edges=linspace(0,1,numBins+1);
        [~,~,binIdxTe] = histcounts(scoresProbTe, edges);
        binProb = accumarray(max(binIdxTe,1), scoresProbTe, [numBins,1], @mean, NaN);
        binObs  = accumarray(max(binIdxTe,1), yDeadTe,      [numBins,1], @mean, NaN);
        [~,~,binIdxTr] = histcounts(scoresProbTr, edges);
        binProbTr = accumarray(max(binIdxTr,1), scoresProbTr, [numBins,1], @mean, NaN);
        binObsTr  = accumarray(max(binIdxTr,1), yDeadTr,      [numBins,1], @mean, NaN);
        figure; plot([0 1],[0 1],'k:','LineWidth',1.2); hold on;
        plot(binProb,binObs,'ro-','LineWidth',2,'MarkerSize',6);
        plot(binProbTr,binObsTr,'bs--','LineWidth',2,'MarkerSize',6);
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('Predicted Probability (Dead)'); ylabel('Observed Rate (Dead)');
        title('Calibration'); legend('Perfect','Test','Train','Location','southeast');
        try, exportgraphics(gcf, fullfile(resultsDir,'calibration_curve_train_test.png'),'Resolution',300); catch, end

        % Decision Curve
        thrList=0.01:0.01:0.99; N=numel(yDeadTe);
        nbModel=nan(size(thrList)); nbAll=nan(size(thrList)); nbNone=zeros(size(thrList));
        preval = mean(yDeadTe);
        for ii=1:numel(thrList)
            pt=thrList(ii);
            yHat = scoresProbTe >= pt;
            TP = sum((yHat==1)&(yDeadTe==1));
            FP = sum((yHat==1)&(yDeadTe==0));
            nbModel(ii) = (TP/N) - (FP/N)*(pt/(1-pt));
            nbAll(ii)   = preval - (1 - preval)*(pt/(1-pt));
        end
        figure; plot(thrList,nbModel,'r-','LineWidth',2); hold on;
        plot(thrList,nbAll,'k--','LineWidth',1.8);
        plot(thrList,nbNone,'k:','LineWidth',1.8);
        grid on; xlim([0,1]);
        xlabel('Threshold Probability (Dead)'); ylabel('Net Benefit');
        title('Decision Curve (Test)');
        legend('Model','Treat All','Treat None','Location','northwest');
        try, exportgraphics(gcf, fullfile(resultsDir,'decision_curve_test.png'),'Resolution',300); catch, end
        fprintf('Saved evaluation plots.\n');
    catch ME
        fprintf('Plot generation error: %s\n', ME.message);
    end
end

% ---------------------------------------------------------------------------------
% SHAP computation (simplified TreeSHAP)
% ---------------------------------------------------------------------------------
function [shapValues, shapImportance] = local_compute_shap_treebased(ensembleModel, X_table, predictorNames)
    % Compute SHAP values for tree ensemble models (simplified TreeSHAP)
    % Inputs:
    %   ensembleModel - MATLAB ensemble classifier
    %   X_table - table of predictors
    %   predictorNames - list of predictor names
    % Outputs:
    %   shapValues - [numSamples x numFeatures] SHAP value matrix
    %   shapImportance - [numFeatures x 1] feature importance (mean |SHAP|)
    
    numSamples = height(X_table);
    numFeatures = numel(predictorNames);
    shapValues = zeros(numSamples, numFeatures);
    
    % Extract feature matrix
    X = zeros(numSamples, numFeatures);
    for i = 1:numFeatures
        X(:, i) = double(X_table.(predictorNames{i}));
    end
    
    % Baseline prediction (average prediction over all samples)
    try
        [~, baseScores] = predict(ensembleModel, X_table);
        cls = ensembleModel.ClassNames;
        if iscell(cls)
            try, cls = cell2mat(cls); catch, cls = [0;1]; end
        end
        col0 = find(cls==0, 1, 'first'); if isempty(col0), col0 = 1; end
        baseScore = mean(baseScores(:, col0));
    catch
        baseScore = 0;
    end
    
    fprintf('    Computing SHAP for %d samples x %d features...\n', numSamples, numFeatures);
    
    % Per-sample SHAP (leave-one-feature-out marginal contribution approximation)
    for s = 1:numSamples
        if mod(s, 20) == 0
            fprintf('      Progress: %d/%d samples\n', s, numSamples);
        end
        
        % Full prediction for current sample
        try
            [~, scores_full] = predict(ensembleModel, X_table(s, :));
            predFull = scores_full(col0);
        catch
            predFull = baseScore;
        end
        
        % Marginal contribution for each feature (replace with mean)
        for f = 1:numFeatures
            try
                % Replace current feature with its mean
                X_perturbed = X_table(s, :);
                featMean = mean(X(:, f), 'omitnan');
                X_perturbed.(predictorNames{f}) = featMean;
                
                % Predict
                [~, scores_perturbed] = predict(ensembleModel, X_perturbed);
                predPerturbed = scores_perturbed(col0);
                
                % SHAP value = full prediction - perturbed prediction
                shapValues(s, f) = predFull - predPerturbed;
            catch
                shapValues(s, f) = 0;
            end
        end
    end
    
    % Importance as mean absolute SHAP
    shapImportance = mean(abs(shapValues), 1, 'omitnan')';
    
    fprintf('    ✓ SHAP computation complete\n');
end
