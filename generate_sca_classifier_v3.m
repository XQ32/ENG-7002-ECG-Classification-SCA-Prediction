% =================================================================================================
% File: generate_sca_classifier_v3.m
% Version: v3 (Refactored Layout)
% Function:
%   Based on *_info.mat metadata generated by predict_shhs1_all.m (no longer reads raw EDF signals):
%     1) Build "variable-window" recovery dynamics feature system for PVC→sinus recovery
%     2) Train imbalance-friendly binary classification model (Alive=1 / Dead=0)
%     3) Output feature tables, train/test sets, model, evaluation results, and publication-quality figures
%
% Feature Focus:
%   - Post-PVC RR / T / approximate QTc recovery half-life, time constant
%   - Overshoot, oscillation index, phased instability (stalls), censoring ratio
%   - Complexity/nonlinearity: sample entropy, LZ complexity, piecewise slope, Poincaré ratio
%   - PVC coupling, peak heart rate acceleration, QTc deviation proportion
%
% Input Requirements (from *_info.mat):
%   predPVCIndices, patientVital, fs, recordNumSamples, rGlobalAll, isPVCBeat,
%   qrs_dur_vec, r_amp_vec, sqi_vec, t_amp_vec, tGlobalIndices
%
% Output:
%   results/post_ectopic_features_v3.mat        Record-level feature table (with NaN)
%   results/SCA_trainingFeatureTable_v3.mat     Training set (missing imputed + normalized)
%   results/SCA_testingFeatureTable_v3.mat      Test set
%   results/sca_classifier_v3.mat               Trained model package
%   results/feature_effectiveness_*_v3.csv      Univariate discriminative power
%   results/roc / pr / calibration / decision_curve PNG
%   results/permutation_importance_test_v3.csv  Permutation importance
%   results/ablation_demographic_physio_combined_v3.csv  Ablation experiment
%
% Main Workflow:
%   1) Parameter configuration and path preparation
%   2) Load covariates (optional) + blacklist (optional)
%   3) Enumerate *_info.mat and extract PVC-level features per record → aggregate to record-level
%   4) Train/test stratified split (by patient ID) + missing imputation + MinMax normalization
%   5) Univariate evaluation
%   6) Model training (auto-tuning available) + threshold/probability calibration
%   7) Evaluation and graphical output
%   8) Ablation / importance / save model package
%
% Constraints and Extensions:
%   - If more precise QT, ST, P-wave morphology indicators are needed later, add fields in *_info.mat generation stage
%   - Script removed unused old functions (local_build_confmat_text / local_build_empty_record etc.)
%
% Update History:
%   2025-09-04: v3 initial version
%   2025-10-08: Refactor reorganization: centralized configuration, sub-functions moved to end, streamlined comments
% =================================================================================================

clc; clear; close all;

%% ================================================================================================
%  Global Configuration (centralized for easy tuning and version tracking)
% ================================================================================================

% Execution Control
processFirstN          = 0;        % Process first N records (<=0 = all)
splitRatioTest         = 0.20;     % Test set ratio
randomSeed             = 42;       % Random seed
enableBlacklist        = false;    % Whether blacklist is active
includeDemographics    = false;    % Whether to include demographic covariates
rng(randomSeed);

% Path Settings
rootDir   = pwd;
edfDir    = fullfile(rootDir, 'shhs','polysomnography','edfs','shhs1');
resultsDir= fullfile(rootDir, 'results');
if ~isfolder(edfDir), error('EDF (info MAT) directory not found: %s', edfDir); end
if ~isfolder(resultsDir), mkdir(resultsDir); end
addpath(genpath(rootDir));

% Input covariate CSV (optional)
covarCsv = fullfile(rootDir, 'shhs','datasets','shhs-cvd-summary-dataset-0.21.0.csv');

% Recovery/baseline/decision parameters
params = struct();
params.baselineSec        = 50;    % Pre-PVC baseline lookback window (seconds)
params.baselineMinBeats   = 10;    % Minimum valid sinus beats in baseline
params.maxObsSec          = 50;    % Maximum post-PVC observation window (seconds)
params.consecStableBeats  = 10;    % Consecutive stable beats required to determine recovery
params.rrTolFrac          = 0.08;  % RR deviation threshold: |RR-μRR| <= max(frac*μRR, sigma*σRR)
params.rrTolSigma         = 1.5;
params.hrt_ts_low_thr     = 0.0;   % HRT TS low threshold (lower = more abnormal)
params.minRR              = 0.30;  % Reasonable RR lower limit (seconds)
params.maxRR              = 2.50;  % Reasonable RR upper limit (seconds)
params.minPVCPerRecord    = 10;    % Minimum PVCs per record
params.minSQIRatio        = 0.60;  % Sinus + good SQI ratio threshold

% Model parameters / imbalance & threshold
modelOptions = struct();
% Optimal parameters based on automatic search (15-feature optimized: AdaBoostM1 NLC=250 LR=0.015 MaxSplits=20 MinLeaf=60)
modelOptions.Method                = 'AdaBoostM1';  % Best method determined by auto-search
modelOptions.NumLearningCycles     = 250;            % Optimal value (CV-AUPRC=0.402)
modelOptions.LearnRate             = 0.015;          % Optimal learning rate
modelOptions.MaxNumSplits          = 20;             % Reduce tree complexity for better generalization (from 30→20)
modelOptions.MinLeafSize           = 60;             % Increase leaf node regularization (from 50→60)
modelOptions.NumVariablesToSample  = 'sqrt';         % Maintain randomness
modelOptions.enableCostMatrix      = true;
modelOptions.costFP                = 2;
modelOptions.costFN                = 8;              % Increase FN penalty (was 6)
modelOptions.enableThresholdMoving = true;
modelOptions.pvcThreshold          = 0.22;           % Optimal threshold (CV F1=0.443)
modelOptions.cvKFold               = 10;
modelOptions.thrCandidates         = 0.02:0.01:0.98;
modelOptions.calibrationMethod     = 'platt';        % Explicitly use Platt calibration
modelOptions.candidateMethods      = {'LogitBoost','GentleBoost','AdaBoostM1','RUSBoost'};
% Adjust search grid to more conservative parameter range (reduce overfitting)
modelOptions.searchGrid = struct();
modelOptions.searchGrid.NumLearningCycles    = [250, 300];
modelOptions.searchGrid.LearnRate            = [0.01, 0.015, 0.02];
modelOptions.searchGrid.MaxNumSplits         = [20, 30, 40];
modelOptions.searchGrid.MinLeafSize          = [50, 60];  % Increase range
modelOptions.searchGrid.NumVariablesToSample = {'sqrt'};
modelOptions.autoSelectVerbose               = true;
modelOptions.disableAutoInAblation           = true;

% Output filenames
featOutFile  = fullfile(resultsDir, 'post_ectopic_features_v3.mat');
trainTblFile = fullfile(resultsDir, 'SCA_trainingFeatureTable_v3.mat');
testTblFile  = fullfile(resultsDir, 'SCA_testingFeatureTable_v3.mat');
modelOutFile = fullfile(resultsDir, 'sca_classifier_v3.mat');

fprintf('=== SCA Risk (v3) Variable-Recovery Feature Pipeline (Refactored) ===\n');
fprintf('Info MAT directory: %s\n', edfDir);

%% ================================================================================================
%  Load Covariates (optional)
% ================================================================================================
covarMap = containers.Map('KeyType','char','ValueType','any');
if exist(covarCsv,'file')
    try
        Tcov = readtable(covarCsv);
        if any(strcmpi(Tcov.Properties.VariableNames,'nsrrid'))
            idStr = string(Tcov.nsrrid);
        else
            idStr = string(Tcov{:,1});
        end
        has_gender = any(strcmpi(Tcov.Properties.VariableNames,'gender'));
        has_race   = any(strcmpi(Tcov.Properties.VariableNames,'race'));
        has_age    = any(strcmpi(Tcov.Properties.VariableNames,'age_s1'));
        for ii = 1:height(Tcov)
            key = char(strtrim(idStr(ii)));
            val = struct();
            if has_gender, val.gender = double(Tcov.gender(ii)); else, val.gender = NaN; end
            if has_race,   val.race   = double(Tcov.race(ii));   else, val.race   = NaN; end
            if has_age,    val.age_s1 = double(Tcov.age_s1(ii)); else, val.age_s1 = NaN; end
            covarMap(key) = val;
        end
        fprintf('Loaded covariates (%d rows)\n', height(Tcov));
    catch ME
        fprintf('Covariate load failed: %s\n', ME.message);
    end
else
    fprintf('Covariate CSV not found, skip.\n');
end

%% ================================================================================================
%  Blacklist (optional)
% ================================================================================================
blackListIds = string([]);
blackListNames = string([]);
if enableBlacklist
    blacklistFile = fullfile(resultsDir, 'badsignallist.csv');
    if exist(blacklistFile,'file')
        try
            C = readcell(blacklistFile);
            vals = string(C(:));
            vals = strtrim(replace(replace(vals, '"',''),'''',''));
            vals = vals(vals~="");
            isDigits = ~cellfun('isempty', regexp(cellstr(vals), '^\d+$', 'once'));
            ids = vals(isDigits);
            names = vals(~isDigits);
            recIdFromNames = strings(0,1);
            for ii = 1:numel(names)
                tok = regexp(names(ii),'(\d+)','tokens','once');
                if ~isempty(tok)
                    recIdFromNames(end+1,1) = string(tok{1});
                end
            end
            blackListIds   = unique([ids; recIdFromNames]);
            blackListNames = unique("shhs1-" + blackListIds);
            fprintf('Blacklist loaded: %d entries\n', numel(blackListIds));
        catch ME
            fprintf('Blacklist load failed: %s\n', ME.message);
        end
    else
        fprintf('Blacklist file missing, skip.\n');
    end
else
    fprintf('Blacklist disabled.\n');
end

%% ================================================================================================
%  Enumerate *_info.mat
% ================================================================================================
infoFilesA = dir(fullfile(edfDir, 'shhs1-*_*info.mat'));
infoFilesB = dir(fullfile(edfDir, 'shhs1-*_info.mat'));
allPaths = unique([ ...
    arrayfun(@(d) fullfile(d.folder,d.name), infoFilesA,'UniformOutput',false), ...
    arrayfun(@(d) fullfile(d.folder,d.name), infoFilesB,'UniformOutput',false) ]);
if isempty(allPaths), error('No *_info.mat found under %s', edfDir); end
allPaths = sort(allPaths);
if processFirstN > 0
    allPaths = allPaths(1:min(processFirstN,numel(allPaths)));
end
fprintf('Planned records: %d\n', numel(allPaths));

%% ================================================================================================
%  Traverse Records → PVC-level Features → Aggregate to Record-level
% ================================================================================================
allRowTables = cell(0,1);
numSkippedQuality = 0;

requiredFields = {'predPVCIndices','patientVital','fs','recordNumSamples','rGlobalAll', ...
    'isPVCBeat','qrs_dur_vec','r_amp_vec','sqi_vec','t_amp_vec','tGlobalIndices'};

for iFile = 1:numel(allPaths)
    infoPath = allPaths{iFile};
    [~, baseName] = fileparts(infoPath);
    recBase = erase(baseName, '_info');
    recIdTok = regexp(recBase, 'shhs1-(\d+)$','tokens','once');
    recIdStr = "";
    if ~isempty(recIdTok), recIdStr = string(recIdTok{1}); end

    if enableBlacklist && (any(blackListNames==string(recBase)) || (strlength(recIdStr)>0 && any(blackListIds==recIdStr)))
        fprintf('  [%d/%d] Skip blacklist: %s\n', iFile, numel(allPaths), recBase);
        continue;
    end

    fprintf('\n--- [%d/%d] Load %s\n', iFile, numel(allPaths), [baseName '.mat']);
    try
        S = load(infoPath);
    catch ME
        fprintf('  Load failed: %s\n', ME.message);
        continue;
    end

    miss = requiredFields(~cellfun(@(f) isfield(S,f) && ~isempty(S.(f)), requiredFields));
    if ~isempty(miss)
        fprintf('  Skip missing fields: %s\n', strjoin(miss, ', '));
        continue;
    end

    patientVital = double(S.patientVital);
    if ~isfinite(patientVital)
        fprintf('  Skip: patientVital invalid.\n');
        continue;
    end

    fs  = double(S.fs);
    N   = double(S.recordNumSamples);
    rGlobalAll = double(S.rGlobalAll(:));
    isPVCBeat  = logical(S.isPVCBeat(:));
    qrs_dur_vec= double(S.qrs_dur_vec(:));
    r_amp_vec  = double(S.r_amp_vec(:)); %#ok<NASGU>
    sqi_vec    = logical(S.sqi_vec(:));
    t_amp_vec  = double(S.t_amp_vec(:));
    tGlobalIdx = double(S.tGlobalIndices(:));
    if any([numel(isPVCBeat),numel(qrs_dur_vec),numel(t_amp_vec),numel(tGlobalIdx),numel(sqi_vec)] ~= numel(rGlobalAll))
        fprintf('  Skip: vector length mismatch.\n');
        continue;
    end

    pvcRidx = sort(double(S.predPVCIndices(:)));
    pvcRidx = pvcRidx(isfinite(pvcRidx));
    numPVC  = numel(pvcRidx);
    fprintf('  Beats=%d fs=%.1f PVC=%d\n', numel(rGlobalAll), fs, numPVC);
    if numPVC == 0
        fprintf('  Skip quality: PVC=0\n');
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    numBeats = numel(rGlobalAll);
    rr_between = nan(numBeats,1);
    if numBeats>=2
        rr_between(2:end) = diff(rGlobalAll)/fs;
    end

    idxPVC_all_sorted = round(interp1(rGlobalAll, 1:numel(rGlobalAll), pvcRidx, 'nearest','extrap'));
    idxPVC_all_sorted = max(1, min(numBeats, idxPVC_all_sorted));

    % Record-level SQI window quality
    try
        j_all = (2:numBeats).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & isfinite(rr_between(j_all));
        sqi_ratio = mean(double(sqi_vec(j_all(mask_all))), 'omitnan');
    catch
        sqi_ratio = NaN;
    end
    if numPVC < params.minPVCPerRecord || ~(isfinite(sqi_ratio) && sqi_ratio>=params.minSQIRatio)
        fprintf('  Skip quality: PVC=%d SQIratio=%.2f (minPVC=%d minSQI=%.2f)\n', ...
            numPVC, sqi_ratio, params.minPVCPerRecord, params.minSQIRatio);
        numSkippedQuality = numSkippedQuality + 1;
        continue;
    end

    % --- PVC-level feature containers ---
    halflife_rr       = nan(numPVC,1);
    halflife_rr_30    = nan(numPVC,1);
    halflife_rr_10    = nan(numPVC,1);
    tau_rr            = nan(numPVC,1);
    auc_rr            = nan(numPVC,1);
    overshoot_rr      = nan(numPVC,1);
    oscill_rr         = nan(numPVC,1);
    stalls_rr         = nan(numPVC,1);
    notRecovered_rr   = false(numPVC,1);

    halflife_tamp     = nan(numPVC,1);
    auc_tamp          = nan(numPVC,1);
    halflife_qtc      = nan(numPVC,1);
    auc_qtc           = nan(numPVC,1);

    hrt_to            = nan(numPVC,1);
    hrt_ts            = nan(numPVC,1);
    coupling_ratio    = nan(numPVC,1);
    hr_peak_accel_bpm = nan(numPVC,1);
    late_var_rr       = nan(numPVC,1);
    pvc_to_next_nonPVC_sec = nan(numPVC,1);

    % High discriminative / complexity
    early_mean_dev_rr = nan(numPVC,1);
    late_mean_dev_rr  = nan(numPVC,1);
    slope_rr_dev_0_5s = nan(numPVC,1);
    slope_rr_dev_5_15s= nan(numPVC,1);
    sampen_rr         = nan(numPVC,1);
    lzc_rr            = nan(numPVC,1);
    poincare_ratio_pp = nan(numPVC,1);
    dev_rr_q95        = nan(numPVC,1);
    tamp_rr_corr      = nan(numPVC,1);
    qtc_over_frac     = nan(numPVC,1);
    qtc_over_mag      = nan(numPVC,1);

    % PVC loop
    for kk = 1:numPVC
        pvcSample = pvcRidx(kk);
        idxPVC    = idxPVC_all_sorted(kk);

        nextPVCsample = inf;
        if kk < numPVC
            nextPVCsample = pvcRidx(kk+1);
        end
        obsEnd = min([double(N), double(pvcSample)+round(params.maxObsSec*fs), double(nextPVCsample)-1]);
        if ~(isfinite(obsEnd) && obsEnd > pvcSample)
            notRecovered_rr(kk) = true;
            continue;
        end

        [muRR, sigRR, baseRR_vec, muTamp, muQTc] = local_baseline_stats(idxPVC, pvcSample, ...
            rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, ...
            qrs_dur_vec, fs, params); %#ok<ASGLU>
        if ~isfinite(muRR) || muRR<=0
            notRecovered_rr(kk) = true;
            continue;
        end

        % Pre/post RR
        rr_pre=nan; rr_post1=nan; rr_post2=nan;
        if idxPVC>=2,      rr_pre   = rr_between(idxPVC); end
        if idxPVC+1<=numBeats, rr_post1 = rr_between(idxPVC+1); end
        if idxPVC+2<=numBeats, rr_post2 = rr_between(idxPVC+2); end

        if isfinite(rr_pre) && isfinite(rr_post1) && rr_pre>0
            hrt_to(kk) = (rr_post1 - rr_pre) / rr_pre;
        end
        hrt_ts(kk) = local_approx_ts(idxPVC, pvcSample, obsEnd, rGlobalAll, rr_between, isPVCBeat, sqi_vec);

        if isfinite(rr_pre), coupling_ratio(kk) = rr_pre / muRR; end
        try
            hr0 = 60/max(muRR,eps);
            hr1 = 60/max(rr_post1,eps);
            hr2 = 60/max(rr_post2,eps);
            hr_peak_accel_bpm(kk) = max([hr1-hr0, hr2-hr0]);
        catch
            hr_peak_accel_bpm(kk)=NaN;
        end

        % Post-PVC non-PVC + good SQI sequence
        j_after = (idxPVC+1):numBeats;
        keep = false(size(j_after));
        for jj = 1:numel(j_after)
            j = j_after(jj);
            if rGlobalAll(j) > obsEnd, break; end
            if j>=2 && ~isPVCBeat(j) && ~isPVCBeat(j-1) && sqi_vec(j) && sqi_vec(j-1) && ...
                    isfinite(rr_between(j)) && rr_between(j)>=params.minRR && rr_between(j)<=params.maxRR
                keep(jj)=true;
            end
        end
        j_after = j_after(keep);
        if isempty(j_after)
            notRecovered_rr(kk)=true;
            continue;
        end
        t_after = (double(rGlobalAll(j_after)) - double(pvcSample))/fs;
        rr_seq  = rr_between(j_after);
        dev_rr  = abs(rr_seq - muRR)./max(muRR,eps);

        % Half-life
        halflife_rr(kk)    = local_halflife(dev_rr, t_after, 0.50, params.consecStableBeats);
        halflife_rr_30(kk) = local_halflife(dev_rr, t_after, 0.30, params.consecStableBeats);
        halflife_rr_10(kk) = local_halflife(dev_rr, t_after, 0.10, params.consecStableBeats);
        if ~isfinite(halflife_rr(kk)) && t_after(end) < params.maxObsSec
            notRecovered_rr(kk) = true;
        end

        tau_rr(kk)      = local_time_constant(dev_rr, t_after, 8);
        auc_rr(kk)      = local_auc(dev_rr, t_after, params.baselineSec);
        overshoot_rr(kk)= max(dev_rr)-dev_rr(1);
        oscill_rr(kk)   = local_oscillation_index(dev_rr);
        stalls_rr(kk)   = local_recovery_stalls(dev_rr, 0.20, params.consecStableBeats);

        late_mask = t_after >= 5;
        if any(late_mask)
            late_var_rr(kk) = var(dev_rr(late_mask), 'omitnan');
        end

        % T amplitude recovery
        tamp_seq = t_amp_vec(j_after);
        if isfinite(muTamp) && muTamp>0
            dev_tamp = abs(tamp_seq - muTamp)./muTamp;
            halflife_tamp(kk)= local_halflife(dev_tamp, t_after, 0.50, params.consecStableBeats);
            auc_tamp(kk)     = local_auc(dev_tamp, t_after, params.baselineSec);
        else
            dev_tamp = nan(size(dev_rr));
        end

        % QTc recovery
        qtc_seq = local_qtc_series(j_after, rGlobalAll, tGlobalIdx, qrs_dur_vec, rr_between, fs);
        if ~isempty(qtc_seq) && any(isfinite(qtc_seq)) && isfinite(muQTc) && muQTc>0
            dev_qtc = abs(qtc_seq - muQTc)./muQTc;
            halflife_qtc(kk) = local_halflife(dev_qtc, t_after(1:numel(qtc_seq)), 0.50, params.consecStableBeats);
            auc_qtc(kk)      = local_auc(dev_qtc, t_after(1:numel(qtc_seq)), params.baselineSec);
            qtc_over_frac(kk)= mean(dev_qtc > 0.10, 'omitnan');
            qtc_over_mag(kk) = max(max(dev_qtc)-0.10, 0);
        end

        % Complexity / slope / entropy / LZ
        early_mask = (t_after>=0)&(t_after<=5);
        late_mask2= (t_after>5)&(t_after<=15);
        if any(early_mask)
            early_mean_dev_rr(kk) = mean(dev_rr(early_mask),'omitnan');
            slope_rr_dev_0_5s(kk) = local_linear_slope(t_after(early_mask), dev_rr(early_mask));
        end
        if any(late_mask2)
            late_mean_dev_rr(kk)  = mean(dev_rr(late_mask2),'omitnan');
            slope_rr_dev_5_15s(kk)= local_linear_slope(t_after(late_mask2), dev_rr(late_mask2));
        end
        try
            sampen_rr(kk) = local_sampen(dev_rr, 2, 0.2*std(dev_rr,'omitnan'));
        catch
            sampen_rr(kk)=NaN;
        end
        try
            d1 = diff(dev_rr(:));
            lzc_rr(kk) = local_lzc_binary(double(d1>=0));
        catch
            lzc_rr(kk)=NaN;
        end
        try
            [SD1_pre, SD2_pre] = local_poincare_sd(baseRR_vec);
            post_mask10 = t_after <= 10;
            if any(post_mask10)
                [SD1_post, SD2_post] = local_poincare_sd(rr_seq(post_mask10));
                r_pre  = SD1_pre/max(SD2_pre,eps);
                r_post = SD1_post/max(SD2_post,eps);
                if isfinite(r_pre) && r_pre>0 && isfinite(r_post)
                    poincare_ratio_pp(kk) = r_post / r_pre;
                end
            end
        catch
            poincare_ratio_pp(kk)=NaN;
        end
        try
            dev_rr_q95(kk) = quantile(dev_rr,0.95);
        catch
            dev_rr_q95(kk)=NaN;
        end
        try
            if numel(dev_tamp)==numel(dev_rr)
                tamp_rr_corr(kk) = corr(dev_rr(:), dev_tamp(:), 'type','Spearman','rows','pairwise');
            end
        catch
            tamp_rr_corr(kk)=NaN;
        end

        % PVC → next non-PVC interval
        try
            j_search = (idxPVC+1):numBeats;
            next_nonPVC_j = NaN;
            for jj = j_search
                if ~isPVCBeat(jj)
                    next_nonPVC_j = jj; break;
                end
            end
            if isfinite(next_nonPVC_j)
                pvc_to_next_nonPVC_sec(kk) = (double(rGlobalAll(next_nonPVC_j)) - double(pvcSample))/fs;
                if ~(isfinite(pvc_to_next_nonPVC_sec(kk)) && pvc_to_next_nonPVC_sec(kk)>=0)
                    pvc_to_next_nonPVC_sec(kk)=NaN;
                end
            end
        catch
            pvc_to_next_nonPVC_sec(kk)=NaN;
        end
    end % end PVC loop

    % --- Record-level aggregation ---
    recDurationHr = (N/fs)/3600;
    pvcPerHour = numPVC / max(recDurationHr,eps);
    recovery_failure_ratio = mean(double(notRecovered_rr));

    agg_mean = @(v) mean(v(isfinite(v)),'omitnan');
    agg_med  = @(v) median(v(isfinite(v)));
    agg_max  = @(v) local_safe_max(v);
    agg_min  = @(v) local_safe_min(v);

    R = struct();
    R.record                     = string(recBase);
    R.patientVital               = patientVital;
    R.PVCs_per_hour              = pvcPerHour;
    R.PVC_count                  = numPVC;
    R.recovery_failure_ratio     = recovery_failure_ratio;
    R.halflife_rr_median         = agg_med(halflife_rr);
    R.halflife_rr30_median       = agg_med(halflife_rr_30);
    R.halflife_rr10_median       = agg_med(halflife_rr_10);
    R.tau_rr_median              = agg_med(tau_rr);
    R.oscill_rr_median           = agg_med(oscill_rr);
    R.HRT_TO_abnormal_frac       = mean(double(isfinite(hrt_to) & (hrt_to>=0)),'omitnan');
    R.HRT_TS_low_frac            = mean(double(isfinite(hrt_ts) & (hrt_ts<=params.hrt_ts_low_thr)),'omitnan');
    R.halflife_tamp_median       = agg_med(halflife_tamp);
    R.halflife_qtc_median        = agg_med(halflife_qtc);
    R.auc_qtc_mean               = agg_mean(auc_qtc);

    % RMSSD baseline (integrated by PVC baseline set)
    try
        rmssd_base = local_rmssd_from_base_perPVC(idxPVC_all_sorted, pvcRidx, rGlobalAll, ...
            rr_between, isPVCBeat, sqi_vec, fs, params);
        R.RR_Pre_RMSSD_mean = mean(rmssd_base(isfinite(rmssd_base)),'omitnan');
    catch
        R.RR_Pre_RMSSD_mean = NaN;
    end

    R.coupling_ratio_mean        = agg_mean(coupling_ratio);
    R.hr_peak_accel_bpm_mean     = agg_mean(hr_peak_accel_bpm);

    medEarly = agg_med(early_mean_dev_rr);
    medLate  = agg_med(late_mean_dev_rr);
    R.rr_dev_late_minus_early    = medLate - medEarly;
    R.rr_dev_slope_0_5s_med      = agg_med(slope_rr_dev_0_5s);
    R.tamp_rr_corr_med           = agg_med(tamp_rr_corr);
    R.qtc_overshoot_frac_mean    = agg_mean(qtc_over_frac);

    try
        hl = halflife_rr(isfinite(halflife_rr) & halflife_rr>0);
        if ~isempty(hl)
            R.recovery_rate_hmean = mean(1./hl);
        else
            R.recovery_rate_hmean = NaN;
        end
    catch
        R.recovery_rate_hmean = NaN;
    end

    R.PVC_to_next_nonPVC_sec_mean = agg_mean(pvc_to_next_nonPVC_sec);
    R.PVC_to_next_nonPVC_sec_max  = agg_max(pvc_to_next_nonPVC_sec);
    R.PVC_to_next_nonPVC_sec_min  = agg_min(pvc_to_next_nonPVC_sec);
    
    % ========== Innovative Features (based on combination and transformation of existing features) ==========
    
    % 1. Nonlinear transformation features (capture nonlinear relationships)
    R.oscill_log            = log(max(R.oscill_rr_median, eps));         % Oscillation log (compress high values)
    R.pvc_freq_sqrt         = sqrt(max(R.PVCs_per_hour, 0));             % PVC frequency square root
    R.halflife10_sqrt       = sqrt(max(R.halflife_rr10_median, eps));    % Early recovery square root
    
    % 2. Feature interactions (products reveal synergistic effects)
    R.oscill_x_pvc_freq     = R.oscill_rr_median * R.PVCs_per_hour;      % Oscillation × frequency (high freq high oscill = high risk)
    R.oscill_x_recovery     = R.oscill_rr_median * R.halflife_rr10_median; % Oscillation × slow recovery = high risk
    R.pvc_x_variability     = R.PVCs_per_hour * R.RR_Pre_RMSSD_mean;     % Frequency × baseline variability
    
    % 3. Ratio features (normalized relative indicators)
    R.oscill_per_pvc        = R.oscill_rr_median / max(R.PVCs_per_hour, eps);  % Oscillation per unit PVC
    R.recovery_efficiency   = 1.0 / max(R.halflife_rr10_median, eps);           % Recovery efficiency (higher is better)
    R.early_late_ratio      = R.halflife_rr10_median / max(R.halflife_rr30_median, eps); % Early/mid-term ratio
    
    % 4. Variability/stability indicators
    try
        % Oscillation coefficient of variation (reflects inconsistency of oscillation)
        oscill_valid = oscill_rr(isfinite(oscill_rr) & oscill_rr > 0);
        if numel(oscill_valid) >= 3
            R.oscill_cv = std(oscill_valid) / max(mean(oscill_valid), eps);
        else
            R.oscill_cv = NaN;
        end
        
        % Recovery time coefficient of variation
        hl_valid = halflife_rr(isfinite(halflife_rr) & halflife_rr > 0);
        if numel(hl_valid) >= 3
            R.halflife_cv = std(hl_valid) / max(mean(hl_valid), eps);
        else
            R.halflife_cv = NaN;
        end
        
        % Fast recovery ratio (proportion of PVCs recovering within 10 seconds)
        hl10_valid = halflife_rr(isfinite(halflife_rr));
        R.fast_recovery_ratio = mean(double(hl10_valid <= 10));
        
    catch
        R.oscill_cv = NaN;
        R.halflife_cv = NaN;
        R.fast_recovery_ratio = NaN;
    end
    
    % 5. Composite risk score (weighted combination)
    % Weighted based on SHAP importance: oscill(0.17) + pvc_freq(0.13) + halflife10(0.12)
    try
        % Normalize to 0-1 (using common ranges)
        oscill_norm = min(R.oscill_rr_median / 0.5, 1.0);  % 0.5 is high oscillation threshold
        pvc_norm    = min(R.PVCs_per_hour / 50, 1.0);      % 50/h is high frequency threshold
        hl10_norm   = min(R.halflife_rr10_median / 30, 1.0); % 30s is slow recovery threshold
        
        R.composite_risk_score = 0.40*oscill_norm + 0.30*pvc_norm + 0.30*hl10_norm;
    catch
        R.composite_risk_score = NaN;
    end
    
    % 6. Temporal dynamics features (early vs late)
    try
        % Recovery deceleration (speed difference between early and mid-term recovery)
        if isfinite(R.halflife_rr10_median) && isfinite(R.halflife_rr30_median) && ...
           R.halflife_rr10_median > 0 && R.halflife_rr30_median > R.halflife_rr10_median
            % Speed = 1/time
            speed_10 = 0.10 / R.halflife_rr10_median;  % Speed to 10%
            speed_30 = 0.20 / (R.halflife_rr30_median - R.halflife_rr10_median); % 10%→30% speed
            R.recovery_deceleration = speed_10 - speed_30; % Positive = deceleration (bad)
        else
            R.recovery_deceleration = NaN;
        end
    catch
        R.recovery_deceleration = NaN;
    end
    
    % 7. HRT comprehensive indicator (overall assessment of heart rate turbulence)
    try
        % HRT abnormality degree (TO abnormal + TS low combined)
        R.HRT_abnormal_combined = R.HRT_TO_abnormal_frac + R.HRT_TS_low_frac;
        
        % HRT risk category (0=normal, 1=mild abnormal, 2=severe abnormal)
        if R.HRT_TO_abnormal_frac > 0.5 && R.HRT_TS_low_frac > 0.5
            R.HRT_risk_category = 2;  % Severe
        elseif R.HRT_TO_abnormal_frac > 0.3 || R.HRT_TS_low_frac > 0.3
            R.HRT_risk_category = 1;  % Mild
        else
            R.HRT_risk_category = 0;  % Normal
        end
    catch
        R.HRT_abnormal_combined = NaN;
        R.HRT_risk_category = NaN;
    end
    
    % ========== Second batch of innovative features (optimized based on first batch results) ==========
    
    % 8. More logarithmic transformations (oscill_log works very well)
    try
        R.pvc_freq_log = log(max(R.PVCs_per_hour, 1));  % PVC frequency log
        R.halflife30_log = log(max(R.halflife_rr30_median, eps));  % Mid-term recovery log
    catch
        R.pvc_freq_log = NaN;
        R.halflife30_log = NaN;
    end
    
    % 9. CV-type feature combinations (halflife_cv and oscill_cv are both important)
    try
        % Combined variability indicator (overall instability of recovery + oscillation)
        R.combined_cv = (R.halflife_cv + R.oscill_cv) / 2;
        
        % Recovery-oscillation variability ratio (which is more unstable)
        R.recovery_oscill_cv_ratio = R.halflife_cv / max(R.oscill_cv, eps);
    catch
        R.combined_cv = NaN;
        R.recovery_oscill_cv_ratio = NaN;
    end
    
    % 10. More high-value interaction features (oscill_x_pvc_freq performs excellently)
    try
        % Log oscillation × frequency (combining two successful features)
        R.oscill_log_x_pvc = R.oscill_log * R.PVCs_per_hour;
        
        % Oscillation × variability (synergistic effect of instability)
        R.oscill_x_cv = R.oscill_rr_median * R.halflife_cv;
        
        % Early-late ratio × variability (recovery pattern and stability)
        R.early_late_x_cv = R.early_late_ratio * R.halflife_cv;
    catch
        R.oscill_log_x_pvc = NaN;
        R.oscill_x_cv = NaN;
        R.early_late_x_cv = NaN;
    end
    
    % 11. Clinically interpretable composite indicators
    try
        % Instability total score (3 most important variability indicators)
        % Normalization: oscill_cv, halflife_cv typically 0-1 range
        instability_score = 0.4*R.oscill_cv + 0.4*R.halflife_cv + 0.2*min(R.recovery_failure_ratio, 1.0);
        R.instability_score = instability_score;
        
        % PVC burden index (frequency × oscillation, normalized)
        pvc_burden = (R.PVCs_per_hour / 50) * (R.oscill_rr_median / 0.5);
        R.pvc_burden_index = min(pvc_burden, 2.0);  % cap at 2.0
        
        % Recovery capacity score (higher is better)
        % Using fast_recovery_ratio (fast recovery proportion) and early_late_ratio
        if isfinite(R.fast_recovery_ratio) && isfinite(R.early_late_ratio) && R.early_late_ratio > 0
            R.recovery_capacity = R.fast_recovery_ratio / R.early_late_ratio;
        else
            R.recovery_capacity = NaN;
        end
    catch
        R.instability_score = NaN;
        R.pvc_burden_index = NaN;
        R.recovery_capacity = NaN;
    end
    
    % 12. Ratios based on Top features
    try
        % Oscillation log / frequency log (normalized relative indicator)
        if isfinite(R.oscill_log) && isfinite(R.pvc_freq_log)
            R.oscill_pvc_log_ratio = R.oscill_log / max(R.pvc_freq_log, eps);
        else
            R.oscill_pvc_log_ratio = NaN;
        end
        
        % tau × variability (time constant and instability)
        R.tau_x_cv = R.tau_rr_median * R.halflife_cv;
    catch
        R.oscill_pvc_log_ratio = NaN;
        R.tau_x_cv = NaN;
    end
    
    % 13. Extreme value indicators (identify high-risk patients)
    try
        % Whether high frequency high oscillation (binary indicator)
        R.high_freq_high_oscill = double(R.PVCs_per_hour > 30 && R.oscill_rr_median > 0.3);
        
        % Whether slow recovery high variability (binary indicator)
        R.slow_recovery_high_cv = double(R.halflife_rr30_median > 20 && R.halflife_cv > 0.5);
        
        % Composite high-risk flag
        R.composite_high_risk = double(R.composite_risk_score > 0.6);
    catch
        R.high_freq_high_oscill = NaN;
        R.slow_recovery_high_cv = NaN;
        R.composite_high_risk = NaN;
    end

    % Covariates
    try
        recIdTok = regexp(string(recBase), 'shhs1-(\d+)$', 'tokens','once');
        nsrrid = "";
        if ~isempty(recIdTok), nsrrid = string(recIdTok{1}); end
        R.nsrrid = nsrrid;
        val = [];
        if strlength(nsrrid)>0 && isKey(covarMap, char(nsrrid))
            val = covarMap(char(nsrrid));
        end
        if ~isempty(val)
            R.age_s1 = val.age_s1;
        else
            R.age_s1 = NaN;
        end
    catch
        R.nsrrid = ""; R.age_s1 = NaN;
    end

    allRowTables{end+1,1} = struct2table(R,'AsArray',true);
end

if isempty(allRowTables)
    error('No usable records → featureTable empty.');
end
featureTable = vertcat(allRowTables{:});
fprintf('\nCollected %d records. Skipped (quality)=%d\n', height(featureTable), numSkippedQuality);

% Save original aggregation (with NaN)
try
    save(featOutFile, 'featureTable');
    fprintf('✓ Saved feature table: %s\n', featOutFile);
catch ME
    fprintf('Save feature table failed: %s\n', ME.message);
end

%% ================================================================================================
%  Missing Imputation + Stratified Split (by Patient ID) + Normalization
% ================================================================================================
names = featureTable.Properties.VariableNames;
isNum = cellfun(@(c) isnumeric(featureTable.(c)) && isvector(featureTable.(c)), names);

% ===== Feature Selection: Keep only the 15 core features =====
% Streamlining criteria: SHAP > 0.020 OR (AUC > 0.600 AND SHAP > 0.015)
lowValueFeatures = {
    % ========== First batch: Original low-value features (verified) ==========
    'PVC_count';                    ... % Redundant with PVCs_per_hour
    'coupling_ratio_mean';          ... % Weak discriminative power
    'recovery_rate_hmean';          ... % Replaced by other recovery features
    'halflife_rr_median';           ... % Replaced by halflife_cv
    'PVC_to_next_nonPVC_sec_mean';  ... % Replaced by max
    'PVC_to_next_nonPVC_sec_min';   ... % Replaced by max
    'HRT_TS_low_frac';              ... % Not as good as HRT_TO
    'auc_qtc_mean';                 ... % Marginal value
    'qtc_overshoot_frac_mean';      ... % QTc not important
    'halflife_tamp_median';         ... % T-wave recovery not important
    'halflife_qtc_median';          ... % QTc recovery not important
    'rr_dev_late_minus_early';      ... % Temporal difference weak
    
    % ========== Second batch: New features with poor performance (SHAP < 0.015) ==========
    'halflife_rr10_median';         ... % SHAP=0.0177, replaced by CV features
    'tamp_rr_corr_med';             ... % SHAP=0.0135, low contribution
    'halflife10_sqrt';              ... % SHAP=0.0131, transformation ineffective
    'recovery_efficiency';          ... % SHAP=0.0118, not as expected
    'oscill_per_pvc';               ... % SHAP=0.0113, good AUC but weak multivariate
    'recovery_deceleration';        ... % SHAP=0.0068, temporal dynamics failed
    'HRT_risk_category';            ... % SHAP=0.0022, categorical feature ineffective
    'pvc_x_variability';            ... % SHAP=0.0153, AUC=0.583 but weak multivariate
    
    % ========== Third batch: Low SHAP features not in LogitBoost Top 15 ==========
    'oscill_x_recovery';            ... % Not in Top 15, replaced by other interaction features
    'halflife30_log';               ... % Not in Top 15, log transformation ineffective for mid-term recovery
    'combined_cv';                  ... % Not in Top 15, simple average not as good as ratio
    'early_late_x_cv';              ... % Not in Top 15, three-way interaction too complex
    'instability_score';            ... % Not in Top 15, weighted sum not as good as univariate
    'high_freq_high_oscill';        ... % Binary indicator weak discriminative power
    'slow_recovery_high_cv';        ... % Binary indicator weak discriminative power
    'composite_high_risk';          ... % Binary indicator weak discriminative power
    'RR_Pre_RMSSD_mean';            ... % Baseline variability, replaced by other CV features
    'recovery_failure_ratio';       ... % Not in Top 15, failure ratio not as good as recovery speed
    'halflife_rr30_median';         ... % Not in Top 15, mid-term recovery replaced by early+CV
    
    % ========== Fourth batch: Further streamline to 15 core features (SHAP < 0.020 and AUC not prominent) ==========
    'tau_x_cv';                     ... % SHAP=0.0161, replaced by tau_rr_median
    'HRT_abnormal_combined';        ... % SHAP=0.0146, replaced by HRT_TO_abnormal_frac
    'pvc_freq_log';                 ... % Low SHAP, redundant with PVCs_per_hour/pvc_freq_sqrt
    'rr_dev_slope_0_5s_med';        ... % SHAP=0.0175, marginal feature
    'oscill_cv';                    ... % SHAP=0.0168, replaced by oscill_x_cv
    'hr_peak_accel_bpm_mean';       ... % SHAP=0.0192, marginal feature
    'recovery_oscill_cv_ratio';     ... % SHAP=0.0179, replaced by halflife_cv+oscill_x_cv
    'PVC_to_next_nonPVC_sec_max';   ... % Low SHAP, compensatory pause not as good as recovery indicators
    'fast_recovery_ratio'           ... % Low SHAP, replaced by recovery_capacity
};

if includeDemographics
    excludePredictors = [{'patientVital', 'nsrrid', 'record'}, lowValueFeatures'];
else
    excludePredictors = [{'patientVital', 'nsrrid', 'record', 'age_s1','gender','race'}, lowValueFeatures'];
end
predictorMask  = isNum & ~ismember(names, excludePredictors);
predictorNames = names(predictorMask);
responseName   = 'patientVital';

% ===== Feature Renaming: Use names with more ECG statistical meaning =====
% Create mapping from original names to new names
featureNameMap = containers.Map(...
    {'oscill_pvc_log_ratio', 'oscill_rr_median', 'oscill_x_cv', 'oscill_log', ...
     'recovery_capacity', 'oscill_log_x_pvc', 'tau_rr_median', 'oscill_x_pvc_freq', ...
     'early_late_ratio', 'halflife_cv', 'HRT_TO_abnormal_frac', 'pvc_burden_index', ...
     'PVCs_per_hour', 'pvc_freq_sqrt', 'composite_risk_score'}, ...
    {'RRI_Oscill_Freq_LogRatio', 'RRI_PostPVC_OscillAmp_Median', 'RRI_Oscill_x_RecoveryCV', 'RRI_PostPVC_OscillAmp_Log', ...
     'RRI_Recovery_CapacityIndex', 'RRI_OscillLog_x_PVC_Freq', 'RRI_Recovery_TimeConst_Median', 'RRI_Oscill_x_PVC_Freq', ...
     'RRI_Recovery_EarlyLate_Ratio', 'RRI_Recovery_TimeVariability_CV', 'HRT_TurbOnset_AbnormalFrac', 'PVC_Burden_NormIndex', ...
     'PVC_FreqPerHour', 'PVC_FreqPerHour_Sqrt', 'Clinical_CompositeRisk_Score'});

% Create reverse mapping (for display)
newNameToOldMap = containers.Map(featureNameMap.values, featureNameMap.keys);

% Apply renaming to featureTable (only for existing features)
renamedPredictors = cell(size(predictorNames));
for i = 1:numel(predictorNames)
    oldName = predictorNames{i};
    if isKey(featureNameMap, oldName)
        newName = featureNameMap(oldName);
        % Rename column in featureTable
        if ismember(oldName, featureTable.Properties.VariableNames)
            featureTable.Properties.VariableNames{strcmp(featureTable.Properties.VariableNames, oldName)} = newName;
        end
        renamedPredictors{i} = newName;
    else
        % If no mapping, keep the original name
        renamedPredictors{i} = oldName;
    end
end
predictorNames = renamedPredictors;

fprintf('\n=== Feature Selection ===\n');
fprintf('Total features extracted: %d\n', sum(isNum)-1);  % -1 for patientVital
fprintf('Low-value features excluded: %d\n', numel(lowValueFeatures));
fprintf('Final predictor count: %d\n', numel(predictorNames));
fprintf('Excluded low-value features:\n');
for i = 1:numel(lowValueFeatures)
    fprintf('  - %s\n', lowValueFeatures{i});
end

% Missing value imputation (median)
for i = 1:numel(predictorNames)
    col = featureTable.(predictorNames{i});
    if ~isnumeric(col), continue; end
    nanMask = isnan(col);
    if any(nanMask)
        medv = median(col(~nanMask));
        if isempty(medv) || isnan(medv), medv = 0; end
        col(nanMask) = medv;
        featureTable.(predictorNames{i}) = col;
    end
end

% Build group (patient ID)
recNames  = string(featureTable.record);
patientId = strings(height(featureTable),1);
for ii = 1:height(featureTable)
    nm = recNames(ii);
    tok = regexp(nm, 'shhs1-(\d+)$','tokens','once');
    if ~isempty(tok)
        patientId(ii) = string(tok{1});
    else
        tok2 = regexp(nm,'(\d+)','tokens','once');
        if ~isempty(tok2)
            patientId(ii) = string(tok2{1});
        else
            patientId(ii) = nm;
        end
    end
end
y = featureTable.(responseName);
if iscell(y), y = cell2mat(y); end
if islogical(y), y = double(y); end

[trainMask, testMask] = local_group_stratified_split(patientId, y, splitRatioTest, randomSeed);
trainingFeatureTable = featureTable(trainMask,:);
testingFeatureTable  = featureTable(testMask,:);
fprintf('Train=%d  Test=%d\n', height(trainingFeatureTable), height(testingFeatureTable));

% Min-Max normalization (based on training set)
try
    scaler = struct();
    scaler.method = 'minmax';
    scaler.featureNames = predictorNames;
    scaler.min = nan(numel(predictorNames),1);
    scaler.max = nan(numel(predictorNames),1);
    for i = 1:numel(predictorNames)
        fn = predictorNames{i};
        tr = double(trainingFeatureTable.(fn));
        mn = min(tr,[],'omitnan');
        mx = max(tr,[],'omitnan');
        if ~isfinite(mn), mn = 0; end
        if ~isfinite(mx), mx = mn + 1; end
        denom = mx - mn;
        if ~(isfinite(denom) && denom>0)
            trScaled = zeros(size(tr));
            teScaled = zeros(size(testingFeatureTable.(fn)));
        else
            trScaled = (tr - mn)./denom;
            te = double(testingFeatureTable.(fn));
            teScaled = (te - mn)./denom;
            trScaled = min(max(trScaled,0),1);
            teScaled = min(max(teScaled,0),1);
        end
        trainingFeatureTable.(fn) = trScaled;
        testingFeatureTable.(fn)  = teScaled;
        scaler.min(i)=mn; scaler.max(i)=mx;
    end
    fprintf('Applied Min-Max normalization.\n');
catch ME
    fprintf('Normalization failed: %s\n', ME.message);
end

% Save training / testing tables
try, save(trainTblFile,'trainingFeatureTable'); fprintf('✓ Saved training table\n'); catch ME, fprintf('Train table save failed: %s\n', ME.message); end
try, save(testTblFile,'testingFeatureTable');   fprintf('✓ Saved testing table\n');  catch ME, fprintf('Test table save failed: %s\n', ME.message); end

%% ================================================================================================
%  Univariate Feature Effectiveness (AUC / d / Mutual Information / Rank-sum test)
% ================================================================================================
fprintf('\n=== Univariate Feature Effectiveness (Train/Test, Dead Positive) ===\n');
featStatsTrain = local_feature_effectiveness(trainingFeatureTable, predictorNames, responseName, 'Train');
featStatsTest  = local_feature_effectiveness(testingFeatureTable,  predictorNames, responseName, 'Test');

try
    writetable(featStatsTrain, fullfile(resultsDir,'feature_effectiveness_train_v3.csv'));
    writetable(featStatsTest,  fullfile(resultsDir,'feature_effectiveness_test_v3.csv'));
catch
end

%% ================================================================================================
%  Model Training + Threshold Tuning + Evaluation
% ================================================================================================
[trainedClassifier, valAcc] = local_train_model(trainingFeatureTable, predictorNames, responseName, modelOptions);
fprintf('Cross-validated accuracy (internal) = %.2f%%\n', 100*valAcc);

if modelOptions.enableThresholdMoving
    trainedClassifier = local_threshold_tuning(trainedClassifier, trainingFeatureTable, predictorNames, responseName, modelOptions);
end

fprintf('\n=== Evaluation (English Confusion Matrices) ===\n');
try
    optsEval = trainedClassifier.Options;
catch
    optsEval = modelOptions;
end
fprintf('Method=%s | NLC=%d | LR=%.3f | MaxSplits=%d | MinLeaf=%d | NumVars=%s\n', ...
    char(optsEval.Method), optsEval.NumLearningCycles, optsEval.LearnRate, ...
    optsEval.MaxNumSplits, optsEval.MinLeafSize, mat2str(optsEval.NumVariablesToSample));

[train_predVital, ~] = local_predict_with_threshold(trainedClassifier, trainingFeatureTable, predictorNames);
train_yTrue = trainingFeatureTable.(responseName); if iscell(train_yTrue), train_yTrue = cell2mat(train_yTrue); end
local_print_confusion('Train', train_yTrue, train_predVital);

[predVital, scoresDead] = local_predict_with_threshold(trainedClassifier, testingFeatureTable, predictorNames);
yTrue = testingFeatureTable.(responseName); if iscell(yTrue), yTrue = cell2mat(yTrue); end
local_print_confusion('Test', yTrue, predVital);

% AUC / AUPRC / Brier (Dead positive)
try
    yTrueDead = 1 - yTrue;
    [~,~,~,aucTest] = perfcurve(yTrueDead, scoresDead, 1);
catch
    aucTest = NaN;
end
try
    scoresProb_eval = scoresDead;
    if isfield(trainedClassifier.Options,'calibrationBeta')
        scoresProb_eval = glmval(trainedClassifier.Options.calibrationBeta, scoresDead, 'logit');
    end
    [~,~,~,aucPR] = perfcurve(yTrueDead, scoresProb_eval, 1, 'XCrit','reca','YCrit','prec');
catch
    aucPR = NaN;
end
try
    brier = mean((scoresProb_eval - yTrueDead).^2);
catch
    brier = NaN;
end
fprintf('Test AUC=%.3f | AUPRC=%.3f | Brier=%.4f\n', aucTest, aucPR, brier);

% Evaluation plots
try
    local_generate_publication_plots(trainedClassifier, trainingFeatureTable, testingFeatureTable, predictorNames, responseName, resultsDir);
catch ME
    fprintf('Plot generation failed: %s\n', ME.message);
end

%% ================================================================================================
%  Ablation Experiments (Demographic / Physio / Combined)
% ================================================================================================
try
    demoVars = intersect(predictorNames, {'age_s1','gender','race'});
    physVars = setdiff(predictorNames, demoVars);
    sets     = {demoVars, physVars, predictorNames};
    setNames = {'demographic_only','physio_only','combined'};
    abRows = strings(0,1); AUCs=[]; AUPRCs=[]; Briers=[]; F1ds=[];
    for si = 1:numel(sets)
        varsS = sets{si};
        if isempty(varsS)
            abRows(end+1,1)=string(setNames{si}); AUCs(end+1,1)=NaN; AUPRCs(end+1,1)=NaN; Briers(end+1,1)=NaN; F1ds(end+1,1)=NaN;
            continue;
        end
        abOptions = modelOptions;
        if isfield(modelOptions,'disableAutoInAblation') && modelOptions.disableAutoInAblation
            if isfield(trainedClassifier,'Options')
                abOptions = trainedClassifier.Options;
            end
            if isfield(abOptions,'selectedMethod'), abOptions.Method = abOptions.selectedMethod; end
            if strcmpi(char(abOptions.Method),'auto'), abOptions.Method='GentleBoost'; end
            abOptions.autoSelectVerbose = false;
        end
        [mdlS, ~] = local_train_model(trainingFeatureTable, varsS, responseName, abOptions);
        mdlS = local_threshold_tuning(mdlS, trainingFeatureTable, varsS, responseName, abOptions);
        [yhatS, scoreS] = local_predict_with_threshold(mdlS, testingFeatureTable, varsS);
        yTrueS = testingFeatureTable.(responseName); if iscell(yTrueS), yTrueS = cell2mat(yTrueS); end
        yDeadS = 1 - yTrueS;
        try
            [~,~,~,aucS] = perfcurve(yDeadS, scoreS, 1);
        catch
            aucS = NaN;
        end
        try
            probS = scoreS;
            if isfield(mdlS.Options,'calibrationBeta')
                probS = glmval(mdlS.Options.calibrationBeta, scoreS, 'logit');
            end
            [~,~,~,auprcS] = perfcurve(yDeadS, probS, 1, 'XCrit','reca','YCrit','prec');
            brierS = mean((probS - yDeadS).^2);
        catch
            auprcS = NaN; brierS = NaN;
        end
        yHatDeadS = 1 - yhatS;
        TP = sum((yDeadS==1)&(yHatDeadS==1));
        FP = sum((yDeadS==0)&(yHatDeadS==1));
        FN = sum((yDeadS==1)&(yHatDeadS==0));
        prec = TP/max(TP+FP,eps); rec = TP/max(TP+FN,eps);
        F1d = (prec+rec>0) * (2*prec*rec/(prec+rec));
        abRows(end+1,1)=string(setNames{si});
        AUCs(end+1,1)=aucS; AUPRCs(end+1,1)=auprcS; Briers(end+1,1)=brierS; F1ds(end+1,1)=F1d;
    end
    abTable = table(abRows,AUCs,AUPRCs,Briers,F1ds,'VariableNames',{'Setting','AUC','AUPRC','Brier','F1_Dead'});
    disp(abTable);
    try, writetable(abTable, fullfile(resultsDir,'ablation_demographic_physio_combined_v3.csv')); catch, end
catch ME
    fprintf('Ablation failed: %s\n', ME.message);
end

%% ================================================================================================
%  Feature Importance (Multi-dimensional evaluation)
% ================================================================================================
fprintf('\n=== Feature Importance Analysis ===\n');

% --- 1. Built-in model importance (Gini gain) ---
modelImportance = struct();
try
    imp = predictorImportance(trainedClassifier.ClassificationEnsemble);
    modelImportance.gain = array2table(imp(:)','VariableNames', trainedClassifier.RequiredVariables);
    fprintf('✓ Computed Gini-based importance\n');
catch ME
    fprintf('Gini importance failed: %s\n', ME.message);
end

% --- 2. SHAP values (Shapley Additive Explanations) ---
shapTable = table();
try
    fprintf('Computing SHAP values (this may take a while)...\n');
    % Compute SHAP values on the test set
    X_test = testingFeatureTable(:, predictorNames);
    
    % Compute Shapley values (MATLAB R2021a+ supported)
    % Compute Shapley values per sample, then take mean absolute value as feature importance
    numSamples = min(100, height(testingFeatureTable));  % limit sample count for speed
    sampledIdx = randperm(height(testingFeatureTable), numSamples);
    X_sampled = testingFeatureTable(sampledIdx, predictorNames);
    
    % Use custom function to compute SHAP (TreeSHAP for ensemble models)
    [shapValues, shapImportance] = local_compute_shap_treebased(...
        trainedClassifier.ClassificationEnsemble, X_sampled, predictorNames);
    
    modelImportance.shap_values = shapValues;  % store detailed SHAP value matrix
    shapTable = table(string(predictorNames(:)), shapImportance(:), ...
        'VariableNames', {'Feature', 'SHAP_Importance'});
    shapTable = sortrows(shapTable, 'SHAP_Importance', 'descend');
    
    try, writetable(shapTable, fullfile(resultsDir,'shap_importance_v3.csv')); catch, end
    fprintf('✓ Computed SHAP values\n');
catch ME
    fprintf('SHAP computation failed: %s\n', ME.message);
    fprintf('  (Note: SHAP requires MATLAB R2021a+ or custom implementation)\n');
end

% --- 3. Output importance rankings ---
fprintf('\n=== Feature Importance Rankings ===\n\n');

% 3.1 SHAP importance (recommended)
if ~isempty(shapTable) && height(shapTable) > 0
    fprintf('[1] SHAP Importance (Mean |SHAP|):\n');
    fprintf('     (Higher = stronger impact on predictions)\n');
    fprintf('     Top 15 Features:\n');
    for i = 1:min(15, height(shapTable))
        fprintf('       %2d. %-40s  SHAP=%.4f\n', ...
            i, char(shapTable.Feature(i)), ...
            shapTable.SHAP_Importance(i));
    end
end

% 3.2 Model gain
if isfield(modelImportance, 'gain') && ~isempty(modelImportance.gain)
    fprintf('\n[2] Gini Gain-based Importance:\n');
    fprintf('     (Higher = more splits in decision trees)\n');
    gains = table2array(modelImportance.gain);
    [sortedGains, sortIdx] = sort(gains, 'descend');
    varNames = modelImportance.gain.Properties.VariableNames;
    fprintf('     Top 15 Features:\n');
    for i = 1:min(15, numel(sortIdx))
        idx = sortIdx(i);
        fprintf('       %2d. %-40s  Gain=%.4f\n', ...
            i, char(varNames{idx}), sortedGains(i));
    end
end

% 3.3 Univariate discriminative power (from existing results)
if exist('featStatsTest', 'var') && ~isempty(featStatsTest)
    fprintf('\n[3] Univariate Discriminative Power (Test Set):\n');
    fprintf('     (AUC: 0.5=random, 1.0=perfect)\n');
    featStatsTest_sorted = sortrows(featStatsTest, 'AUC', 'descend');
    fprintf('     Top 15 Features by AUC:\n');
    for i = 1:min(15, height(featStatsTest_sorted))
        fprintf('       %2d. %-40s  AUC=%.3f | Cohen_d=%+.2f | MI=%.3f bits\n', ...
            i, char(featStatsTest_sorted.Feature(i)), ...
            featStatsTest_sorted.AUC(i), ...
            featStatsTest_sorted.CohensD(i), ...
            featStatsTest_sorted.MI_bits(i));
    end
end

% 3.4 Statistical significance
if exist('featStatsTest', 'var') && ~isempty(featStatsTest)
    featStatsSig = featStatsTest(featStatsTest.p_ranksum < 0.05, :);
    featStatsSig = sortrows(featStatsSig, 'p_ranksum', 'ascend');
    if height(featStatsSig) > 0
        fprintf('\n[4] Statistical Significance (Wilcoxon Rank-Sum, p<0.05):\n');
        fprintf('     Significant features: %d/%d\n', height(featStatsSig), numel(predictorNames));
        fprintf('     Top 10 Most Significant:\n');
        for i = 1:min(10, height(featStatsSig))
            fprintf('       %2d. %-40s  p=%.2e\n', ...
                i, char(featStatsSig.Feature(i)), featStatsSig.p_ranksum(i));
        end
    end
end


%% ================================================================================================
%  Save model package
% ================================================================================================
trainedModelPackage = struct();
trainedModelPackage.trainedClassifier           = trainedClassifier;
trainedModelPackage.requiredVariables           = predictorNames;
trainedModelPackage.responseName                = responseName;
trainedModelPackage.options                     = modelOptions;
trainedModelPackage.validationAccuracy          = valAcc;
trainedModelPackage.testAUC                     = aucTest;
trainedModelPackage.testAUPRC                   = aucPR;
trainedModelPackage.testBrier                   = brier;
trainedModelPackage.featureEffectivenessTrain   = featStatsTrain;
trainedModelPackage.featureEffectivenessTest    = featStatsTest;
trainedModelPackage.modelImportance             = modelImportance;
trainedModelPackage.shapImportance              = shapTable;  % SHAP importance
trainedModelPackage.savedAt                     = datetime("now","Format","yyyy-MM-dd HH:mm:ss");

try
    save(modelOutFile, 'trainedClassifier','predictorNames','responseName','modelOptions','trainedModelPackage');
    fprintf('✓ Saved model: %s\n', modelOutFile);
catch ME
    fprintf('Save model failed: %s\n', ME.message);
end

fprintf('\nAll done.\n');

%% =================================================================================================
%  Local functions in this file (grouped by topic)
% =================================================================================================

% ---------------------------------------------------------------------------------
% Baseline and recovery core
% ---------------------------------------------------------------------------------
function [muRR, sigRR, baseRR_vec, muTamp, muQTc] = local_baseline_stats(~, pvcSample, rGlobalAll, rr_between, isPVCBeat, sqi_vec, t_amp_vec, tGlobalIdx, qrs_dur_vec, fs, params)
    baseA = max(1, double(pvcSample) - round(params.baselineSec*fs));
    baseB = max(1, double(pvcSample) - 1);
    j_in = find(rGlobalAll >= baseA & rGlobalAll <= baseB);
    j_in = j_in(:); j_in = j_in(j_in>=2);
    if ~isempty(j_in)
        mask_ok = ~isPVCBeat(j_in) & ~isPVCBeat(j_in-1) & sqi_vec(j_in) & sqi_vec(j_in-1) & isfinite(rr_between(j_in));
        baseRR_vec = rr_between(j_in(mask_ok));
    else
        baseRR_vec = [];
    end
    baseRR_vec = baseRR_vec(isfinite(baseRR_vec) & baseRR_vec>=params.minRR & baseRR_vec<=params.maxRR);

    if numel(baseRR_vec) < max(3, params.baselineMinBeats)
        j_all = (2:numel(rGlobalAll)).';
        mask_all = ~isPVCBeat(j_all) & ~isPVCBeat(j_all-1) & sqi_vec(j_all) & sqi_vec(j_all-1) & isfinite(rr_between(j_all));
        rr_all = rr_between(j_all(mask_all)); rr_all = rr_all(isfinite(rr_all));
        if isempty(rr_all)
            muRR = median(rr_between(isfinite(rr_between)));
            sigRR= std(rr_between(isfinite(rr_between)));
        else
            muRR = median(rr_all);
            sigRR= std(rr_all);
        end
    else
        muRR = mean(baseRR_vec);
        sigRR= std(baseRR_vec);
    end
    if ~isfinite(muRR) || muRR<=0, muRR = median(rr_between(isfinite(rr_between))); end
    if ~isfinite(sigRR), sigRR = 0; end

    muTamp = NaN; muQTc = NaN;
    try
        if ~isempty(j_in)
            j_ok = j_in(~isnan(t_amp_vec(j_in)) & isfinite(t_amp_vec(j_in)));
            t_base = t_amp_vec(j_ok);
            if ~isempty(t_base), muTamp = mean(abs(t_base)); end
        end
    catch, muTamp = NaN; end
    try
        j_ok = j_in;
        keep = false(size(j_ok));
        for mm = 1:numel(j_ok)
            j = j_ok(mm);
            if isfinite(tGlobalIdx(j)) && tGlobalIdx(j) > rGlobalAll(j) && isfinite(rr_between(j)) && rr_between(j)>0
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                if isfinite(QT_approx) && QT_approx>=0.20 && QT_approx<=0.60
                    keep(mm)=true;
                end
            end
        end
        j_ok = j_ok(keep);
        if ~isempty(j_ok)
            qtc_vals = nan(numel(j_ok),1);
            for k = 1:numel(j_ok)
                j = j_ok(k);
                rrj = rr_between(j);
                QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
                qtc_vals(k)= QT_approx / (rrj^(1/3));
            end
            qtc_vals = qtc_vals(isfinite(qtc_vals) & qtc_vals>=0.30 & qtc_vals<=0.70);
            if ~isempty(qtc_vals), muQTc = mean(qtc_vals); end
        end
    catch, muQTc = NaN; end
end

function ts_val = local_approx_ts(idxPVC, pvcSample, obsEnd, rGlobalAll, rr_between, isPVCBeat, sqi_vec)
    numBeats = numel(rGlobalAll);
    j_after = (idxPVC+1):min(numBeats, idxPVC+20);
    j_after = j_after(:);
    mask_rr_ok = (rGlobalAll(j_after) <= obsEnd) & (rGlobalAll(j_after-1) >= pvcSample) & ...
        ~isPVCBeat(j_after) & ~isPVCBeat(j_after-1) & sqi_vec(j_after) & sqi_vec(j_after-1) & ...
        isfinite(rr_between(j_after));
    j_after = j_after(mask_rr_ok);
    ts_val = NaN;
    if numel(j_after) >= 7
        rr_seq = rr_between(j_after);
        maxSlope = -inf;
        for uu = 1:(numel(rr_seq)-4)
            slope = (rr_seq(uu+4) - rr_seq(uu)) / 4.0;
            if slope > maxSlope, maxSlope = slope; end
        end
        ts_val = maxSlope;
    end
end

function t50 = local_halflife(dev, t, thr, consecBeats)
    t50 = NaN;
    if isempty(dev)||isempty(t), return; end
    dev = dev(:); t = t(:);
    hit = dev <= thr;
    run = 0;
    for i = 1:numel(hit)
        if hit(i), run=run+1; else, run=0; end
        if run>=consecBeats
            t50 = t(i - consecBeats + 1); return;
        end
    end
end

function tau = local_time_constant(dev, t, K)
    tau = NaN;
    if isempty(dev)||isempty(t), return; end
    dev = dev(:); t = t(:);
    K = min([K, numel(dev), numel(t)]);
    y = log(max(dev(1:K), eps));
    x = t(1:K);
    if numel(unique(x))<2, return; end
    p = polyfit(x,y,1);
    a = p(1);
    if a < 0, tau = -1/a; end
end

function A = local_auc(dev, t, normTime)
    A=NaN;
    if numel(dev)<2 || numel(t)<2, A=0; return; end
    dev = dev(:); t = t(:);
    dt = diff(t)./max(normTime,1);
    for i=2:numel(dev)
        if isnan(dev(i)), dev(i)=dev(i-1); end
    end
    if isnan(dev(1)), dev(1)=0; end
    A = sum(0.5*(dev(1:end-1)+dev(2:end)).*dt);
    if ~isfinite(A), A = NaN; end
end

function oi = local_oscillation_index(dev)
    oi = NaN;
    if numel(dev)<3, oi=0; return; end
    d1 = diff(dev(:));
    s  = sign(d1); s(s==0)=1;
    oi = sum(abs(diff(s))>0) / max(numel(d1)-1,1);
end

function cnt = local_recovery_stalls(dev, thr, consecBeats)
    cnt=0;
    if isempty(dev), return; end
    under = dev(:) <= thr;
    if ~any(under), return; end
    enterIdx = find([false; (~under(1:end-1) & under(2:end))]);
    i=1; validSegments=0;
    while i<=numel(enterIdx)
        startPos = enterIdx(i)+1;
        len=0; j=startPos;
        while j<=numel(under) && under(j)
            len=len+1; j=j+1;
        end
        if len>=consecBeats, validSegments = validSegments+1; end
        i=i+1;
    end
    cnt = max(validSegments-1,0);
end

function qtc_seq = local_qtc_series(j_after, rGlobalAll, tGlobalIdx, qrs_dur_vec, rr_between, fs)
    qtc = nan(numel(j_after),1);
    for i=1:numel(j_after)
        j = j_after(i);
        if isfinite(tGlobalIdx(j)) && tGlobalIdx(j)>rGlobalAll(j) && isfinite(rr_between(j)) && rr_between(j)>0
            QT_approx = ((tGlobalIdx(j)-rGlobalAll(j))/fs) + 0.5*max(qrs_dur_vec(j),0);
            if isfinite(QT_approx) && QT_approx>=0.20 && QT_approx<=0.60
                qtc(i) = QT_approx / (rr_between(j)^(1/3));
            end
        end
    end
    qtc_seq = qtc(isfinite(qtc));
end

function rmssd_vec = local_rmssd_from_base_perPVC(idxPVC_all_sorted, pvcRidx, rGlobalAll, rr_between, isPVCBeat, sqi_vec, fs, params)
    numPVC = numel(idxPVC_all_sorted);
    rmssd_vec = nan(numPVC,1);
    for kk = 1:numPVC
        pvcSample = pvcRidx(kk);
        baseA = max(1, double(pvcSample)-round(params.baselineSec*fs));
        baseB = max(1, double(pvcSample)-1);
        j_in = find(rGlobalAll>=baseA & rGlobalAll<=baseB);
        j_in = j_in(:); j_in = j_in(j_in>=2);
        if isempty(j_in), continue; end
        mask_ok = ~isPVCBeat(j_in) & ~isPVCBeat(j_in-1) & sqi_vec(j_in) & sqi_vec(j_in-1) & isfinite(rr_between(j_in));
        rr_base = rr_between(j_in(mask_ok));
        rr_base = rr_base(isfinite(rr_base));
        if numel(rr_base)>=2
            d = diff(rr_base);
            rmssd_vec(kk) = sqrt(mean(d.^2));
        end
    end
end

% ---------------------------------------------------------------------------------
% Model / Prediction / Threshold
% ---------------------------------------------------------------------------------
function [trainedClassifier, validationAccuracy] = local_train_model(trainingTable, predictorNames, responseName, modelOptions)
    predictors = trainingTable(:, predictorNames);
    response = trainingTable.(responseName); if iscell(response), response = cell2mat(response); end

    % Compute number of subfeatures to sample
    numVars = modelOptions.NumVariablesToSample;
    if ischar(numVars) || isstring(numVars)
        if strcmpi(char(numVars),'all')
            numVarsArgGlobal = 'all';
        elseif strcmpi(char(numVars),'sqrt')
            p = width(predictors);
            numVarsArgGlobal = max(1, floor(sqrt(max(1,double(p)))));
        else
            numVarsArgGlobal = 'all';
        end
    else
        numVarsArgGlobal = max(1, round(double(numVars)));
    end

    % Non-auto mode
    if isfield(modelOptions,'Method') && ~strcmpi(char(modelOptions.Method),'auto')
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal,'MinLeafSize',modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal);
        end
        fitArgs = {predictors, response,'Method',modelOptions.Method,'NumLearningCycles',modelOptions.NumLearningCycles,'Learners',template};
        if ~strcmpi(char(modelOptions.Method),'bag') && isfield(modelOptions,'LearnRate')
            fitArgs = [fitArgs, {'LearnRate', modelOptions.LearnRate}];
        end
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost',C}];
        end
        cls = fitcensemble(fitArgs{:});
        try
            cv = crossval(cls,'KFold',modelOptions.cvKFold);
            validationAccuracy = 1 - kfoldLoss(cv);
        catch
            validationAccuracy = NaN;
        end
        trainedClassifier = struct();
        trainedClassifier.ClassificationEnsemble = cls;
        trainedClassifier.RequiredVariables = predictorNames;
        trainedClassifier.Options = modelOptions;
        return;
    end

    % Auto search
    if ~isfield(modelOptions,'candidateMethods') || isempty(modelOptions.candidateMethods)
        modelOptions.candidateMethods = {'LogitBoost','GentleBoost','AdaBoostM1','RUSBoost'};
    end
    if ~isfield(modelOptions,'searchGrid') || isempty(modelOptions.searchGrid)
        modelOptions.searchGrid = struct();
    end
    sg = modelOptions.searchGrid;
    if ~isfield(sg,'NumLearningCycles'),    sg.NumLearningCycles    = [200,300]; end
    if ~isfield(sg,'LearnRate'),            sg.LearnRate            = [0.03,0.05,0.10]; end
    if ~isfield(sg,'MaxNumSplits'),         sg.MaxNumSplits         = [40,60,100]; end
    if ~isfield(sg,'MinLeafSize'),          sg.MinLeafSize          = [8,12,20]; end
    if ~isfield(sg,'NumVariablesToSample'), sg.NumVariablesToSample = {'sqrt','all'}; end

    bestAUPRC = -inf; bestInfo = struct(); bestCls = []; bestValAcc = NaN;
    verbose = isfield(modelOptions,'autoSelectVerbose') && modelOptions.autoSelectVerbose;

    numMethods = numel(modelOptions.candidateMethods);
    combPerMethod = numel(sg.NumLearningCycles)*numel(sg.LearnRate)*numel(sg.MaxNumSplits)* ...
                    numel(sg.MinLeafSize)*numel(sg.NumVariablesToSample);
    totalComb = numMethods * combPerMethod;
    tried = 0;

    for mi = 1:numMethods
        methodName = char(modelOptions.candidateMethods{mi});
        for nlc = sg.NumLearningCycles
            for mns = sg.MaxNumSplits
                for mls = sg.MinLeafSize
                    for nvs = 1:numel(sg.NumVariablesToSample)
                        nvsVal = sg.NumVariablesToSample{nvs};
                        if ischar(nvsVal)||isstring(nvsVal)
                            if strcmpi(char(nvsVal),'all')
                                numVarsArg = 'all';
                            elseif strcmpi(char(nvsVal),'sqrt')
                                p = width(predictors);
                                numVarsArg = max(1,floor(sqrt(max(1,double(p)))));
                            else
                                numVarsArg = 'all';
                            end
                        else
                            numVarsArg = max(1, round(double(nvsVal)));
                        end
                        if ~isempty(mls)
                            template = templateTree('MaxNumSplits',mns,'NumVariablesToSample',numVarsArg,'MinLeafSize',mls);
                        else
                            template = templateTree('MaxNumSplits',mns,'NumVariablesToSample',numVarsArg);
                        end
                        for lr = sg.LearnRate
                            fitArgs = {predictors,response,'Method',methodName,'NumLearningCycles',nlc,'Learners',template,'LearnRate',lr};
                            if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
                                C = [0, modelOptions.costFP; modelOptions.costFN, 0];
                                fitArgs = [fitArgs, {'Cost',C}];
                            end
                            tried = tried + 1;
                            if verbose
                                fprintf('  [Auto %d/%d] %s NLC=%d LR=%.3f MaxSplits=%d MinLeaf=%d NumVars=%s\n', ...
                                    tried, totalComb, methodName, nlc, lr, mns, mls, mat2str(numVarsArg));
                            end
                            try
                                cls = fitcensemble(fitArgs{:});
                            catch
                                continue;
                            end
                            % CV AUPRC (Dead)
                            try
                                cvMdl = crossval(cls,'KFold',modelOptions.cvKFold);
                                valAcc = 1 - kfoldLoss(cvMdl);
                                [~, cvScores] = kfoldPredict(cvMdl);
                                try
                                    clsNames = cvMdl.ClassNames;
                                catch
                                    clsNames = [0;1];
                                end
                                if iscell(clsNames)
                                    try, clsNames = cell2mat(clsNames); catch, clsNames=[0;1]; end
                                end
                                col0 = find(clsNames==0,1,'first'); if isempty(col0), col0=1; end
                                scoresDead = cvScores(:,col0);
                                yDead = 1 - response;
                                try
                                    beta = glmfit(scoresDead, yDead, 'binomial','link','logit');
                                    scoresProb = glmval(beta, scoresDead, 'logit');
                                catch
                                    scoresProb = scoresDead;
                                end
                                try
                                    [~,~,~,auprcLocal] = perfcurve(yDead, scoresProb, 1, 'XCrit','reca','YCrit','prec');
                                catch
                                    [~,~,~,auprcLocal] = perfcurve(yDead, scoresDead, 1, 'XCrit','reca','YCrit','prec');
                                end
                                if isfinite(auprcLocal) && auprcLocal > bestAUPRC
                                    bestAUPRC = auprcLocal; bestCls = cls; bestValAcc = valAcc;
                                    bestInfo = struct('Method',methodName,'NumLearningCycles',nlc,'LearnRate',lr, ...
                                        'MaxNumSplits',mns,'MinLeafSize',mls,'NumVariablesToSample',nvsVal);
                                end
                            catch
                            end
                        end
                    end
                end
            end
        end
    end

    if verbose && ~isempty(fieldnames(bestInfo))
        fprintf('Auto-selected: %s NLC=%d LR=%.3f MaxSplits=%d MinLeaf=%d NumVars=%s | CV-AUPRC(Dead)=%.3f\n', ...
            string(bestInfo.Method), bestInfo.NumLearningCycles, bestInfo.LearnRate, ...
            bestInfo.MaxNumSplits, bestInfo.MinLeafSize, mat2str(bestInfo.NumVariablesToSample), bestAUPRC);
    end

    if isempty(bestCls)
        if verbose, fprintf('Auto-selection failed → fallback GentleBoost.\n'); end
        maxSplits = modelOptions.MaxNumSplits;
        if isfield(modelOptions,'MinLeafSize') && ~isempty(modelOptions.MinLeafSize)
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal,'MinLeafSize',modelOptions.MinLeafSize);
        else
            template = templateTree('MaxNumSplits',maxSplits,'NumVariablesToSample',numVarsArgGlobal);
        end
        fitArgs = {predictors,response,'Method','GentleBoost','NumLearningCycles',modelOptions.NumLearningCycles,'Learners',template,'LearnRate',modelOptions.LearnRate};
        if isfield(modelOptions,'enableCostMatrix') && modelOptions.enableCostMatrix
            C = [0, modelOptions.costFP; modelOptions.costFN, 0];
            fitArgs = [fitArgs, {'Cost',C}];
        end
        bestCls = fitcensemble(fitArgs{:});
        try
            cv = crossval(bestCls,'KFold',modelOptions.cvKFold);
            bestValAcc = 1 - kfoldLoss(cv);
        catch
            bestValAcc = NaN;
        end
        bestInfo = struct('Method','GentleBoost','NumLearningCycles',modelOptions.NumLearningCycles, ...
            'LearnRate',modelOptions.LearnRate,'MaxNumSplits',maxSplits,'MinLeafSize',modelOptions.MinLeafSize, ...
            'NumVariablesToSample',numVars);
    end

    trainedClassifier = struct();
    trainedClassifier.ClassificationEnsemble = bestCls;
    trainedClassifier.RequiredVariables = predictorNames;
    % Record selected hyperparameters
    modelOptions.selectedMethod          = string(bestInfo.Method);
    modelOptions.Method                  = string(bestInfo.Method);
    modelOptions.NumLearningCycles       = double(bestInfo.NumLearningCycles);
    modelOptions.LearnRate               = double(bestInfo.LearnRate);
    modelOptions.MaxNumSplits            = double(bestInfo.MaxNumSplits);
    modelOptions.MinLeafSize             = double(bestInfo.MinLeafSize);
    modelOptions.NumVariablesToSample    = bestInfo.NumVariablesToSample;
    trainedClassifier.Options            = modelOptions;
    validationAccuracy = bestValAcc;
end

function model = local_threshold_tuning(model, Ttrain, predictorNames, responseName, modelOptions)
    rng(0);
    try
        cvMdl = crossval(model.ClassificationEnsemble,'KFold',modelOptions.cvKFold);
        [~, cvScores] = kfoldPredict(cvMdl);
        try
            cls = cvMdl.ClassNames;
        catch
            cls = [0;1];
        end
        if iscell(cls)
            try, cls = cell2mat(cls); catch, cls=[0;1]; end
        end
        col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
        scoresDead = cvScores(:,col0);
    catch
        [~, scoresDead] = local_predict_with_threshold(model, Ttrain, predictorNames);
    end
    yTrain = Ttrain.(responseName); if iscell(yTrain), yTrain = cell2mat(yTrain); end
    yDead = 1 - yTrain;
    calibBeta = [];
    try
        calibBeta = glmfit(scoresDead, yDead, 'binomial','link','logit');
        scoresProb = glmval(calibBeta, scoresDead, 'logit');
    catch
        scoresProb = scoresDead;
        calibBeta = [];
    end

    bestThr = 0.5; bestF1 = -inf;
    for thr = modelOptions.thrCandidates
        yHatDead = double(scoresProb >= thr);
        TP = sum((yHatDead==1)&(yDead==1));
        FP = sum((yHatDead==1)&(yDead==0));
        FN = sum((yHatDead==0)&(yDead==1));
        prec = TP/max(TP+FP,eps); rec = TP/max(TP+FN,eps);
        F1 = (prec+rec>0) * (2*prec*rec/(prec+rec));
        if F1 > bestF1
            bestF1 = F1; bestThr = thr;
        end
    end
    model.Options.pvcThreshold = bestThr;
    if ~isempty(calibBeta)
        model.Options.calibrationBeta = calibBeta;
        fprintf('Platt calibration: beta=[%.4f %.4f]\n', calibBeta(1), calibBeta(2));
    end
    fprintf('Best threshold (CV F1[Dead]): %.2f (F1=%.3f)\n', bestThr, bestF1);
end

function [predVital, scoresDead] = local_predict_with_threshold(model, T, predictorNames)
    X = T(:, predictorNames);
    try
        [~, rawScores] = predict(model.ClassificationEnsemble, X);
    catch
        [~, rawScores] = predict(model.ClassificationEnsemble, X{:,:});
    end
    try
        cls = model.ClassificationEnsemble.ClassNames;
    catch
        cls = [0;1];
    end
    if iscell(cls)
        try, cls = cell2mat(cls); catch, cls=[0;1]; end
    end
    col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
    scoresDead = rawScores(:,col0);
    scoresProb = scoresDead;
    if isfield(model,'Options') && isfield(model.Options,'calibrationBeta') && numel(model.Options.calibrationBeta)==2 && all(isfinite(model.Options.calibrationBeta))
        try
            scoresProb = glmval(model.Options.calibrationBeta, scoresDead, 'logit');
        catch
            scoresProb = scoresDead;
        end
    end
    thr = 0.5;
    if isfield(model,'Options') && isfield(model.Options,'enableThresholdMoving') && model.Options.enableThresholdMoving
        if isfield(model.Options,'pvcThreshold') && isfinite(model.Options.pvcThreshold)
            thr = model.Options.pvcThreshold;
        end
    end
    yHatDead = double(scoresProb >= thr);
    predVital = 1 - yHatDead;
end

function local_print_confusion(tag, yTrueVital, yHatVital)
    yTrueVital = double(yTrueVital(:));
    yHatVital  = double(yHatVital(:));
    yTrueDead  = 1 - yTrueVital;
    yHatDead   = 1 - yHatVital;
    TP = sum((yTrueDead==1)&(yHatDead==1));
    FN = sum((yTrueDead==1)&(yHatDead==0));
    FP = sum((yTrueDead==0)&(yHatDead==1));
    TN = sum((yTrueDead==0)&(yHatDead==0));
    acc = mean(yTrueVital==yHatVital);
    sens = TP/max(TP+FN,eps);
    spec = TN/max(TN+FP,eps);
    prec = TP/max(TP+FP,eps);
    f1   = (prec+sens>0)*(2*prec*sens/(prec+sens));

    fprintf('\n--- %s Confusion Matrix (English) ---\n', tag);
    hdr = sprintf('%-14s%12s%12s','Actual\\Pred','Alive','Dead');
    fprintf('%s\n', hdr);
    fprintf('%s\n', repmat('-',1,length(hdr)));
    fprintf('%-14s%12d%12d\n','Alive',TN,FP);
    fprintf('%-14s%12d%12d\n','Dead', FN,TP);
    fprintf('Accuracy=%.2f%% | Recall(Dead)=%.2f%% | Specificity(Alive)=%.2f%% | Precision(Dead)=%.2f%% | F1(Dead)=%.3f\n', ...
        100*acc, 100*sens, 100*spec, 100*prec, f1);
    try
        figure;
        confMat = [TN FP; FN TP];
        confusionchart(confMat, {'Alive','Dead'}, 'RowSummary','row-normalized','ColumnSummary','column-normalized');
        title([tag ' Confusion']);
    catch
    end
end

% ---------------------------------------------------------------------------------
% Data split / Linear helpers
% ---------------------------------------------------------------------------------
function [trainMask, testMask] = local_group_stratified_split(groupIds, y, testRatio, seed)
    rng(seed);
    groupIds = string(groupIds(:));
    y = double(y(:));
    ug = unique(groupIds);
    G = struct('id',[], 'n0',0, 'n1',0, 'idx',[]);
    G = repmat(G, numel(ug),1);
    for i=1:numel(ug)
        gid = ug(i);
        idx = find(groupIds==gid);
        yi  = y(idx);
        G(i).id = gid; G(i).n0 = sum(yi==0); G(i).n1=sum(yi==1); G(i).idx=idx;
    end
    target0 = sum(y==0)*testRatio;
    target1 = sum(y==1)*testRatio;
    cur0=0; cur1=0; ord = randperm(numel(G));
    testGroups = false(numel(G),1);
    for k = ord
        g = G(k);
        gain0 = abs((cur0+g.n0)-target0) - abs(cur0-target0);
        gain1 = abs((cur1+g.n1)-target1) - abs(cur1-target1);
        if (cur0<target0 || cur1<target1) && ((gain0<=0)||(gain1<=0))
            testGroups(k)=true; cur0=cur0+g.n0; cur1=cur1+g.n1;
        end
    end
    k=1;
    while (cur0<target0 || cur1<target1) && k<=numel(G)
        if ~testGroups(k)
            g=G(k); testGroups(k)=true; cur0=cur0+g.n0; cur1=cur1+g.n1;
        end
        k=k+1;
    end
    testMask = false(numel(y),1);
    for i=1:numel(G)
        if testGroups(i)
            testMask(G(i).idx)=true;
        end
    end
    trainMask = ~testMask;
    % Ensure both classes present
    if sum(y(testMask)==1)==0
        idxMove = find(trainMask&(y==1),1,'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(testMask)==0)==0
        idxMove = find(trainMask&(y==0),1,'first'); if ~isempty(idxMove), testMask(idxMove)=true; trainMask(idxMove)=false; end
    end
    if sum(y(trainMask)==1)==0
        idxMove = find(testMask&(y==1),1,'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
    if sum(y(trainMask)==0)==0
        idxMove = find(testMask&(y==0),1,'first'); if ~isempty(idxMove), trainMask(idxMove)=true; testMask(idxMove)=false; end
    end
end

function slope = local_linear_slope(t,x)
    slope=NaN;
    t=t(:); x=x(:);
    ok=isfinite(t)&isfinite(x); t=t(ok); x=x(ok);
    if numel(t)<2, return; end
    p=polyfit(t,x,1); slope=p(1);
end

% ---------------------------------------------------------------------------------
% Complexity / Statistics
% ---------------------------------------------------------------------------------
function [SD1, SD2] = local_poincare_sd(rr)
    SD1=NaN; SD2=NaN;
    rr=rr(:); rr=rr(isfinite(rr));
    if numel(rr)<3, return; end
    d=diff(rr);
    sd_rr=std(rr); sd_d=std(d);
    SD1 = sqrt(0.5)*sd_d;
    tmp = 2*sd_rr^2 - 0.5*sd_d^2;
    if tmp>0, SD2 = sqrt(tmp); end
end

function v = local_safe_max(x)
    x=x(:); x=x(isfinite(x));
    if isempty(x), v=NaN; else, v=max(x); end
end

function v = local_safe_min(x)
    x=x(:); x=x(isfinite(x));
    if isempty(x), v=NaN; else, v=min(x); end
end

function s = local_sampen(x,m,r)
    s=NaN;
    x=x(:); x=x(isfinite(x));
    n=numel(x);
    if n < m+2 || ~isfinite(r) || r<=0, return; end
    count_m=0; count_m1=0;
    for i=1:(n-m)
        Xi = x(i:(i+m-1));
        for j=(i+1):(n-m+1)
            Xj = x(j:(j+m-1));
            if max(abs(Xi-Xj))<=r
                count_m = count_m + 1;
                if j <= n - m
                    if abs(x(i+m)-x(j+m))<=r
                        count_m1 = count_m1 + 1;
                    end
                end
            end
        end
    end
    if count_m==0 || count_m1==0, return; end
    s = -log(count_m1 / count_m);
end

function c = local_lzc_binary(b)
    c=NaN;
    b=b(:); b=b(isfinite(b));
    if isempty(b), return; end
    b=double(b~=0);
    n=numel(b);
    if n<2, c=0; return; end
    s=char(b+'0').';
    i=1; c_raw=1; l=1; k=1; k_max=1; nlen=length(s);
    while true
        if i+k>nlen || l+k>nlen
            c_raw=c_raw+1; break;
        end
        if s(i+k)==s(l+k)
            k=k+1; if k>k_max, k_max=k; end
        else
            if k>k_max, k_max=k; end
            if k_max==1
                c_raw=c_raw+1; l=l+1; i=1; k=1; k_max=1;
            else
                i=i+1;
                if i==l
                    c_raw=c_raw+1; l=l+k_max; i=1; k=1; k_max=1;
                else
                    k=1;
                end
            end
        end
        if l>nlen, break; end
    end
    c = c_raw * (log2(n))/n;
end

function [featStats] = local_feature_effectiveness(T, predictorNames, responseName, tag)
    yVital = T.(responseName); if iscell(yVital), yVital = cell2mat(yVital); end
    yDead = 1 - double(yVital(:));
    rows = numel(predictorNames);
    Feature=strings(rows,1); AUC=nan(rows,1); CohensD=nan(rows,1);
    SpearmanR=nan(rows,1); p_ranksum=nan(rows,1); MI_bits=nan(rows,1);
    for i=1:rows
        name = predictorNames{i}; Feature(i)=string(name);
        x = double(T.(name)); x = x(:);
        ok = isfinite(x)&isfinite(yDead);
        x = x(ok); y = yDead(ok);
        if numel(unique(y))<2 || numel(x)<5, continue; end
        try
            [~,~,~,auc] = perfcurve(y,x,1);
            if ~isfinite(auc), auc=NaN; end
            if auc<0.5, auc = 1 - auc; end
            AUC(i)=auc;
        catch
            AUC(i)=NaN;
        end
        try
            xd = x(y==1); xa = x(y==0);
            md=mean(xd); ma=mean(xa);
            sd_d=std(xd); sd_a=std(xa);
            n_d=numel(xd); n_a=numel(xa);
            s_p = sqrt(((n_d-1)*sd_d^2 + (n_a-1)*sd_a^2)/max(n_d+n_a-2,1));
            if s_p>0, CohensD(i)=(md-ma)/s_p; end
        catch, CohensD(i)=NaN; end
        try
            SpearmanR(i)=corr(x,y,'type','Spearman','rows','pairwise');
        catch, SpearmanR(i)=NaN; end
        try
            xd=x(y==1); xa=x(y==0);
            if numel(xd)>=2 && numel(xa)>=2
                p_ranksum(i)=ranksum(xd,xa);
            end
        catch, p_ranksum(i)=NaN; end
        try
            bins = local_quantile_bins(x,10);
            MI_bits(i) = local_mutual_info_bits(bins,y);
        catch, MI_bits(i)=NaN; end
    end
    featStats = table(Feature,AUC,CohensD,SpearmanR,p_ranksum,MI_bits);
    try
        fprintf('  [%s] median AUC=%.3f | top AUC=%.3f (%s)\n', tag, median(AUC,'omitnan'), max(AUC), string(Feature(AUC==max(AUC))));
    catch
    end
end

function bins = local_quantile_bins(x, numBins)
    x=x(:); ok=isfinite(x); x=x(ok);
    if isempty(x), bins=nan(size(ok)); return; end
    edges = quantile(x, linspace(0,1,numBins+1));
    edges(1)=-inf; edges(end)=inf;
    bins_full = nan(numel(ok),1);
    bins_full(ok) = discretize(x, edges);
    bins = bins_full;
end

function mi = local_mutual_info_bits(xBins, yBin)
    mi=NaN;
    if numel(xBins)~=numel(yBin), return; end
    ok=isfinite(xBins)&isfinite(yBin);
    xBins=xBins(ok); yBin=yBin(ok);
    if isempty(xBins), mi=NaN; return; end
    K = max(xBins);
    if K<=1, mi=0; return; end
    N=numel(xBins);
    px = accumarray(xBins,1,[K,1],@sum,0)/N;
    py = [sum(yBin==0); sum(yBin==1)]/N;
    pxy=zeros(K,2);
    for k=1:K
        sel = (xBins==k);
        pxy(k,1) = sum(sel & (yBin==0))/N;
        pxy(k,2) = sum(sel & (yBin==1))/N;
    end
    mi_val=0;
    for k=1:K
        for j=1:2
            if pxy(k,j)>0 && px(k)>0 && py(j)>0
                mi_val = mi_val + pxy(k,j)*log2(pxy(k,j)/(px(k)*py(j)));
            end
        end
    end
    mi=mi_val;
end

% ---------------------------------------------------------------------------------
% Publication-quality plots
% ---------------------------------------------------------------------------------
function local_generate_publication_plots(model, Ttrain, Ttest, predictorNames, responseName, resultsDir)
    try
        [~, rawScoresTr] = predict(model.ClassificationEnsemble, Ttrain(:,predictorNames));
        try
            cls = model.ClassificationEnsemble.ClassNames;
        catch
            cls = [0;1];
        end
        if iscell(cls)
            try, cls=cell2mat(cls); catch, cls=[0;1]; end
        end
        col0 = find(cls==0,1,'first'); if isempty(col0), col0=1; end
        scoresDeadTr = rawScoresTr(:,col0);
        scoresProbTr = scoresDeadTr;
        if isfield(model,'Options') && isfield(model.Options,'calibrationBeta')
            try
                scoresProbTr = glmval(model.Options.calibrationBeta, scoresDeadTr, 'logit');
            catch
            end
        end
        yVitalTr = Ttrain.(responseName); if iscell(yVitalTr), yVitalTr = cell2mat(yVitalTr); end
        yDeadTr = 1 - double(yVitalTr(:));

        [~, rawScoresTe] = predict(model.ClassificationEnsemble, Ttest(:,predictorNames));
        scoresDeadTe = rawScoresTe(:,col0);
        scoresProbTe = scoresDeadTe;
        if isfield(model,'Options') && isfield(model.Options,'calibrationBeta')
            try
                scoresProbTe = glmval(model.Options.calibrationBeta, scoresDeadTe, 'logit');
            catch
            end
        end
        yVitalTe = Ttest.(responseName); if iscell(yVitalTe), yVitalTe = cell2mat(yVitalTe); end
        yDeadTe = 1 - double(yVitalTe(:));

        % ROC
        try
            [Xtr,Ytr,~,aucTr] = perfcurve(yDeadTr, scoresProbTr,1);
        catch
            [Xtr,Ytr,~,aucTr] = perfcurve(yDeadTr, scoresDeadTr,1);
        end
        try
            [Xte,Yte,~,aucTe] = perfcurve(yDeadTe, scoresProbTe,1);
        catch
            [Xte,Yte,~,aucTe] = perfcurve(yDeadTe, scoresDeadTe,1);
        end
        figure; plot(Xte,Yte,'r-','LineWidth',2); hold on;
        plot(Xtr,Ytr,'b--','LineWidth',2); plot([0 1],[0 1],'k:','LineWidth',1.2);
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('False Positive Rate'); ylabel('True Positive Rate');
        title('ROC Curve (Dead=Positive)');
        legend({sprintf('Test (AUC=%.3f)',aucTe), sprintf('Train (AUC=%.3f)',aucTr), 'Chance'}, 'Location','southeast');
        try, exportgraphics(gcf, fullfile(resultsDir,'roc_curve_train_test.png'),'Resolution',300); catch, end

        % PR
        try
            [Rtr,Ptr,~,auprcTr] = perfcurve(yDeadTr, scoresProbTr,1,'XCrit','reca','YCrit','prec');
        catch
            [Rtr,Ptr,~,auprcTr] = perfcurve(yDeadTr, scoresDeadTr,1,'XCrit','reca','YCrit','prec');
        end
        try
            [Rte,Pte,~,auprcTe] = perfcurve(yDeadTe, scoresProbTe,1,'XCrit','reca','YCrit','prec');
        catch
            [Rte,Pte,~,auprcTe] = perfcurve(yDeadTe, scoresDeadTe,1,'XCrit','reca','YCrit','prec');
        end
        prevalTe = mean(yDeadTe);
        figure; plot(Rte,Pte,'r-','LineWidth',2); hold on;
        plot(Rtr,Ptr,'b--','LineWidth',2);
        yline(prevalTe,'k:','LineWidth',1.2,'Label','Prevalence (Test)','LabelHorizontalAlignment','left');
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('Recall'); ylabel('Precision');
        title('Precision-Recall (Dead=Positive)');
        legend({sprintf('Test (AUPRC=%.3f)',auprcTe), sprintf('Train (AUPRC=%.3f)',auprcTr), 'Baseline'}, 'Location','southwest');
        try, exportgraphics(gcf, fullfile(resultsDir,'pr_curve_train_test.png'),'Resolution',300); catch, end

        % Calibration
        numBins=10; edges=linspace(0,1,numBins+1);
        [~,~,binIdxTe] = histcounts(scoresProbTe, edges);
        binProb = accumarray(max(binIdxTe,1), scoresProbTe, [numBins,1], @mean, NaN);
        binObs  = accumarray(max(binIdxTe,1), yDeadTe,      [numBins,1], @mean, NaN);
        [~,~,binIdxTr] = histcounts(scoresProbTr, edges);
        binProbTr = accumarray(max(binIdxTr,1), scoresProbTr, [numBins,1], @mean, NaN);
        binObsTr  = accumarray(max(binIdxTr,1), yDeadTr,      [numBins,1], @mean, NaN);
        figure; plot([0 1],[0 1],'k:','LineWidth',1.2); hold on;
        plot(binProb,binObs,'ro-','LineWidth',2,'MarkerSize',6);
        plot(binProbTr,binObsTr,'bs--','LineWidth',2,'MarkerSize',6);
        grid on; xlim([0,1]); ylim([0,1]);
        xlabel('Predicted Probability (Dead)'); ylabel('Observed Rate (Dead)');
        title('Calibration'); legend('Perfect','Test','Train','Location','southeast');
        try, exportgraphics(gcf, fullfile(resultsDir,'calibration_curve_train_test.png'),'Resolution',300); catch, end

        % Decision Curve
        thrList=0.01:0.01:0.99; N=numel(yDeadTe);
        nbModel=nan(size(thrList)); nbAll=nan(size(thrList)); nbNone=zeros(size(thrList));
        preval = mean(yDeadTe);
        for ii=1:numel(thrList)
            pt=thrList(ii);
            yHat = scoresProbTe >= pt;
            TP = sum((yHat==1)&(yDeadTe==1));
            FP = sum((yHat==1)&(yDeadTe==0));
            nbModel(ii) = (TP/N) - (FP/N)*(pt/(1-pt));
            nbAll(ii)   = preval - (1 - preval)*(pt/(1-pt));
        end
        figure; plot(thrList,nbModel,'r-','LineWidth',2); hold on;
        plot(thrList,nbAll,'k--','LineWidth',1.8);
        plot(thrList,nbNone,'k:','LineWidth',1.8);
        grid on; xlim([0,1]);
        xlabel('Threshold Probability (Dead)'); ylabel('Net Benefit');
        title('Decision Curve (Test)');
        legend('Model','Treat All','Treat None','Location','northwest');
        try, exportgraphics(gcf, fullfile(resultsDir,'decision_curve_test.png'),'Resolution',300); catch, end
        fprintf('Saved evaluation plots.\n');
    catch ME
        fprintf('Plot generation error: %s\n', ME.message);
    end
end

% ---------------------------------------------------------------------------------
% SHAP computation (simplified TreeSHAP)
% ---------------------------------------------------------------------------------
function [shapValues, shapImportance] = local_compute_shap_treebased(ensembleModel, X_table, predictorNames)
    % Compute SHAP values for a tree ensemble (simplified TreeSHAP)
    % Inputs:
    %   ensembleModel - MATLAB ensemble classifier
    %   X_table - feature table (table)
    %   predictorNames - feature name list
    % Outputs:
    %   shapValues - [numSamples x numFeatures] SHAP value matrix
    %   shapImportance - [numFeatures x 1] feature importance (mean |SHAP|)
    
    numSamples = height(X_table);
    numFeatures = numel(predictorNames);
    shapValues = zeros(numSamples, numFeatures);
    
    % Extract feature matrix
    X = zeros(numSamples, numFeatures);
    for i = 1:numFeatures
        X(:, i) = double(X_table.(predictorNames{i}));
    end
    
    % Compute baseline prediction (average over all samples)
    try
        [~, baseScores] = predict(ensembleModel, X_table);
        cls = ensembleModel.ClassNames;
        if iscell(cls)
            try, cls = cell2mat(cls); catch, cls = [0;1]; end
        end
        col0 = find(cls==0, 1, 'first'); if isempty(col0), col0 = 1; end
        baseScore = mean(baseScores(:, col0));
    catch
        baseScore = 0;
    end
    
    fprintf('    Computing SHAP for %d samples x %d features...\n', numSamples, numFeatures);
    
    % Compute SHAP for each sample (leave-one-out marginal contribution approximation)
    for s = 1:numSamples
        if mod(s, 20) == 0
            fprintf('      Progress: %d/%d samples\n', s, numSamples);
        end
        
        % Current sample prediction
        try
            [~, scores_full] = predict(ensembleModel, X_table(s, :));
            predFull = scores_full(col0);
        catch
            predFull = baseScore;
        end
        
        % Marginal contribution per feature (leave-one-out approximation)
        for f = 1:numFeatures
            try
                % Create perturbed sample (replace this feature with its mean)
                X_perturbed = X_table(s, :);
                featMean = mean(X(:, f), 'omitnan');
                X_perturbed.(predictorNames{f}) = featMean;
                
                % Predict
                [~, scores_perturbed] = predict(ensembleModel, X_perturbed);
                predPerturbed = scores_perturbed(col0);
                
                % SHAP value = full prediction - prediction after removing this feature
                shapValues(s, f) = predFull - predPerturbed;
            catch
                shapValues(s, f) = 0;
            end
        end
    end
    
    % Compute feature importance (mean absolute SHAP)
    shapImportance = mean(abs(shapValues), 1, 'omitnan')';
    
    fprintf('    ✓ SHAP computation complete\n');
end
